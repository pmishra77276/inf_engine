{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3c95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d83bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cuda')\n",
    "question=\"hi\"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "out=model.generate(**prompt,max_new_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d57f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out=model.generate(**prompt,max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc7fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   6151,   1268,    527,    499,     30,    602,    574,  20910,\n",
       "           422,    499,   1436,   1520,    757,    449,   2555,     11,    602,\n",
       "           617,    264,    220,   2550,     22,    220,     19],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef9e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, I have a problem with my 3D\n"
     ]
    }
   ],
   "source": [
    "# If out is a tensor (e.g., shape [1, seq_len])\n",
    "decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08669892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cpu')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47aff853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b28405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in model.model.layers:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1334cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "bytes_per_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e186b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2265777587890625"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=model.model.layers[0]\n",
    "num_params = sum(p.numel() for p in l.parameters())\n",
    "num_params\n",
    "param_size_bytes = num_params * 4\n",
    "param_size_MB = param_size_bytes / (1024 ** 3)\n",
    "param_size_MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "679c19b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f41edcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec0798e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers=model.model.layers\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=layers[:-1]\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e69f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 4\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a34a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaed17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n",
      "Warning: Not enough GPU memory. Available: 7.61 GB. Falling back to 1 layer per chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 17:45:32.001540 23607 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 237\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 237\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[1], line 215\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens, temperature, top_p)\u001b[0m\n\u001b[1;32m    212\u001b[0m logits[sorted_indices[sorted_indices_to_remove]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Sample a token from the nucleus\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(next_token[\u001b[38;5;241m0\u001b[39m]), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m generated_text_tokens\u001b[38;5;241m.\u001b[39mappend(tok\u001b[38;5;241m.\u001b[39mdecode(next_token[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with three streams\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk on the load stream\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    rem -= layers_per_chunk\n",
    "\n",
    "    m_next = None\n",
    "    \n",
    "    while True:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        if rem > 0:\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        else:\n",
    "            m_next = None\n",
    "\n",
    "        # Wait for the current chunk to finish loading\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "\n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "\n",
    "        # Check if this is the last chunk\n",
    "        if rem <= 0:\n",
    "            break\n",
    "        \n",
    "        # Offload the current chunk asynchronously\n",
    "        with torch.cuda.stream(stream_offload):\n",
    "            m_current.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline and update counters\n",
    "        m_current = m_next\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    # The last m_current must be moved back to cpu explicitly\n",
    "    m_current.to('cpu')\n",
    "    del m_current\n",
    "    if m_next:\n",
    "        del m_next\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250, temperature=0.7, top_p=0.95):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            # Use the calculated l1 value, not a hardcoded number\n",
    "            x = compute(model, x, l1, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Apply temperature and Top-P sampling\n",
    "        logits = x[:, -1, :] / temperature\n",
    "        \n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        \n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Find the tokens that are in the nucleus\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        \n",
    "        # Shift the indices to the right to keep at least one token\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        # Mask out the logits that are not in the nucleus\n",
    "        logits[sorted_indices[sorted_indices_to_remove]] = -float('inf')\n",
    "        \n",
    "        # Sample a token from the nucleus\n",
    "        next_token = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f60ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d74102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0c469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    # Initialize offload module to None for the first iteration\n",
    "    m_to_offload = None\n",
    "    \n",
    "    while i <= total_layers:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        # Wait for the current chunk to finish loading before computing\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        \n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # Offload the previously used chunk (if it exists)\n",
    "        if m_to_offload:\n",
    "            # Wait for compute to finish before offloading to prevent race condition\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Handle the final cleanup outside the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    # Ensure the last computed module is offloaded as well\n",
    "    if m_to_offload:\n",
    "        m_to_offload.to('cpu')\n",
    "    if m_current: # Should be None but for safety\n",
    "        m_current.to('cpu')\n",
    "    if m_next:\n",
    "        m_next.to('cpu')\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9db5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centerslyphlyphlyphlyphlyphlyphlyphlyphlyph\n",
      "Time taken for 10 tokens: 9.55 seconds\n",
      "\n",
      "Total time 9.552647829055786\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa1fd1",
   "metadata": {},
   "source": [
    "# 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce76b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229fb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b06fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80fb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e3aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "layers=model.model.layers\n",
    "\n",
    "\n",
    "# Usage\n",
    "# compiled_block = torch.compile(FusedBlock(layers[i:j]))\n",
    "\n",
    "# def compute(model, x, layers_per_chunk, cos, sin,k,max_new_tokens,attention_mask):\n",
    "#     total_layers = len(model.model.layers)\n",
    "#     gpu_stream = torch.cuda.current_stream()\n",
    "#     if layers_per_chunk >= total_layers:\n",
    "#         # model = model.to('cuda')\n",
    "#         # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "#         # return out.argmax(-1)\n",
    "#         i=0\n",
    "#         l=total_layers\n",
    "#         m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "#         x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         return x\n",
    "#     i = 0\n",
    "#     rem = total_layers\n",
    "#     while rem > 0:\n",
    "#         l = min(layers_per_chunk, rem)\n",
    "        \n",
    "#         m = Module(model.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "#         # torch.cuda.current_stream().wait_stream(gpu_stream)\n",
    "#         # m=torch.compile(m)\n",
    "#         # Run chunk\n",
    "#         with torch.no_grad():\n",
    "#             x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         m=m.to('cpu')\n",
    "#         torch.cuda.empty_cache()\n",
    "#         # update indices\n",
    "#         i += l\n",
    "#         rem -= l\n",
    "#     torch.cuda.current_stream().synchronize()\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de68337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i=0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True), \n",
    "                              cos.to('cuda', non_blocking=True), \n",
    "                              sin.to('cuda', non_blocking=True), \n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            m_current = m_current.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_current = m_current.to('cpu', non_blocking=True)\n",
    "            del m_current\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        torch.cuda.empty_cache() \n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec619479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda') \n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,1,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb2f9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428e865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.548299551010132\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2460c40",
   "metadata": {},
   "source": [
    "# other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e71b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Loading new layers (stream_load)\n",
    "    Worker 2: Computation (stream_compute)\n",
    "    Worker 3: Offloading old layers (stream_offload)\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    m_to_offload = None # This will hold the chunk from the previous iteration to be offloaded\n",
    "    m_current = None # This is the chunk currently being computed\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    while i <= total_layers + layers_per_chunk: # Loop until all layers are processed and the last chunk is offloaded\n",
    "        # 1. Load the next chunk on a separate stream if there are layers left\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        \n",
    "        # 2. Wait for the current chunk to finish loading, then compute\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # 3. Wait for the computation to finish, then offload the previous chunk\n",
    "        if m_to_offload:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "        \n",
    "        # Advance the pipeline to the next stage\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6fb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birbirbirbirbirbirbirbirbirbir\n",
      "Time taken for 10 tokens: 4.10 seconds\n",
      "\n",
      "Total time 4.095994472503662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ae734",
   "metadata": {},
   "source": [
    "12.1\n",
    "11.9\n",
    "11.5\n",
    "11.9\n",
    "10.9\n",
    "11.1\n",
    "10.3\n",
    "9.5\n",
    "9.0 (10)\n",
    "9.3 (11)\n",
    "10.5 (12)\n",
    "9.8 (13)\n",
    "9.9 (14)\n",
    "9.1 (15)\n",
    "\n",
    "1.9 (16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "50c91944",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b454b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=nn.ModuleList(layers[i:j+1])\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        model = model.to('cuda')\n",
    "        out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        return out.argmax(-1)\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l)\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cpu'), cos.to('cpu'), sin.to('cpu'))\n",
    "\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "66c6d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "39589d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dcdfb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2278, -0.1952,  1.0512,  ...,  0.4470,  0.9027,  0.4017],\n",
       "         [-0.0171,  0.0437,  0.0796,  ..., -0.0353, -0.0933, -0.0494],\n",
       "         [-0.0076,  0.0305, -0.0894,  ..., -0.1101, -0.0858,  0.0215],\n",
       "         [-0.0955,  0.1930, -0.0288,  ..., -0.0590, -0.0255,  0.1503],\n",
       "         [-0.0718,  0.0508, -0.0462,  ...,  0.0025, -0.0664, -0.0365]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,7)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c06aaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        # model = model.to('cuda')\n",
    "        # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        # return out.argmax(-1)\n",
    "        i=0\n",
    "        l=total_layers\n",
    "        m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "        x=m(x.to('cuda'),cos.to('cuda'),sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l).to('cuda')\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c72c7af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,get_layers(model),cos,sin)\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "daf313c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d5247463e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "faf2a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4072, -0.0109,  0.1410,  ...,  0.2553, -0.2387, -0.3508],\n",
       "         [ 0.1307, -0.0781,  0.1387,  ...,  0.4531,  0.1098, -0.2780],\n",
       "         [ 0.1409,  0.0029,  0.2776,  ...,  0.8693,  0.6018, -0.1494],\n",
       "         [-0.0449,  0.0310,  0.0482,  ...,  0.3607, -0.3491, -0.5668],\n",
       "         [-0.0326,  0.3774, -0.2075,  ...,  0.3641, -0.2429, -0.2585]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5fd69379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4545,  2.3900, -1.2563,  ..., -1.2801, -1.2815, -1.2818],\n",
       "         [-0.5335,  1.2898, -0.0679,  ..., -0.9111, -0.9123, -0.9122],\n",
       "         [ 0.2582,  0.9092,  1.3728,  ..., -1.5733, -1.5758, -1.5756],\n",
       "         [-1.0597,  2.9733,  0.6937,  ..., -0.5993, -0.6006, -0.6005],\n",
       "         [ 0.2114,  0.6242,  1.3618,  ..., -0.0651, -0.0660, -0.0650]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98977648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   3\n",
      "4   7\n",
      "8   11\n",
      "12   15\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=3\n",
    "l=4\n",
    "rem=16\n",
    "while(rem!=0):\n",
    "    if(rem>=l):\n",
    "        m=nn.Sequential(\n",
    "            layers[i:j]\n",
    "        )\n",
    "        print(i,\" \",j)\n",
    "        rem-=l\n",
    "        i+=l;j+=l\n",
    "    else:\n",
    "        m=nn.Sequential(\n",
    "            layers[i:i+rem-1]\n",
    "        )\n",
    "        print('else')\n",
    "        print(i,\" \",i+rem-1)\n",
    "        rem=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b31e4353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ModuleList(\n",
       "    (0-2): 3 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.Sequential(\n",
    "        layers[0:3]\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ccb6",
   "metadata": {},
   "source": [
    "if int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 > total layers  : load whole model into sequential model\n",
    "\n",
    "else load int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 layers into the seq \n",
    "\n",
    "keep track of the layers which went inside the seq model\n",
    "\n",
    "[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] if 6-2=4 layers are allowed then iterations = while i not>= number of layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.814697265625e-06\n",
      "0.4892578125\n",
      "0.4892578125\n"
     ]
    }
   ],
   "source": [
    "# LlamaRMSNorm((2048,), eps=1e-05)\n",
    "#     (rotary_emb): LlamaRotaryEmbedding()\n",
    "# print((sum(p.numel() for p in model.model.norm.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.lm_head.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.model.embed_tokens.parameters())*2)/(1024**3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889b1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ea19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa53c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=input()\n",
    "converstation=[\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are an assitant help user by solving their query\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"query-{question}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe16aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "# prompt.to('cpu'//)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95944161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   6151,   1268,    527,    220]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d89c9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out=model.generate(**prompt,max_new_tokens=1000)\n",
    "# print(tok.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68deb58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79079cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=layers[i:j]\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "            print()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c373bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2181, -0.3543,  1.0459,  ...,  0.3998,  0.9639,  0.3820],\n",
       "         [ 0.0501, -0.0546, -0.0243,  ...,  0.1164, -0.1678,  0.0384],\n",
       "         [ 0.0353, -0.0942, -0.1205,  ..., -0.0287, -0.1063,  0.0035],\n",
       "         [ 0.0649,  0.0549, -0.0413,  ..., -0.0517, -0.0086,  0.0737],\n",
       "         [-0.0263,  0.0541, -0.0978,  ...,  0.0248, -0.0258, -0.0238]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,3)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7f6659f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
      "           8.4305e-04,  7.0190e-04],\n",
      "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
      "           9.5215e-03,  4.7363e-02],\n",
      "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
      "          -4.8828e-03,  1.2451e-02],\n",
      "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
      "          -1.5869e-02,  1.1520e-03],\n",
      "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
      "          -1.8677e-02, -3.5156e-02]]], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.5403,  0.7878,  0.9046,  0.9576,  0.9813,  0.9917,  0.9964,\n",
      "           0.9984,  0.9993,  0.9997,  0.9999,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.5403,  0.7878,  0.9046,\n",
      "           0.9576,  0.9813,  0.9917,  0.9964,  0.9984,  0.9993,  0.9997,\n",
      "           0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.4161,  0.2412,  0.6366,  0.8340,  0.9257,  0.9671,  0.9855,\n",
      "           0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.4161,  0.2412,  0.6366,\n",
      "           0.8340,  0.9257,  0.9671,  0.9855,  0.9936,  0.9972,  0.9988,\n",
      "           0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.9900, -0.4078,  0.2471,  0.6397,  0.8355,  0.9264,  0.9674,\n",
      "           0.9856,  0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.9900, -0.4078,  0.2471,\n",
      "           0.6397,  0.8355,  0.9264,  0.9674,  0.9856,  0.9936,  0.9972,\n",
      "           0.9988,  0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.6536, -0.8837, -0.1895,  0.3912,  0.7139,  0.8704,  0.9422,\n",
      "           0.9744,  0.9887,  0.9950,  0.9978,  0.9990,  0.9996,  0.9998,\n",
      "           0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.6536, -0.8837, -0.1895,\n",
      "           0.3912,  0.7139,  0.8704,  0.9422,  0.9744,  0.9887,  0.9950,\n",
      "           0.9978,  0.9990,  0.9996,  0.9998,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000]]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 8.4147e-01,  6.1596e-01,  4.2627e-01,  2.8809e-01,  1.9271e-01,\n",
      "           1.2833e-01,  8.5293e-02,  5.6639e-02,  3.7597e-02,  2.4953e-02,\n",
      "           1.6560e-02,  1.0989e-02,  7.2926e-03,  4.8394e-03,  3.2114e-03,\n",
      "           1.2905e-03,  4.2956e-04,  9.7083e-05,  1.9462e-05,  1.2915e-05,\n",
      "           8.5703e-06,  5.6872e-06,  3.7741e-06,  2.5045e-06,  1.6620e-06,\n",
      "           1.1029e-06,  7.3187e-07,  4.8567e-07,  3.2229e-07,  2.1387e-07,\n",
      "           1.4193e-07,  9.4183e-08,  8.4147e-01,  6.1596e-01,  4.2627e-01,\n",
      "           2.8809e-01,  1.9271e-01,  1.2833e-01,  8.5293e-02,  5.6639e-02,\n",
      "           3.7597e-02,  2.4953e-02,  1.6560e-02,  1.0989e-02,  7.2926e-03,\n",
      "           4.8394e-03,  3.2114e-03,  1.2905e-03,  4.2956e-04,  9.7083e-05,\n",
      "           1.9462e-05,  1.2915e-05,  8.5703e-06,  5.6872e-06,  3.7741e-06,\n",
      "           2.5045e-06,  1.6620e-06,  1.1029e-06,  7.3187e-07,  4.8567e-07,\n",
      "           3.2229e-07,  2.1387e-07,  1.4193e-07,  9.4183e-08],\n",
      "         [ 9.0930e-01,  9.7048e-01,  7.7121e-01,  5.5175e-01,  3.7819e-01,\n",
      "           2.5454e-01,  1.6997e-01,  1.1310e-01,  7.5141e-02,  4.9890e-02,\n",
      "           3.3115e-02,  2.1977e-02,  1.4585e-02,  9.6787e-03,  6.4228e-03,\n",
      "           2.5811e-03,  8.5911e-04,  1.9417e-04,  3.8923e-05,  2.5830e-05,\n",
      "           1.7141e-05,  1.1374e-05,  7.5481e-06,  5.0089e-06,  3.3239e-06,\n",
      "           2.2058e-06,  1.4637e-06,  9.7135e-07,  6.4459e-07,  4.2775e-07,\n",
      "           2.8385e-07,  1.8837e-07,  9.0930e-01,  9.7048e-01,  7.7121e-01,\n",
      "           5.5175e-01,  3.7819e-01,  2.5454e-01,  1.6997e-01,  1.1310e-01,\n",
      "           7.5141e-02,  4.9890e-02,  3.3115e-02,  2.1977e-02,  1.4585e-02,\n",
      "           9.6787e-03,  6.4228e-03,  2.5811e-03,  8.5911e-04,  1.9417e-04,\n",
      "           3.8923e-05,  2.5830e-05,  1.7141e-05,  1.1374e-05,  7.5481e-06,\n",
      "           5.0089e-06,  3.3239e-06,  2.2058e-06,  1.4637e-06,  9.7135e-07,\n",
      "           6.4459e-07,  4.2775e-07,  2.8385e-07,  1.8837e-07],\n",
      "         [ 1.4112e-01,  9.1309e-01,  9.6899e-01,  7.6862e-01,  5.4950e-01,\n",
      "           3.7654e-01,  2.5340e-01,  1.6919e-01,  1.1258e-01,  7.4796e-02,\n",
      "           4.9661e-02,  3.2963e-02,  2.1876e-02,  1.4518e-02,  9.6342e-03,\n",
      "           3.8716e-03,  1.2887e-03,  2.9125e-04,  5.8385e-05,  3.8744e-05,\n",
      "           2.5711e-05,  1.7062e-05,  1.1322e-05,  7.5134e-06,  4.9859e-06,\n",
      "           3.3087e-06,  2.1956e-06,  1.4570e-06,  9.6688e-07,  6.4162e-07,\n",
      "           4.2578e-07,  2.8255e-07,  1.4112e-01,  9.1309e-01,  9.6899e-01,\n",
      "           7.6862e-01,  5.4950e-01,  3.7654e-01,  2.5340e-01,  1.6919e-01,\n",
      "           1.1258e-01,  7.4796e-02,  4.9661e-02,  3.2963e-02,  2.1876e-02,\n",
      "           1.4518e-02,  9.6342e-03,  3.8716e-03,  1.2887e-03,  2.9125e-04,\n",
      "           5.8385e-05,  3.8744e-05,  2.5711e-05,  1.7062e-05,  1.1322e-05,\n",
      "           7.5134e-06,  4.9859e-06,  3.3087e-06,  2.1956e-06,  1.4570e-06,\n",
      "           9.6688e-07,  6.4162e-07,  4.2578e-07,  2.8255e-07],\n",
      "         [-7.5680e-01,  4.6814e-01,  9.8188e-01,  9.2033e-01,  7.0021e-01,\n",
      "           4.9232e-01,  3.3498e-01,  2.2474e-01,  1.4986e-01,  9.9656e-02,\n",
      "           6.6193e-02,  4.3944e-02,  2.9167e-02,  1.9356e-02,  1.2845e-02,\n",
      "           5.1622e-03,  1.7182e-03,  3.8833e-04,  7.7847e-05,  5.1659e-05,\n",
      "           3.4281e-05,  2.2749e-05,  1.5096e-05,  1.0018e-05,  6.6479e-06,\n",
      "           4.4115e-06,  2.9275e-06,  1.9427e-06,  1.2892e-06,  8.5550e-07,\n",
      "           5.6771e-07,  3.7673e-07, -7.5680e-01,  4.6814e-01,  9.8188e-01,\n",
      "           9.2033e-01,  7.0021e-01,  4.9232e-01,  3.3498e-01,  2.2474e-01,\n",
      "           1.4986e-01,  9.9656e-02,  6.6193e-02,  4.3944e-02,  2.9167e-02,\n",
      "           1.9356e-02,  1.2845e-02,  5.1622e-03,  1.7182e-03,  3.8833e-04,\n",
      "           7.7847e-05,  5.1659e-05,  3.4281e-05,  2.2749e-05,  1.5096e-05,\n",
      "           1.0018e-05,  6.6479e-06,  4.4115e-06,  2.9275e-06,  1.9427e-06,\n",
      "           1.2892e-06,  8.5550e-07,  5.6771e-07,  3.7673e-07]]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[117], line 11\u001b[0m, in \u001b[0;36mmodule.forward\u001b[0;34m(self, x, cos, sin)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sin)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m---> 11\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:287\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    286\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    291\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    292\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:61\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m---> 61\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m     62\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     63\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2f4d3c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0131, -0.0472,  0.0412,  ..., -0.0090,  0.0760, -0.0241],\n",
       "         [-0.0173, -0.0174, -0.0555,  ...,  0.0072, -0.0251, -0.0046],\n",
       "         [-0.0093,  0.0029, -0.1033,  ..., -0.0247, -0.0518,  0.0019],\n",
       "         [ 0.0227,  0.0492, -0.0206,  ..., -0.0301,  0.0070,  0.0488],\n",
       "         [-0.0341,  0.0040, -0.0712,  ..., -0.0127, -0.0116,  0.0161]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=model.model.layers[0]\n",
    "m(x.to('cpu'),position_embeddings=(cos.to('cpu'),sin.to('cpu')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1f23b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out=model(prompt['input_ids'].to('cpu'))\n",
    "m=layers[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d086ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e801d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d526b7b8e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(prompt['input_ids'].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57b0ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextt=out['logits'][:,-1,:].argmax(dim=-1)\n",
    "nextt\n",
    "tok.decode(nextt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "059c40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=prompt['input_ids']\n",
    "del prompt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2480b343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fadee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "# del input_ids\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebfa8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d91acbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.decoder.embed_positions.to('cuda')\n",
    "# x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "# model.model.decoder.embed_positions.to('cpu')\n",
    "# # del model.model.decoder.embed_positions\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976c6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's max_position_embeddings: 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's max_position_embeddings: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b082e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1f0ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b42bffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "cnt=0\n",
    "for i in model.model.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    i.to('cuda')\n",
    "    x.to('cuda')\n",
    "    # print(x)\n",
    "    with torch.no_grad():\n",
    "        x=i(x,position_embeddings=(cos,sin))\n",
    "        # print(x)\n",
    "    i.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    x=x[0]\n",
    "    # x.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "        # time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18251d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f129a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7762, -5.0166, 13.8534,  ..., -1.9152,  1.8989, -3.0755],\n",
       "         [-0.2314,  1.3026,  1.7797,  ..., -0.2708, -1.0364, -0.2083],\n",
       "         [ 0.2276,  0.8936,  0.8521,  ..., -0.0246, -1.4944, -0.0458],\n",
       "         [ 0.0748,  1.2204,  0.5945,  ..., -0.7305, -1.4607, -0.6370],\n",
       "         [ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc202f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1caa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.model.norm.to('cuda')\n",
    "    x=model.model.norm(x)\n",
    "    model.model.norm.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cuda')\n",
    "    x=model.lm_head(x)\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21f836c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aca9e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "560056fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fe40c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   6151,   1268,    527,    220,     17]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([input_ids,x1],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc43b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 "
     ]
    }
   ],
   "source": [
    "x1=x[:,-1,:]\n",
    "x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "print(tok.decode(x1[0]),end=\" \")\n",
    "# print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9cd9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device=torch.device('cuda:0')\n",
    "props=torch.cuda.get_device_properties('cuda')\n",
    "props.total_memory/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a20b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda')\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    del prompt\n",
    "    for i in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        input_ids.to('cpu')\n",
    "        # del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # Safe check\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        cnt=0\n",
    "        for i in model.model.layers:\n",
    "            cnt+=1\n",
    "            # print(cnt)\n",
    "            i.to('cuda')\n",
    "            x.to('cuda')\n",
    "            # print(x)\n",
    "            with torch.no_grad():\n",
    "                x=i(x,position_embeddings=(cos,sin))\n",
    "                # print(x)\n",
    "            i.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x=x[0]\n",
    "            # x.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "501122ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I'm a new user and I'm trying"
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9202f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(prompt_text, tokens=250):\n",
    "    # Initial tokenization\n",
    "    prompt = tok(prompt_text, return_tensors='pt')\n",
    "    input_ids = prompt['input_ids'].to('cuda')  # [1, seq_len]\n",
    "    del prompt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # Run only the latest token through embed layer and model\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for layer in model.model.layers:\n",
    "            layer.to('cuda')\n",
    "            x = x.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                x = layer(x, position_embeddings=(cos, sin))\n",
    "            layer.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x = x[0]  # Remove tuple\n",
    "\n",
    "        # Final norm and lm_head\n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x = model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            model.lm_head.to('cuda')\n",
    "            logits = model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]  # Only the last token matters\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # shape [1,1]\n",
    "\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Decode and print latest token\n",
    "        print(tok.decode(next_token[0]), end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506a694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am very happy to meet you all. I am a very friendly person and I love to meet new people. I am a very good listener and I am very easy going. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi how are you ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(prompt_text, tokens)\u001b[0m\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Only the last token matters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi how are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39d98000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_oov(text, tokenizer, max_token_id=2048, oov_token='[OOV]'):\n",
    "    # First, add [OOV] token to tokenizer if not present\n",
    "    if oov_token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': [oov_token]})\n",
    "\n",
    "    oov_id = tokenizer.convert_tokens_to_ids(oov_token)\n",
    "\n",
    "    # Raw tokenization\n",
    "    encoding = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "\n",
    "    # Replace tokens ≥ max_token_id with OOV\n",
    "    processed_ids = []\n",
    "    for token_id in input_ids:\n",
    "        if token_id >= max_token_id:\n",
    "            processed_ids.append(oov_id)\n",
    "        else:\n",
    "            processed_ids.append(token_id)\n",
    "\n",
    "    return torch.tensor([processed_ids]), encoding['attention_mask']\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    try:\n",
    "        for _ in range(tokens):\n",
    "            # prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "            # input_ids = prompt['input_ids']\n",
    "            # attention_mask = prompt['attention_mask']\n",
    "            input_ids, attention_mask = tokenize_with_oov(question, tok)\n",
    "\n",
    "            del prompt\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Truncate if too long\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            max_positions = model.config.max_position_embeddings\n",
    "            if seq_len > max_positions:\n",
    "                input_ids = input_ids[:, -max_positions:]\n",
    "                attention_mask = attention_mask[:, -max_positions:]\n",
    "                seq_len = input_ids.shape[1]\n",
    "\n",
    "            # Embeddings\n",
    "            model.model.decoder.embed_tokens.to('cpu')\n",
    "            x = model.model.decoder.embed_tokens(input_ids)\n",
    "\n",
    "            # Positional IDs\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device='cpu')\n",
    "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            model.model.decoder.embed_positions.to('cpu')\n",
    "            x = x + model.model.decoder.embed_positions(position_ids)\n",
    "\n",
    "            # Decoder layers\n",
    "            for layer in model.model.decoder.layers:\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                with torch.no_grad():\n",
    "                    x = layer(x, attention_mask=attention_mask.to(torch.float32))[0]\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Final LM head\n",
    "            model.lm_head.to('cpu')\n",
    "            logits = model.lm_head(x.to('cpu'))\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Decode\n",
    "            next_token_id = logits.argmax(-1)\n",
    "            next_token_text = tok.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            print(next_token_text, end=\" \")\n",
    "            question += \" \" + next_token_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n[Exception Caught]\")\n",
    "        print(\"Question So Far:\", question)\n",
    "        prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "        input_ids = prompt['input_ids']\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        y = torch.arange(x.shape[1]).unsqueeze(0)\n",
    "        print(\"Embedding Shape:\", x[0].shape)\n",
    "        print(\"Position ID Shape:\", y.shape)\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbc1b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy how are you 're post .''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy\n",
      "\n",
      "torch.Size([35, 768])\n",
      "torch.Size([1, 35])\n",
      "index out of range in self\n"
     ]
    }
   ],
   "source": [
    "model_inference(\"how are you 're post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Ensure model components are initially on CPU\n",
    "    model.model.decoder.embed_tokens.to('cpu')\n",
    "    model.model.decoder.embed_positions.to('cpu')\n",
    "    for layer in model.model.decoder.layers:\n",
    "        layer.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    \n",
    "    max_position_embeddings = model.config.max_position_embeddings\n",
    "    print(f\"Model's max_position_embeddings: {max_position_embeddings}\")\n",
    "\n",
    "    # Initialize input_ids_for_next_step. This will be the tensor that grows.\n",
    "    input_ids_for_next_step = tok(question, return_tensors='pt')['input_ids'] \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # The 'x' variable for the current iteration is the input_ids_for_next_step\n",
    "        current_input_ids = input_ids_for_next_step \n",
    "        current_seq_len = current_input_ids.shape[1]\n",
    "\n",
    "        # --- Truncation ---\n",
    "        if current_seq_len > max_position_embeddings:\n",
    "            current_input_ids = current_input_ids[:, current_seq_len - max_position_embeddings:]\n",
    "            print(f\"Warning: Sequence truncated from {current_seq_len} to {current_input_ids.shape[1]} for max_position_embeddings.\")\n",
    "            current_seq_len = current_input_ids.shape[1] # Update after truncation\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # Use current_input_ids for embedding lookup\n",
    "        x_embeddings = model.model.decoder.embed_tokens(current_input_ids.to('cpu')) # x_embeddings is on CPU\n",
    "        model.model.decoder.embed_tokens.to('cpu') # Move layer back\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Add Positional Embeddings\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        try:\n",
    "            position_offset = model.model.decoder.embed_positions.offset\n",
    "        except AttributeError:\n",
    "            position_offset = 2 # Common default for OPT\n",
    "\n",
    "        pos_ids = torch.arange(position_offset, position_offset + current_seq_len, device='cpu').unsqueeze(0) \n",
    "        \n",
    "        # The main `x` variable throughout the layer processing will be `current_token_embeddings_plus_pos`\n",
    "        current_token_embeddings_plus_pos = x_embeddings + model.model.decoder.embed_positions(pos_ids) \n",
    "        model.model.decoder.embed_positions.to('cpu') \n",
    "        del x_embeddings \n",
    "        del pos_ids \n",
    "        gc.collect()\n",
    "\n",
    "        # --- Decoder Layers Loop ---\n",
    "        # Rename 'x' to something that reflects its current state (embeddings in, embeddings out)\n",
    "        x_current_layer_output = current_token_embeddings_plus_pos # Start of layer processing\n",
    "        for j, layer in enumerate(model.model.decoder.layers):\n",
    "            try:\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    x_current_layer_output = layer(x_current_layer_output)[0]\n",
    "\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "                \n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder layer {j}: {e}\")\n",
    "                break\n",
    "        else: \n",
    "            # --- LM Head and Token Generation ---\n",
    "            model.lm_head.to('cpu')\n",
    "            # The input to lm_head is x_current_layer_output (the final embeddings from decoder)\n",
    "            logits = model.lm_head(x_current_layer_output.to('cpu')) \n",
    "            model.lm_head.to('cpu') \n",
    "            \n",
    "            logits = logits.to('cpu') # Ensure logits are on CPU\n",
    "            gc.collect()\n",
    "\n",
    "            # Get the predicted token ID (on CPU)\n",
    "            # Take the last token's prediction from batch 0\n",
    "            predicted_token_id = logits.argmax(-1)[:, -1].item() \n",
    "\n",
    "            decoded_token = tok.decode(predicted_token_id)\n",
    "            print(decoded_token, end=\" \")\n",
    "            \n",
    "            # --- Prepare input_ids_for_next_step for the next iteration ---\n",
    "            # Append the new token ID to the input_ids_for_next_step tensor (on CPU)\n",
    "            new_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long, device='cpu')\n",
    "            \n",
    "            # This is the correct concatenation for input_ids\n",
    "            input_ids_for_next_step = torch.cat((input_ids_for_next_step, new_token_tensor), dim=1)\n",
    "            \n",
    "            # Update the 'question' string for printing and future reference\n",
    "            question = question + \" \" + decoded_token\n",
    "            \n",
    "        if 'predicted_token_id' not in locals(): # If inner loop broke, stop outer loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e86df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    for i in range(tokens):\n",
    "        prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "        input_ids=prompt['input_ids']\n",
    "        del prompt\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cuda')\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # del model.model.decoder.embed_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_positions.to('cuda')\n",
    "        x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        # del model.model.decoder.embed_positions\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in model.model.decoder.layers:\n",
    "            try:\n",
    "                i.to('cuda')\n",
    "                x=x.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    x=i(x)[0]\n",
    "                i.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                x=x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                # time.sleep(3)\n",
    "            except:\n",
    "                print(x)\n",
    "                break\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x.to('cuda'))\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x.argmax(-1)\n",
    "        print(tok.decode(x1[0]),end=\" \")\n",
    "        question=question+\" \"+tok.decode(x1[0])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35778bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. your retarded're .''. you retarded're .dylib mom retard.''. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:76\u001b[0m, in \u001b[0;36mOPTLearnedPositionalEmbedding.forward\u001b[0;34m(self, attention_mask, past_key_values_length, position_ids)\u001b[0m\n\u001b[1;32m     74\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids[:, past_key_values_length:]\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m]),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         question\u001b[38;5;241m=\u001b[39mquestion\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43me\u001b[49m:\n\u001b[1;32m     48\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mtok(question,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mprompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "model_inference(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ce7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "cnt=0\n",
    "for i in model.model.decoder.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    try:\n",
    "        i.to('cuda')\n",
    "        x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            x=i(x)[0]\n",
    "        i.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # time.sleep(3)\n",
    "    except:\n",
    "        print(cnt)\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33d7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.to('cuda')\n",
    "x=model.lm_head(x.to('cuda'))\n",
    "model.lm_head.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e12f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f7d35ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d2228",
   "metadata": {},
   "source": [
    "# 3 Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first chunk\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        # Wait for transfer to complete before starting computation\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "        # Move pointers\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        # Offload previous chunk to CPU (if exists)\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If this is the last chunk, break\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            # Offload the final chunk\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Load next chunk\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "       \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8292bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.665896415710449\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf827a7c",
   "metadata": {},
   "source": [
    "# 4 Workers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5dc92640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "411b3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory/param_size_GB)-drop)//2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        # print(self.model.device)\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            # print(layer.device)\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first two chunks\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "        \n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "    while True:\n",
    "    # Wait for previous transfer to complete before compute\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        # Move pointers forward\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    cos.to('cuda', non_blocking=True),\n",
    "                    sin.to('cuda', non_blocking=True),\n",
    "                    attention_mask.to('cuda', non_blocking=True)\n",
    "                )\n",
    "\n",
    "        # Offload the previous chunk after compute is done\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If no more chunks left to load after this, finish\n",
    "        if rem == 0 and m_next is None and m_next2 is None:\n",
    "            # Wait for compute to complete\n",
    "            stream_compute.synchronize()\n",
    "\n",
    "            # Offload the current module\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Start preloading next-next chunk if any\n",
    "        if rem > 0:\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            stream_transfer2.wait_stream(stream_transfer)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "    return x\n",
    "    # while True:\n",
    "    #     # Wait for transfer to complete before starting computation\n",
    "    #     stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "    #     # Move pointers\n",
    "    #     m_previous = m_current\n",
    "    #     m_current = m_next\n",
    "    #     m_next = m_next2\n",
    "    #     m_next2 = None\n",
    "\n",
    "    #     # Compute on current chunk\n",
    "    #     with torch.cuda.stream(stream_compute):\n",
    "    #         with torch.no_grad():\n",
    "    #             x = m_current(x.to('cuda', non_blocking=True),\n",
    "    #                           cos.to('cuda', non_blocking=True),\n",
    "    #                           sin.to('cuda', non_blocking=True),\n",
    "    #                           attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "    #     # Offload previous chunk to CPU (if exists)\n",
    "    #     if m_previous is not None:\n",
    "    #         stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "    #         with torch.cuda.stream(stream_offload):\n",
    "    #             m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "    #             del m_previous\n",
    "    #         torch.cuda.empty_cache()\n",
    "\n",
    "    #     # If this is the last chunk, break\n",
    "    #     if rem == 0:\n",
    "    #         stream_compute.synchronize()\n",
    "    #         # Offload the final chunk\n",
    "    #         with torch.cuda.stream(stream_offload):\n",
    "    #             m_current = m_current.to('cpu', non_blocking=True)\n",
    "    #         stream_offload.synchronize()\n",
    "    #         torch.cuda.empty_cache()\n",
    "    #         break\n",
    "\n",
    "    #     # Load next chunk (2 steps ahead)\n",
    "    #     # if rem > 0:\n",
    "    #     #     time.sleep(0.01) \n",
    "    #     #     l_next2 = min(layers_per_chunk, rem)\n",
    "    #     #     with torch.cuda.stream(stream_transfer2):\n",
    "    #     #         m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "    #     #     time.sleep(0.1)\n",
    "    #     #     i += l_next2\n",
    "    #     #     rem -= l_next2\n",
    "    #     if rem > 0:\n",
    "    #         l_next2 = min(layers_per_chunk, rem)\n",
    "\n",
    "    #         # Create CUDA events for synchronization\n",
    "    #         transfer_start = torch.cuda.Event(blocking=False)\n",
    "    #         transfer_end = torch.cuda.Event(blocking=True)  # blocking=True ensures CPU waits for it\n",
    "\n",
    "    #         # Record the start of the transfer on the stream\n",
    "    #         stream_transfer2.record_event(transfer_start)\n",
    "\n",
    "    #         with torch.cuda.stream(stream_transfer2):\n",
    "    #             m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "    #             stream_transfer2.record_event(transfer_end)  # Record when transfer is done\n",
    "\n",
    "    #         # Wait until transfer is completed before proceeding\n",
    "    #         transfer_end.synchronize()  # 100% guaranteed sync\n",
    "\n",
    "    #         # Advance the layer index\n",
    "    #         i += l_next2\n",
    "    #         rem -= l_next2\n",
    "\n",
    "    # return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu',non_blocking=True)\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "    model.model.norm.to('cuda',non_blocking=True)\n",
    "    model.lm_head.to('cuda',non_blocking=True)\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,6,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda',non_blocking=True)\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda',non_blocking=True)\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "26fedf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? I am a 19 year old girl who\n",
      "Total time 3.690537452697754\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI how are you\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9005d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model, drop=2):\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024 ** 3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory / param_size_GB) - drop) // 2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers = model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        time.sleep(5.01)  # delay 1\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        time.sleep(5.01)  # delay 2\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            time.sleep(5.01)  # delay 3\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        time.sleep(5.01)  # delay 4\n",
    "\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        time.sleep(5.01)  # delay 5\n",
    "\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            time.sleep(5.01)  # delay 6\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        time.sleep(5.01)  # delay 7\n",
    "\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            time.sleep(5.01)  # delay 8\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            time.sleep(5.01)  # delay 9\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            time.sleep(8)  # delay 10\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            time.sleep(8)  # delay 11\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        if rem > 0:\n",
    "            print(\"runing test\")\n",
    "            time.sleep(15)  # delay 12\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            time.sleep(15)  # delay 13\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    prompt = tok(question, return_tensors='pt').to('cpu', non_blocking=True)\n",
    "    input_ids = prompt['input_ids'].to('cuda', non_blocking=True)\n",
    "    attention_mask = prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    time.sleep(5.01)  # delay 14\n",
    "\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "\n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(5.01)  # delay 15\n",
    "\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "        time.sleep(5.01)  # delay 16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            x = compute(model, x, 9, cos, sin, k, tokens, attention_mask)\n",
    "            time.sleep(5.01)  # delay 17\n",
    "\n",
    "            x = model.model.norm(x)\n",
    "            time.sleep(5.01)  # delay 18\n",
    "\n",
    "            x = model.lm_head(x)\n",
    "            time.sleep(5.01)  # delay 19\n",
    "\n",
    "        x1 = x[:, -1, :]\n",
    "        x1 = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        print(tok.decode(x1[0]), end=\"\")\n",
    "        input_ids = torch.cat([input_ids, x1], dim=-1)\n",
    "        time.sleep(5.01)  # delay 20\n",
    "\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236073b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 11:36:46.319881 52923 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI nonetheless"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "Cell \u001b[0;32mIn[3], line 130\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tokens):\n\u001b[1;32m    129\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5.01\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# delay 15\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m    133\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7659fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
