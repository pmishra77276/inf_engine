{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad3c95a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d83bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cuda')\n",
    "question=\"hi\"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "out=model.generate(**prompt,max_new_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d57f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out=model.generate(**prompt,max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccc7fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   6151,   1268,    527,    499,     30,    602,    574,  20910,\n",
       "           422,    499,   1436,   1520,    757,    449,   2555,     11,    602,\n",
       "           617,    264,    220,   2550,     22,    220,     19],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef9e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, I have a problem with my 3D\n"
     ]
    }
   ],
   "source": [
    "# If out is a tensor (e.g., shape [1, seq_len])\n",
    "decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08669892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cpu')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47aff853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28b28405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in model.model.layers:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1334cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "bytes_per_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e186b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2265777587890625"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=model.model.layers[0]\n",
    "num_params = sum(p.numel() for p in l.parameters())\n",
    "num_params\n",
    "param_size_bytes = num_params * 4\n",
    "param_size_MB = param_size_bytes / (1024 ** 3)\n",
    "param_size_MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "679c19b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f41edcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec0798e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers=model.model.layers\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed31bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=layers[:-1]\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e69f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 4\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a34a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaed17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n",
      "Warning: Not enough GPU memory. Available: 7.61 GB. Falling back to 1 layer per chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 17:45:32.001540 23607 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [32,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [33,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [34,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [35,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [36,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [37,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [38,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [39,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [40,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [41,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [42,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [43,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [44,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [45,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [50,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [51,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [54,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [55,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [58,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [59,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [23,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [64,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [65,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [66,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [67,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [68,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [69,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [70,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [71,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [72,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [73,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [74,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [75,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [76,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [77,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [78,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [79,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [80,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [81,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [82,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [83,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [84,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [85,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [86,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [87,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [88,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [89,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [90,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [91,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [92,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [93,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [94,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [95,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [96,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [97,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [98,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [99,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [100,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [101,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [102,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [103,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [104,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [105,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [106,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [107,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [108,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [109,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [110,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [111,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [112,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [113,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [114,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [115,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [116,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [117,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [118,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [119,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [120,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [121,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [122,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [123,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [124,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [125,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [126,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
      "/pytorch/aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [25,0,0], thread: [127,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 237\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 237\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[1], line 215\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens, temperature, top_p)\u001b[0m\n\u001b[1;32m    212\u001b[0m logits[sorted_indices[sorted_indices_to_remove]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Sample a token from the nucleus\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m next_token \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(next_token[\u001b[38;5;241m0\u001b[39m]), end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    218\u001b[0m generated_text_tokens\u001b[38;5;241m.\u001b[39mappend(tok\u001b[38;5;241m.\u001b[39mdecode(next_token[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with three streams\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk on the load stream\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    rem -= layers_per_chunk\n",
    "\n",
    "    m_next = None\n",
    "    \n",
    "    while True:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        if rem > 0:\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        else:\n",
    "            m_next = None\n",
    "\n",
    "        # Wait for the current chunk to finish loading\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "\n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "\n",
    "        # Check if this is the last chunk\n",
    "        if rem <= 0:\n",
    "            break\n",
    "        \n",
    "        # Offload the current chunk asynchronously\n",
    "        with torch.cuda.stream(stream_offload):\n",
    "            m_current.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline and update counters\n",
    "        m_current = m_next\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    # The last m_current must be moved back to cpu explicitly\n",
    "    m_current.to('cpu')\n",
    "    del m_current\n",
    "    if m_next:\n",
    "        del m_next\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250, temperature=0.7, top_p=0.95):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            # Use the calculated l1 value, not a hardcoded number\n",
    "            x = compute(model, x, l1, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Apply temperature and Top-P sampling\n",
    "        logits = x[:, -1, :] / temperature\n",
    "        \n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        \n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Find the tokens that are in the nucleus\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        \n",
    "        # Shift the indices to the right to keep at least one token\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        # Mask out the logits that are not in the nucleus\n",
    "        logits[sorted_indices[sorted_indices_to_remove]] = -float('inf')\n",
    "        \n",
    "        # Sample a token from the nucleus\n",
    "        next_token = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52f60ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d74102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae0c469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    # Initialize offload module to None for the first iteration\n",
    "    m_to_offload = None\n",
    "    \n",
    "    while i <= total_layers:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        # Wait for the current chunk to finish loading before computing\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        \n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # Offload the previously used chunk (if it exists)\n",
    "        if m_to_offload:\n",
    "            # Wait for compute to finish before offloading to prevent race condition\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Handle the final cleanup outside the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    # Ensure the last computed module is offloaded as well\n",
    "    if m_to_offload:\n",
    "        m_to_offload.to('cpu')\n",
    "    if m_current: # Should be None but for safety\n",
    "        m_current.to('cpu')\n",
    "    if m_next:\n",
    "        m_next.to('cpu')\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9db5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centerslyphlyphlyphlyphlyphlyphlyphlyphlyph\n",
      "Time taken for 10 tokens: 9.55 seconds\n",
      "\n",
      "Total time 9.552647829055786\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa1fd1",
   "metadata": {},
   "source": [
    "# 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce76b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229fb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b06fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e80fb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1e3aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "layers=model.model.layers\n",
    "\n",
    "\n",
    "# Usage\n",
    "# compiled_block = torch.compile(FusedBlock(layers[i:j]))\n",
    "\n",
    "# def compute(model, x, layers_per_chunk, cos, sin,k,max_new_tokens,attention_mask):\n",
    "#     total_layers = len(model.model.layers)\n",
    "#     gpu_stream = torch.cuda.current_stream()\n",
    "#     if layers_per_chunk >= total_layers:\n",
    "#         # model = model.to('cuda')\n",
    "#         # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "#         # return out.argmax(-1)\n",
    "#         i=0\n",
    "#         l=total_layers\n",
    "#         m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "#         x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         return x\n",
    "#     i = 0\n",
    "#     rem = total_layers\n",
    "#     while rem > 0:\n",
    "#         l = min(layers_per_chunk, rem)\n",
    "        \n",
    "#         m = Module(model.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "#         # torch.cuda.current_stream().wait_stream(gpu_stream)\n",
    "#         # m=torch.compile(m)\n",
    "#         # Run chunk\n",
    "#         with torch.no_grad():\n",
    "#             x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         m=m.to('cpu')\n",
    "#         torch.cuda.empty_cache()\n",
    "#         # update indices\n",
    "#         i += l\n",
    "#         rem -= l\n",
    "#     torch.cuda.current_stream().synchronize()\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de68337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i=0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True), \n",
    "                              cos.to('cuda', non_blocking=True), \n",
    "                              sin.to('cuda', non_blocking=True), \n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            m_current = m_current.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_current = m_current.to('cpu', non_blocking=True)\n",
    "            del m_current\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        torch.cuda.empty_cache() \n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec619479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda') \n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,1,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb2f9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "428e865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.548299551010132\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2460c40",
   "metadata": {},
   "source": [
    "# other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e71b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Loading new layers (stream_load)\n",
    "    Worker 2: Computation (stream_compute)\n",
    "    Worker 3: Offloading old layers (stream_offload)\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    m_to_offload = None # This will hold the chunk from the previous iteration to be offloaded\n",
    "    m_current = None # This is the chunk currently being computed\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    while i <= total_layers + layers_per_chunk: # Loop until all layers are processed and the last chunk is offloaded\n",
    "        # 1. Load the next chunk on a separate stream if there are layers left\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        \n",
    "        # 2. Wait for the current chunk to finish loading, then compute\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # 3. Wait for the computation to finish, then offload the previous chunk\n",
    "        if m_to_offload:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "        \n",
    "        # Advance the pipeline to the next stage\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6fb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birbirbirbirbirbirbirbirbirbir\n",
      "Time taken for 10 tokens: 4.10 seconds\n",
      "\n",
      "Total time 4.095994472503662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ae734",
   "metadata": {},
   "source": [
    "12.1\n",
    "11.9\n",
    "11.5\n",
    "11.9\n",
    "10.9\n",
    "11.1\n",
    "10.3\n",
    "9.5\n",
    "9.0 (10)\n",
    "9.3 (11)\n",
    "10.5 (12)\n",
    "9.8 (13)\n",
    "9.9 (14)\n",
    "9.1 (15)\n",
    "\n",
    "1.9 (16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "50c91944",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "b454b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=nn.ModuleList(layers[i:j+1])\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        model = model.to('cuda')\n",
    "        out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        return out.argmax(-1)\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l)\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cpu'), cos.to('cpu'), sin.to('cpu'))\n",
    "\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "66c6d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "39589d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dcdfb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2278, -0.1952,  1.0512,  ...,  0.4470,  0.9027,  0.4017],\n",
       "         [-0.0171,  0.0437,  0.0796,  ..., -0.0353, -0.0933, -0.0494],\n",
       "         [-0.0076,  0.0305, -0.0894,  ..., -0.1101, -0.0858,  0.0215],\n",
       "         [-0.0955,  0.1930, -0.0288,  ..., -0.0590, -0.0255,  0.1503],\n",
       "         [-0.0718,  0.0508, -0.0462,  ...,  0.0025, -0.0664, -0.0365]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,7)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c06aaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        # model = model.to('cuda')\n",
    "        # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        # return out.argmax(-1)\n",
    "        i=0\n",
    "        l=total_layers\n",
    "        m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "        x=m(x.to('cuda'),cos.to('cuda'),sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l).to('cuda')\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c72c7af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,get_layers(model),cos,sin)\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "daf313c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d5247463e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "faf2a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4072, -0.0109,  0.1410,  ...,  0.2553, -0.2387, -0.3508],\n",
       "         [ 0.1307, -0.0781,  0.1387,  ...,  0.4531,  0.1098, -0.2780],\n",
       "         [ 0.1409,  0.0029,  0.2776,  ...,  0.8693,  0.6018, -0.1494],\n",
       "         [-0.0449,  0.0310,  0.0482,  ...,  0.3607, -0.3491, -0.5668],\n",
       "         [-0.0326,  0.3774, -0.2075,  ...,  0.3641, -0.2429, -0.2585]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5fd69379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4545,  2.3900, -1.2563,  ..., -1.2801, -1.2815, -1.2818],\n",
       "         [-0.5335,  1.2898, -0.0679,  ..., -0.9111, -0.9123, -0.9122],\n",
       "         [ 0.2582,  0.9092,  1.3728,  ..., -1.5733, -1.5758, -1.5756],\n",
       "         [-1.0597,  2.9733,  0.6937,  ..., -0.5993, -0.6006, -0.6005],\n",
       "         [ 0.2114,  0.6242,  1.3618,  ..., -0.0651, -0.0660, -0.0650]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98977648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   3\n",
      "4   7\n",
      "8   11\n",
      "12   15\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=3\n",
    "l=4\n",
    "rem=16\n",
    "while(rem!=0):\n",
    "    if(rem>=l):\n",
    "        m=nn.Sequential(\n",
    "            layers[i:j]\n",
    "        )\n",
    "        print(i,\" \",j)\n",
    "        rem-=l\n",
    "        i+=l;j+=l\n",
    "    else:\n",
    "        m=nn.Sequential(\n",
    "            layers[i:i+rem-1]\n",
    "        )\n",
    "        print('else')\n",
    "        print(i,\" \",i+rem-1)\n",
    "        rem=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b31e4353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ModuleList(\n",
       "    (0-2): 3 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.Sequential(\n",
    "        layers[0:3]\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ccb6",
   "metadata": {},
   "source": [
    "if int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 > total layers  : load whole model into sequential model\n",
    "\n",
    "else load int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 layers into the seq \n",
    "\n",
    "keep track of the layers which went inside the seq model\n",
    "\n",
    "[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] if 6-2=4 layers are allowed then iterations = while i not>= number of layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.814697265625e-06\n",
      "0.4892578125\n",
      "0.4892578125\n"
     ]
    }
   ],
   "source": [
    "# LlamaRMSNorm((2048,), eps=1e-05)\n",
    "#     (rotary_emb): LlamaRotaryEmbedding()\n",
    "# print((sum(p.numel() for p in model.model.norm.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.lm_head.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.model.embed_tokens.parameters())*2)/(1024**3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "889b1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ea19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa53c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=input()\n",
    "converstation=[\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are an assitant help user by solving their query\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"query-{question}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe16aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "# prompt.to('cpu'//)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95944161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   6151,   1268,    527,    220]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d89c9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out=model.generate(**prompt,max_new_tokens=1000)\n",
    "# print(tok.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68deb58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79079cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=layers[i:j]\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "            print()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c373bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2181, -0.3543,  1.0459,  ...,  0.3998,  0.9639,  0.3820],\n",
       "         [ 0.0501, -0.0546, -0.0243,  ...,  0.1164, -0.1678,  0.0384],\n",
       "         [ 0.0353, -0.0942, -0.1205,  ..., -0.0287, -0.1063,  0.0035],\n",
       "         [ 0.0649,  0.0549, -0.0413,  ..., -0.0517, -0.0086,  0.0737],\n",
       "         [-0.0263,  0.0541, -0.0978,  ...,  0.0248, -0.0258, -0.0238]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,3)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7f6659f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
      "           8.4305e-04,  7.0190e-04],\n",
      "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
      "           9.5215e-03,  4.7363e-02],\n",
      "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
      "          -4.8828e-03,  1.2451e-02],\n",
      "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
      "          -1.5869e-02,  1.1520e-03],\n",
      "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
      "          -1.8677e-02, -3.5156e-02]]], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.5403,  0.7878,  0.9046,  0.9576,  0.9813,  0.9917,  0.9964,\n",
      "           0.9984,  0.9993,  0.9997,  0.9999,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.5403,  0.7878,  0.9046,\n",
      "           0.9576,  0.9813,  0.9917,  0.9964,  0.9984,  0.9993,  0.9997,\n",
      "           0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.4161,  0.2412,  0.6366,  0.8340,  0.9257,  0.9671,  0.9855,\n",
      "           0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.4161,  0.2412,  0.6366,\n",
      "           0.8340,  0.9257,  0.9671,  0.9855,  0.9936,  0.9972,  0.9988,\n",
      "           0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.9900, -0.4078,  0.2471,  0.6397,  0.8355,  0.9264,  0.9674,\n",
      "           0.9856,  0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.9900, -0.4078,  0.2471,\n",
      "           0.6397,  0.8355,  0.9264,  0.9674,  0.9856,  0.9936,  0.9972,\n",
      "           0.9988,  0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.6536, -0.8837, -0.1895,  0.3912,  0.7139,  0.8704,  0.9422,\n",
      "           0.9744,  0.9887,  0.9950,  0.9978,  0.9990,  0.9996,  0.9998,\n",
      "           0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.6536, -0.8837, -0.1895,\n",
      "           0.3912,  0.7139,  0.8704,  0.9422,  0.9744,  0.9887,  0.9950,\n",
      "           0.9978,  0.9990,  0.9996,  0.9998,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000]]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 8.4147e-01,  6.1596e-01,  4.2627e-01,  2.8809e-01,  1.9271e-01,\n",
      "           1.2833e-01,  8.5293e-02,  5.6639e-02,  3.7597e-02,  2.4953e-02,\n",
      "           1.6560e-02,  1.0989e-02,  7.2926e-03,  4.8394e-03,  3.2114e-03,\n",
      "           1.2905e-03,  4.2956e-04,  9.7083e-05,  1.9462e-05,  1.2915e-05,\n",
      "           8.5703e-06,  5.6872e-06,  3.7741e-06,  2.5045e-06,  1.6620e-06,\n",
      "           1.1029e-06,  7.3187e-07,  4.8567e-07,  3.2229e-07,  2.1387e-07,\n",
      "           1.4193e-07,  9.4183e-08,  8.4147e-01,  6.1596e-01,  4.2627e-01,\n",
      "           2.8809e-01,  1.9271e-01,  1.2833e-01,  8.5293e-02,  5.6639e-02,\n",
      "           3.7597e-02,  2.4953e-02,  1.6560e-02,  1.0989e-02,  7.2926e-03,\n",
      "           4.8394e-03,  3.2114e-03,  1.2905e-03,  4.2956e-04,  9.7083e-05,\n",
      "           1.9462e-05,  1.2915e-05,  8.5703e-06,  5.6872e-06,  3.7741e-06,\n",
      "           2.5045e-06,  1.6620e-06,  1.1029e-06,  7.3187e-07,  4.8567e-07,\n",
      "           3.2229e-07,  2.1387e-07,  1.4193e-07,  9.4183e-08],\n",
      "         [ 9.0930e-01,  9.7048e-01,  7.7121e-01,  5.5175e-01,  3.7819e-01,\n",
      "           2.5454e-01,  1.6997e-01,  1.1310e-01,  7.5141e-02,  4.9890e-02,\n",
      "           3.3115e-02,  2.1977e-02,  1.4585e-02,  9.6787e-03,  6.4228e-03,\n",
      "           2.5811e-03,  8.5911e-04,  1.9417e-04,  3.8923e-05,  2.5830e-05,\n",
      "           1.7141e-05,  1.1374e-05,  7.5481e-06,  5.0089e-06,  3.3239e-06,\n",
      "           2.2058e-06,  1.4637e-06,  9.7135e-07,  6.4459e-07,  4.2775e-07,\n",
      "           2.8385e-07,  1.8837e-07,  9.0930e-01,  9.7048e-01,  7.7121e-01,\n",
      "           5.5175e-01,  3.7819e-01,  2.5454e-01,  1.6997e-01,  1.1310e-01,\n",
      "           7.5141e-02,  4.9890e-02,  3.3115e-02,  2.1977e-02,  1.4585e-02,\n",
      "           9.6787e-03,  6.4228e-03,  2.5811e-03,  8.5911e-04,  1.9417e-04,\n",
      "           3.8923e-05,  2.5830e-05,  1.7141e-05,  1.1374e-05,  7.5481e-06,\n",
      "           5.0089e-06,  3.3239e-06,  2.2058e-06,  1.4637e-06,  9.7135e-07,\n",
      "           6.4459e-07,  4.2775e-07,  2.8385e-07,  1.8837e-07],\n",
      "         [ 1.4112e-01,  9.1309e-01,  9.6899e-01,  7.6862e-01,  5.4950e-01,\n",
      "           3.7654e-01,  2.5340e-01,  1.6919e-01,  1.1258e-01,  7.4796e-02,\n",
      "           4.9661e-02,  3.2963e-02,  2.1876e-02,  1.4518e-02,  9.6342e-03,\n",
      "           3.8716e-03,  1.2887e-03,  2.9125e-04,  5.8385e-05,  3.8744e-05,\n",
      "           2.5711e-05,  1.7062e-05,  1.1322e-05,  7.5134e-06,  4.9859e-06,\n",
      "           3.3087e-06,  2.1956e-06,  1.4570e-06,  9.6688e-07,  6.4162e-07,\n",
      "           4.2578e-07,  2.8255e-07,  1.4112e-01,  9.1309e-01,  9.6899e-01,\n",
      "           7.6862e-01,  5.4950e-01,  3.7654e-01,  2.5340e-01,  1.6919e-01,\n",
      "           1.1258e-01,  7.4796e-02,  4.9661e-02,  3.2963e-02,  2.1876e-02,\n",
      "           1.4518e-02,  9.6342e-03,  3.8716e-03,  1.2887e-03,  2.9125e-04,\n",
      "           5.8385e-05,  3.8744e-05,  2.5711e-05,  1.7062e-05,  1.1322e-05,\n",
      "           7.5134e-06,  4.9859e-06,  3.3087e-06,  2.1956e-06,  1.4570e-06,\n",
      "           9.6688e-07,  6.4162e-07,  4.2578e-07,  2.8255e-07],\n",
      "         [-7.5680e-01,  4.6814e-01,  9.8188e-01,  9.2033e-01,  7.0021e-01,\n",
      "           4.9232e-01,  3.3498e-01,  2.2474e-01,  1.4986e-01,  9.9656e-02,\n",
      "           6.6193e-02,  4.3944e-02,  2.9167e-02,  1.9356e-02,  1.2845e-02,\n",
      "           5.1622e-03,  1.7182e-03,  3.8833e-04,  7.7847e-05,  5.1659e-05,\n",
      "           3.4281e-05,  2.2749e-05,  1.5096e-05,  1.0018e-05,  6.6479e-06,\n",
      "           4.4115e-06,  2.9275e-06,  1.9427e-06,  1.2892e-06,  8.5550e-07,\n",
      "           5.6771e-07,  3.7673e-07, -7.5680e-01,  4.6814e-01,  9.8188e-01,\n",
      "           9.2033e-01,  7.0021e-01,  4.9232e-01,  3.3498e-01,  2.2474e-01,\n",
      "           1.4986e-01,  9.9656e-02,  6.6193e-02,  4.3944e-02,  2.9167e-02,\n",
      "           1.9356e-02,  1.2845e-02,  5.1622e-03,  1.7182e-03,  3.8833e-04,\n",
      "           7.7847e-05,  5.1659e-05,  3.4281e-05,  2.2749e-05,  1.5096e-05,\n",
      "           1.0018e-05,  6.6479e-06,  4.4115e-06,  2.9275e-06,  1.9427e-06,\n",
      "           1.2892e-06,  8.5550e-07,  5.6771e-07,  3.7673e-07]]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[117], line 11\u001b[0m, in \u001b[0;36mmodule.forward\u001b[0;34m(self, x, cos, sin)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sin)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m---> 11\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:287\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    286\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    291\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    292\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:61\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m---> 61\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m     62\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     63\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2f4d3c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0131, -0.0472,  0.0412,  ..., -0.0090,  0.0760, -0.0241],\n",
       "         [-0.0173, -0.0174, -0.0555,  ...,  0.0072, -0.0251, -0.0046],\n",
       "         [-0.0093,  0.0029, -0.1033,  ..., -0.0247, -0.0518,  0.0019],\n",
       "         [ 0.0227,  0.0492, -0.0206,  ..., -0.0301,  0.0070,  0.0488],\n",
       "         [-0.0341,  0.0040, -0.0712,  ..., -0.0127, -0.0116,  0.0161]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=model.model.layers[0]\n",
    "m(x.to('cpu'),position_embeddings=(cos.to('cpu'),sin.to('cpu')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c1f23b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out=model(prompt['input_ids'].to('cpu'))\n",
    "m=layers[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9d086ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e801d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d526b7b8e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(prompt['input_ids'].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "57b0ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextt=out['logits'][:,-1,:].argmax(dim=-1)\n",
    "nextt\n",
    "tok.decode(nextt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "059c40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=prompt['input_ids']\n",
    "del prompt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2480b343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fadee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "# del input_ids\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ebfa8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d91acbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.decoder.embed_positions.to('cuda')\n",
    "# x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "# model.model.decoder.embed_positions.to('cpu')\n",
    "# # del model.model.decoder.embed_positions\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "976c6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's max_position_embeddings: 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's max_position_embeddings: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b082e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1f0ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b42bffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "cnt=0\n",
    "for i in model.model.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    i.to('cuda')\n",
    "    x.to('cuda')\n",
    "    # print(x)\n",
    "    with torch.no_grad():\n",
    "        x=i(x,position_embeddings=(cos,sin))\n",
    "        # print(x)\n",
    "    i.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    x=x[0]\n",
    "    # x.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "        # time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18251d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f129a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7762, -5.0166, 13.8534,  ..., -1.9152,  1.8989, -3.0755],\n",
       "         [-0.2314,  1.3026,  1.7797,  ..., -0.2708, -1.0364, -0.2083],\n",
       "         [ 0.2276,  0.8936,  0.8521,  ..., -0.0246, -1.4944, -0.0458],\n",
       "         [ 0.0748,  1.2204,  0.5945,  ..., -0.7305, -1.4607, -0.6370],\n",
       "         [ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc202f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef1caa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.model.norm.to('cuda')\n",
    "    x=model.model.norm(x)\n",
    "    model.model.norm.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cuda')\n",
    "    x=model.lm_head(x)\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "21f836c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4aca9e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "560056fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fe40c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   6151,   1268,    527,    220,     17]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([input_ids,x1],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bc43b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 "
     ]
    }
   ],
   "source": [
    "x1=x[:,-1,:]\n",
    "x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "print(tok.decode(x1[0]),end=\" \")\n",
    "# print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a9cd9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device=torch.device('cuda:0')\n",
    "props=torch.cuda.get_device_properties('cuda')\n",
    "props.total_memory/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4a20b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda')\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    del prompt\n",
    "    for i in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        input_ids.to('cpu')\n",
    "        # del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # Safe check\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        cnt=0\n",
    "        for i in model.model.layers:\n",
    "            cnt+=1\n",
    "            # print(cnt)\n",
    "            i.to('cuda')\n",
    "            x.to('cuda')\n",
    "            # print(x)\n",
    "            with torch.no_grad():\n",
    "                x=i(x,position_embeddings=(cos,sin))\n",
    "                # print(x)\n",
    "            i.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x=x[0]\n",
    "            # x.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "501122ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I'm a new user and I'm trying"
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9202f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(prompt_text, tokens=250):\n",
    "    # Initial tokenization\n",
    "    prompt = tok(prompt_text, return_tensors='pt')\n",
    "    input_ids = prompt['input_ids'].to('cuda')  # [1, seq_len]\n",
    "    del prompt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # Run only the latest token through embed layer and model\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for layer in model.model.layers:\n",
    "            layer.to('cuda')\n",
    "            x = x.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                x = layer(x, position_embeddings=(cos, sin))\n",
    "            layer.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x = x[0]  # Remove tuple\n",
    "\n",
    "        # Final norm and lm_head\n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x = model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            model.lm_head.to('cuda')\n",
    "            logits = model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]  # Only the last token matters\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # shape [1,1]\n",
    "\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Decode and print latest token\n",
    "        print(tok.decode(next_token[0]), end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "506a694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am very happy to meet you all. I am a very friendly person and I love to meet new people. I am a very good listener and I am very easy going. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi how are you ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(prompt_text, tokens)\u001b[0m\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Only the last token matters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi how are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39d98000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_oov(text, tokenizer, max_token_id=2048, oov_token='[OOV]'):\n",
    "    # First, add [OOV] token to tokenizer if not present\n",
    "    if oov_token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': [oov_token]})\n",
    "\n",
    "    oov_id = tokenizer.convert_tokens_to_ids(oov_token)\n",
    "\n",
    "    # Raw tokenization\n",
    "    encoding = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "\n",
    "    # Replace tokens ≥ max_token_id with OOV\n",
    "    processed_ids = []\n",
    "    for token_id in input_ids:\n",
    "        if token_id >= max_token_id:\n",
    "            processed_ids.append(oov_id)\n",
    "        else:\n",
    "            processed_ids.append(token_id)\n",
    "\n",
    "    return torch.tensor([processed_ids]), encoding['attention_mask']\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    try:\n",
    "        for _ in range(tokens):\n",
    "            # prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "            # input_ids = prompt['input_ids']\n",
    "            # attention_mask = prompt['attention_mask']\n",
    "            input_ids, attention_mask = tokenize_with_oov(question, tok)\n",
    "\n",
    "            del prompt\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Truncate if too long\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            max_positions = model.config.max_position_embeddings\n",
    "            if seq_len > max_positions:\n",
    "                input_ids = input_ids[:, -max_positions:]\n",
    "                attention_mask = attention_mask[:, -max_positions:]\n",
    "                seq_len = input_ids.shape[1]\n",
    "\n",
    "            # Embeddings\n",
    "            model.model.decoder.embed_tokens.to('cpu')\n",
    "            x = model.model.decoder.embed_tokens(input_ids)\n",
    "\n",
    "            # Positional IDs\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device='cpu')\n",
    "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            model.model.decoder.embed_positions.to('cpu')\n",
    "            x = x + model.model.decoder.embed_positions(position_ids)\n",
    "\n",
    "            # Decoder layers\n",
    "            for layer in model.model.decoder.layers:\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                with torch.no_grad():\n",
    "                    x = layer(x, attention_mask=attention_mask.to(torch.float32))[0]\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Final LM head\n",
    "            model.lm_head.to('cpu')\n",
    "            logits = model.lm_head(x.to('cpu'))\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Decode\n",
    "            next_token_id = logits.argmax(-1)\n",
    "            next_token_text = tok.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            print(next_token_text, end=\" \")\n",
    "            question += \" \" + next_token_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n[Exception Caught]\")\n",
    "        print(\"Question So Far:\", question)\n",
    "        prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "        input_ids = prompt['input_ids']\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        y = torch.arange(x.shape[1]).unsqueeze(0)\n",
    "        print(\"Embedding Shape:\", x[0].shape)\n",
    "        print(\"Position ID Shape:\", y.shape)\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dbc1b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy how are you 're post .''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy\n",
      "\n",
      "torch.Size([35, 768])\n",
      "torch.Size([1, 35])\n",
      "index out of range in self\n"
     ]
    }
   ],
   "source": [
    "model_inference(\"how are you 're post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Ensure model components are initially on CPU\n",
    "    model.model.decoder.embed_tokens.to('cpu')\n",
    "    model.model.decoder.embed_positions.to('cpu')\n",
    "    for layer in model.model.decoder.layers:\n",
    "        layer.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    \n",
    "    max_position_embeddings = model.config.max_position_embeddings\n",
    "    print(f\"Model's max_position_embeddings: {max_position_embeddings}\")\n",
    "\n",
    "    # Initialize input_ids_for_next_step. This will be the tensor that grows.\n",
    "    input_ids_for_next_step = tok(question, return_tensors='pt')['input_ids'] \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # The 'x' variable for the current iteration is the input_ids_for_next_step\n",
    "        current_input_ids = input_ids_for_next_step \n",
    "        current_seq_len = current_input_ids.shape[1]\n",
    "\n",
    "        # --- Truncation ---\n",
    "        if current_seq_len > max_position_embeddings:\n",
    "            current_input_ids = current_input_ids[:, current_seq_len - max_position_embeddings:]\n",
    "            print(f\"Warning: Sequence truncated from {current_seq_len} to {current_input_ids.shape[1]} for max_position_embeddings.\")\n",
    "            current_seq_len = current_input_ids.shape[1] # Update after truncation\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # Use current_input_ids for embedding lookup\n",
    "        x_embeddings = model.model.decoder.embed_tokens(current_input_ids.to('cpu')) # x_embeddings is on CPU\n",
    "        model.model.decoder.embed_tokens.to('cpu') # Move layer back\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Add Positional Embeddings\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        try:\n",
    "            position_offset = model.model.decoder.embed_positions.offset\n",
    "        except AttributeError:\n",
    "            position_offset = 2 # Common default for OPT\n",
    "\n",
    "        pos_ids = torch.arange(position_offset, position_offset + current_seq_len, device='cpu').unsqueeze(0) \n",
    "        \n",
    "        # The main `x` variable throughout the layer processing will be `current_token_embeddings_plus_pos`\n",
    "        current_token_embeddings_plus_pos = x_embeddings + model.model.decoder.embed_positions(pos_ids) \n",
    "        model.model.decoder.embed_positions.to('cpu') \n",
    "        del x_embeddings \n",
    "        del pos_ids \n",
    "        gc.collect()\n",
    "\n",
    "        # --- Decoder Layers Loop ---\n",
    "        # Rename 'x' to something that reflects its current state (embeddings in, embeddings out)\n",
    "        x_current_layer_output = current_token_embeddings_plus_pos # Start of layer processing\n",
    "        for j, layer in enumerate(model.model.decoder.layers):\n",
    "            try:\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    x_current_layer_output = layer(x_current_layer_output)[0]\n",
    "\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "                \n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder layer {j}: {e}\")\n",
    "                break\n",
    "        else: \n",
    "            # --- LM Head and Token Generation ---\n",
    "            model.lm_head.to('cpu')\n",
    "            # The input to lm_head is x_current_layer_output (the final embeddings from decoder)\n",
    "            logits = model.lm_head(x_current_layer_output.to('cpu')) \n",
    "            model.lm_head.to('cpu') \n",
    "            \n",
    "            logits = logits.to('cpu') # Ensure logits are on CPU\n",
    "            gc.collect()\n",
    "\n",
    "            # Get the predicted token ID (on CPU)\n",
    "            # Take the last token's prediction from batch 0\n",
    "            predicted_token_id = logits.argmax(-1)[:, -1].item() \n",
    "\n",
    "            decoded_token = tok.decode(predicted_token_id)\n",
    "            print(decoded_token, end=\" \")\n",
    "            \n",
    "            # --- Prepare input_ids_for_next_step for the next iteration ---\n",
    "            # Append the new token ID to the input_ids_for_next_step tensor (on CPU)\n",
    "            new_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long, device='cpu')\n",
    "            \n",
    "            # This is the correct concatenation for input_ids\n",
    "            input_ids_for_next_step = torch.cat((input_ids_for_next_step, new_token_tensor), dim=1)\n",
    "            \n",
    "            # Update the 'question' string for printing and future reference\n",
    "            question = question + \" \" + decoded_token\n",
    "            \n",
    "        if 'predicted_token_id' not in locals(): # If inner loop broke, stop outer loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e86df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    for i in range(tokens):\n",
    "        prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "        input_ids=prompt['input_ids']\n",
    "        del prompt\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cuda')\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # del model.model.decoder.embed_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_positions.to('cuda')\n",
    "        x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        # del model.model.decoder.embed_positions\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in model.model.decoder.layers:\n",
    "            try:\n",
    "                i.to('cuda')\n",
    "                x=x.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    x=i(x)[0]\n",
    "                i.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                x=x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                # time.sleep(3)\n",
    "            except:\n",
    "                print(x)\n",
    "                break\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x.to('cuda'))\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x.argmax(-1)\n",
    "        print(tok.decode(x1[0]),end=\" \")\n",
    "        question=question+\" \"+tok.decode(x1[0])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35778bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. your retarded're .''. you retarded're .dylib mom retard.''. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:76\u001b[0m, in \u001b[0;36mOPTLearnedPositionalEmbedding.forward\u001b[0;34m(self, attention_mask, past_key_values_length, position_ids)\u001b[0m\n\u001b[1;32m     74\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids[:, past_key_values_length:]\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m]),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         question\u001b[38;5;241m=\u001b[39mquestion\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43me\u001b[49m:\n\u001b[1;32m     48\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mtok(question,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mprompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "model_inference(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ce7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "cnt=0\n",
    "for i in model.model.decoder.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    try:\n",
    "        i.to('cuda')\n",
    "        x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            x=i(x)[0]\n",
    "        i.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # time.sleep(3)\n",
    "    except:\n",
    "        print(cnt)\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33d7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.to('cuda')\n",
    "x=model.lm_head(x.to('cuda'))\n",
    "model.lm_head.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e12f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f7d35ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d2228",
   "metadata": {},
   "source": [
    "# 3 Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e7274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first chunk\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        # Wait for transfer to complete before starting computation\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "        # Move pointers\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        # Offload previous chunk to CPU (if exists)\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If this is the last chunk, break\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            # Offload the final chunk\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Load next chunk\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "       \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8292bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.665896415710449\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf827a7c",
   "metadata": {},
   "source": [
    "# 4 Workers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc92640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                          #  offload_folder='/offload_nvm',\n",
    "                                          #  offload_state_dict=True,\n",
    "                                          #  max_memory={\"cpu\":'0GB'},\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5edfeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: torch.Size([1, 3, 512])\n",
      "v: torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "inputs = tokenizer(\"This is\", return_tensors=\"pt\").to(\"cuda\")\n",
    "kv_store = {}\n",
    "\n",
    "def capture_kv(layer, inp, out):\n",
    "    x = inp[0]\n",
    "    attn = layer.self_attn\n",
    "    kv_store[0] = {\n",
    "        \"k\": attn.k_proj(x).detach().cpu(),\n",
    "        \"v\": attn.v_proj(x).detach().cpu()\n",
    "    }\n",
    "\n",
    "layer = model.model.layers[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "    pos_ids = torch.arange(x.shape[1], device=x.device).unsqueeze(0)\n",
    "    cos, sin = model.model.rotary_emb(x, pos_ids)\n",
    "\n",
    "    capture_kv(layer, (x,), None)\n",
    "    _ = layer(x, position_ids=pos_ids, position_embeddings=(cos, sin), use_cache=False)\n",
    "\n",
    "for k, v in kv_store[0].items():\n",
    "    print(f\"{k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "411b3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory/param_size_GB)-drop)//2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        # print(self.model.device)\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            # print(layer.device)\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "def slice_past(past_list, start, end):\n",
    "    return past_list[start:end] if past_list else None\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first two chunks\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "        \n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "    while True:\n",
    "    # Wait for previous transfer to complete before compute\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        # Move pointers forward\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    cos.to('cuda', non_blocking=True),\n",
    "                    sin.to('cuda', non_blocking=True),\n",
    "                    attention_mask.to('cuda', non_blocking=True)\n",
    "                )\n",
    "\n",
    "        # Offload the previous chunk after compute is done\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If no more chunks left to load after this, finish\n",
    "        if rem == 0 and m_next is None and m_next2 is None:\n",
    "            # Wait for compute to complete\n",
    "            stream_compute.synchronize()\n",
    "\n",
    "            # Offload the current module\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Start preloading next-next chunk if any\n",
    "        if rem > 0:\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            stream_transfer2.wait_stream(stream_transfer)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "    return x\n",
    "    # while True:\n",
    "    #     # Wait for transfer to complete before starting computation\n",
    "    #     stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "    #     # Move pointers\n",
    "    #     m_previous = m_current\n",
    "    #     m_current = m_next\n",
    "    #     m_next = m_next2\n",
    "    #     m_next2 = None\n",
    "\n",
    "    #     # Compute on current chunk\n",
    "    #     with torch.cuda.stream(stream_compute):\n",
    "    #         with torch.no_grad():\n",
    "    #             x = m_current(x.to('cuda', non_blocking=True),\n",
    "    #                           cos.to('cuda', non_blocking=True),\n",
    "    #                           sin.to('cuda', non_blocking=True),\n",
    "    #                           attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "    #     # Offload previous chunk to CPU (if exists)\n",
    "    #     if m_previous is not None:\n",
    "    #         stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "    #         with torch.cuda.stream(stream_offload):\n",
    "    #             m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "    #             del m_previous\n",
    "    #         torch.cuda.empty_cache()\n",
    "\n",
    "    #     # If this is the last chunk, break\n",
    "    #     if rem == 0:\n",
    "    #         stream_compute.synchronize()\n",
    "    #         # Offload the final chunk\n",
    "    #         with torch.cuda.stream(stream_offload):\n",
    "    #             m_current = m_current.to('cpu', non_blocking=True)\n",
    "    #         stream_offload.synchronize()\n",
    "    #         torch.cuda.empty_cache()\n",
    "    #         break\n",
    "\n",
    "    #     # Load next chunk (2 steps ahead)\n",
    "    #     # if rem > 0:\n",
    "    #     #     time.sleep(0.01) \n",
    "    #     #     l_next2 = min(layers_per_chunk, rem)\n",
    "    #     #     with torch.cuda.stream(stream_transfer2):\n",
    "    #     #         m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "    #     #     time.sleep(0.1)\n",
    "    #     #     i += l_next2\n",
    "    #     #     rem -= l_next2\n",
    "    #     if rem > 0:\n",
    "    #         l_next2 = min(layers_per_chunk, rem)\n",
    "\n",
    "    #         # Create CUDA events for synchronization\n",
    "    #         transfer_start = torch.cuda.Event(blocking=False)\n",
    "    #         transfer_end = torch.cuda.Event(blocking=True)  # blocking=True ensures CPU waits for it\n",
    "\n",
    "    #         # Record the start of the transfer on the stream\n",
    "    #         stream_transfer2.record_event(transfer_start)\n",
    "\n",
    "    #         with torch.cuda.stream(stream_transfer2):\n",
    "    #             m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "    #             stream_transfer2.record_event(transfer_end)  # Record when transfer is done\n",
    "\n",
    "    #         # Wait until transfer is completed before proceeding\n",
    "    #         transfer_end.synchronize()  # 100% guaranteed sync\n",
    "\n",
    "    #         # Advance the layer index\n",
    "    #         i += l_next2\n",
    "    #         rem -= l_next2\n",
    "\n",
    "    # return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu',non_blocking=True)\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "    model.model.norm.to('cuda',non_blocking=True)\n",
    "    model.lm_head.to('cuda',non_blocking=True)\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda',non_blocking=True)\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda',non_blocking=True)\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26fedf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? I'm a big fan of your work. I'm a big fan of your work. I'm a big fan of your work. I'm a big fan of your work. I'm a big fan of your work. I'm a big\n",
      "Total time 18.601060390472412\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI how are you\",50)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9005d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model, drop=2):\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024 ** 3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory / param_size_GB) - drop) // 2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers = model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        time.sleep(5.01)  # delay 1\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        time.sleep(5.01)  # delay 2\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            time.sleep(5.01)  # delay 3\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        time.sleep(5.01)  # delay 4\n",
    "\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        time.sleep(5.01)  # delay 5\n",
    "\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            time.sleep(5.01)  # delay 6\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        time.sleep(5.01)  # delay 7\n",
    "\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            time.sleep(5.01)  # delay 8\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            time.sleep(5.01)  # delay 9\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            time.sleep(8)  # delay 10\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            time.sleep(8)  # delay 11\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        if rem > 0:\n",
    "            print(\"runing test\")\n",
    "            time.sleep(15)  # delay 12\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            time.sleep(15)  # delay 13\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    prompt = tok(question, return_tensors='pt').to('cpu', non_blocking=True)\n",
    "    input_ids = prompt['input_ids'].to('cuda', non_blocking=True)\n",
    "    attention_mask = prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    time.sleep(5.01)  # delay 14\n",
    "\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "\n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(5.01)  # delay 15\n",
    "\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "        time.sleep(5.01)  # delay 16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            x = compute(model, x, 9, cos, sin, k, tokens, attention_mask)\n",
    "            time.sleep(5.01)  # delay 17\n",
    "\n",
    "            x = model.model.norm(x)\n",
    "            time.sleep(5.01)  # delay 18\n",
    "\n",
    "            x = model.lm_head(x)\n",
    "            time.sleep(5.01)  # delay 19\n",
    "\n",
    "        x1 = x[:, -1, :]\n",
    "        x1 = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        print(tok.decode(x1[0]), end=\"\")\n",
    "        input_ids = torch.cat([input_ids, x1], dim=-1)\n",
    "        time.sleep(5.01)  # delay 20\n",
    "\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236073b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 11:36:46.319881 52923 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI nonetheless"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "Cell \u001b[0;32mIn[3], line 130\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tokens):\n\u001b[1;32m    129\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5.01\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# delay 15\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m    133\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7659fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Input: Hello! How are you today?\n",
      "Output: "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaAttention' object has no attribute 'num_key_value_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 418\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    417\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 418\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 300\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, max_new_tokens)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Get attention config from the model\u001b[39;00m\n\u001b[1;32m    297\u001b[0m sample_attn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\n\u001b[1;32m    298\u001b[0m kv_cache \u001b[38;5;241m=\u001b[39m KVCache(\n\u001b[1;32m    299\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[0;32m--> 300\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[43msample_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_heads\u001b[49m,  \u001b[38;5;66;03m# Use KV heads for cache\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     head_dim\u001b[38;5;241m=\u001b[39msample_attn\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    302\u001b[0m     max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Move necessary components to GPU\u001b[39;00m\n\u001b[1;32m    306\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaAttention' object has no attribute 'num_key_value_heads'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory / param_size_GB) - drop) // 2\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self, num_layers):\n",
    "        self.num_layers = num_layers\n",
    "        # Store KV cache on CPU to save GPU memory\n",
    "        self.cache = {}\n",
    "        \n",
    "    def get_cache(self, layer_idx):\n",
    "        \"\"\"Get cache for a specific layer\"\"\"\n",
    "        return self.cache.get(layer_idx, None)\n",
    "    \n",
    "    def update_cache(self, layer_idx, k, v):\n",
    "        \"\"\"Update cache for a specific layer\"\"\"\n",
    "        if layer_idx not in self.cache:\n",
    "            self.cache[layer_idx] = {\"k\": k.detach().cpu(), \"v\": v.detach().cpu()}\n",
    "        else:\n",
    "            # Concatenate new k,v with existing cache along sequence dimension\n",
    "            existing_k = self.cache[layer_idx][\"k\"]\n",
    "            existing_v = self.cache[layer_idx][\"v\"]\n",
    "            self.cache[layer_idx][\"k\"] = torch.cat([existing_k, k.detach().cpu()], dim=2)  # Fixed: dim=2 for seq_len\n",
    "            self.cache[layer_idx][\"v\"] = torch.cat([existing_v, v.detach().cpu()], dim=2)  # Fixed: dim=2 for seq_len\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    \"\"\"Apply rotary positional embeddings to query and key tensors\"\"\"\n",
    "    batch_size = q.shape[0]\n",
    "    q_seq_len = q.shape[2]\n",
    "    k_seq_len = k.shape[2]\n",
    "    \n",
    "    # Get cos/sin for query positions\n",
    "    if position_ids.shape[1] == q_seq_len:\n",
    "        # Use provided position_ids directly\n",
    "        cos_q = cos[:, position_ids.squeeze(0), :].unsqueeze(1)  # [batch_size, 1, seq_len, head_dim]\n",
    "        sin_q = sin[:, position_ids.squeeze(0), :].unsqueeze(1)\n",
    "    else:\n",
    "        # Single token case\n",
    "        pos_idx = position_ids.squeeze()\n",
    "        cos_q = cos[:, pos_idx:pos_idx+1, :].unsqueeze(1)\n",
    "        sin_q = sin[:, pos_idx:pos_idx+1, :].unsqueeze(1)\n",
    "    \n",
    "    # Apply to query\n",
    "    q_embed = (q * cos_q) + (rotate_half(q) * sin_q)\n",
    "    \n",
    "    # For keys, handle full sequence\n",
    "    if k_seq_len == q_seq_len:\n",
    "        # Same sequence length, use same cos/sin\n",
    "        cos_k = cos_q\n",
    "        sin_k = sin_q\n",
    "    else:\n",
    "        # Different sequence length (cached keys), create full position sequence\n",
    "        start_pos = k_seq_len - q_seq_len\n",
    "        full_positions = torch.arange(start_pos, k_seq_len, device=k.device, dtype=torch.long)\n",
    "        cos_k = cos[:, full_positions, :].unsqueeze(1)\n",
    "        sin_k = sin[:, full_positions, :].unsqueeze(1)\n",
    "    \n",
    "    k_embed = (k * cos_k) + (rotate_half(k) * sin_k)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def capture_and_apply_kv(layer, x, layer_idx, kv_cache, position_embeddings, attention_mask, position_ids, is_first_token=False):\n",
    "    \"\"\"Capture K,V values and apply KV caching for a layer\"\"\"\n",
    "    cos, sin = position_embeddings\n",
    "    \n",
    "    # PRE-NORM: Apply input layer norm first\n",
    "    hidden_states = layer.input_layernorm(x)\n",
    "    \n",
    "    # Get attention module\n",
    "    attn = layer.self_attn\n",
    "    \n",
    "    # Model dimensions for Llama-3.2-1B\n",
    "    num_heads = 32\n",
    "    head_dim = 64\n",
    "    num_key_value_heads = 8\n",
    "    \n",
    "    # Compute Q, K, V projections\n",
    "    q = attn.q_proj(hidden_states)\n",
    "    k_new = attn.k_proj(hidden_states)\n",
    "    v_new = attn.v_proj(hidden_states)\n",
    "    \n",
    "    bsz, seq_len, _ = q.shape\n",
    "    \n",
    "    # Reshape for multi-head attention\n",
    "    q = q.view(bsz, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "    k_new = k_new.view(bsz, seq_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    v_new = v_new.view(bsz, seq_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    \n",
    "    # Handle KV caching - fixed to avoid position embedding issues\n",
    "    if is_first_token:\n",
    "        # First token - no existing cache, apply RoPE to all tokens\n",
    "        k_full = k_new\n",
    "        v_full = v_new\n",
    "        kv_cache.update_cache(layer_idx, k_new, v_new)\n",
    "        \n",
    "        # Apply rotary embeddings to q and k\n",
    "        if cos is not None and sin is not None:\n",
    "            q, k_full = apply_rotary_pos_emb(q, k_full, cos, sin, position_ids)\n",
    "    else:\n",
    "        # Get existing cache and concatenate\n",
    "        cached = kv_cache.get_cache(layer_idx)\n",
    "        if cached is not None:\n",
    "            k_cached = cached[\"k\"].to(k_new.device)\n",
    "            v_cached = cached[\"v\"].to(v_new.device)\n",
    "            \n",
    "            # Apply RoPE to new k tokens only\n",
    "            if cos is not None and sin is not None:\n",
    "                q, k_new = apply_rotary_pos_emb(q, k_new, cos, sin, position_ids)\n",
    "            \n",
    "            # Concatenate cached (already has RoPE) with new (just applied RoPE)\n",
    "            k_full = torch.cat([k_cached, k_new], dim=2)\n",
    "            v_full = torch.cat([v_cached, v_new], dim=2)\n",
    "            \n",
    "            # Update cache with the new tokens\n",
    "            kv_cache.update_cache(layer_idx, k_new, v_new)\n",
    "        else:\n",
    "            # No cache, treat as first token\n",
    "            k_full = k_new\n",
    "            v_full = v_new\n",
    "            kv_cache.update_cache(layer_idx, k_new, v_new)\n",
    "            if cos is not None and sin is not None:\n",
    "                q, k_full = apply_rotary_pos_emb(q, k_full, cos, sin, position_ids)\n",
    "    \n",
    "    # Expand k,v for grouped query attention (GQA: 8 KV heads -> 32 Q heads)\n",
    "    if num_key_value_heads != num_heads:\n",
    "        k_full = k_full.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "        v_full = v_full.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    attn_weights = torch.matmul(q, k_full.transpose(2, 3)) / torch.sqrt(torch.tensor(head_dim, dtype=q.dtype, device=q.device))\n",
    "    full_seq_len = k_full.shape[2]\n",
    "    # Apply causal mask - only mask future tokens\n",
    "    if full_seq_len > seq_len:\n",
    "        # We have cached tokens, create appropriate mask\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, full_seq_len, device=q.device, dtype=torch.bool))\n",
    "    else:\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=q.device, dtype=torch.bool))\n",
    "    \n",
    "    # Apply the mask\n",
    "    attn_weights = attn_weights.masked_fill(~causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    \n",
    "    # Apply softmax\n",
    "    attn_weights = torch.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    attn_output = torch.matmul(attn_weights, v_full)\n",
    "    \n",
    "    # Reshape back to [batch_size, seq_len, hidden_size]\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
    "    \n",
    "    # Apply output projection\n",
    "    attn_output = attn.o_proj(attn_output)\n",
    "    \n",
    "    # Add residual connection\n",
    "    hidden_states = x + attn_output\n",
    "    \n",
    "    # POST-NORM: Apply post attention layer norm\n",
    "    residual = hidden_states\n",
    "    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "    \n",
    "    # Apply MLP\n",
    "    mlp_output = layer.mlp(hidden_states)\n",
    "    \n",
    "    # Add residual connection\n",
    "    hidden_states = residual + mlp_output\n",
    "    \n",
    "    return hidden_states\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        self.start_idx = i\n",
    "    \n",
    "    def forward(self, x, cos, sin, attention_mask, kv_cache, position_ids, is_first_token=False):\n",
    "        for idx, layer in enumerate(self.model):\n",
    "            layer_idx = self.start_idx + idx\n",
    "            x = capture_and_apply_kv(layer, x, layer_idx, kv_cache, (cos, sin), attention_mask, position_ids, is_first_token)\n",
    "        return x\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask, kv_cache, position_ids, is_first_token=False):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'), kv_cache, position_ids.to('cuda'), is_first_token)\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first two chunks\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "        \n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "\n",
    "    while True:\n",
    "        # Wait for previous transfer to complete before compute\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        # Move pointers forward\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    cos.to('cuda', non_blocking=True),\n",
    "                    sin.to('cuda', non_blocking=True),\n",
    "                    attention_mask.to('cuda', non_blocking=True),\n",
    "                    kv_cache,\n",
    "                    position_ids.to('cuda', non_blocking=True),\n",
    "                    is_first_token\n",
    "                )\n",
    "\n",
    "        # Offload the previous chunk after compute is done\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If no more chunks left to load after this, finish\n",
    "        if rem == 0 and m_next is None and m_next2 is None:\n",
    "            # Wait for compute to complete\n",
    "            stream_compute.synchronize()\n",
    "\n",
    "            # Offload the current module\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Start preloading next-next chunk if any\n",
    "        if rem > 0:\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            stream_transfer2.wait_stream(stream_transfer)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "    return x\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Add proper prompt formatting for Llama\n",
    "    if not question.strip():\n",
    "        question = \"Hello! How are you today?\"\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = prompt['input_ids'].to('cuda', non_blocking=True)\n",
    "    attention_mask = prompt['attention_mask'].to('cuda', non_blocking=True)\n",
    "    \n",
    "    print(f\"Input: {question}\")\n",
    "    print(\"Output: \", end=\"\")\n",
    "    \n",
    "    # Initialize KV cache\n",
    "    num_layers = len(model.model.layers)\n",
    "    kv_cache = KVCache(num_layers)\n",
    "    \n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    \n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Determine if this is the first token generation\n",
    "        is_first_token = (k == 0)\n",
    "        \n",
    "        # For subsequent tokens, only process the last token\n",
    "        if not is_first_token:\n",
    "            current_input_ids = input_ids[:, -1:].contiguous()\n",
    "        else:\n",
    "            current_input_ids = input_ids\n",
    "            \n",
    "        x = model.model.embed_tokens(current_input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Position IDs for current token(s)\n",
    "        if is_first_token:\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        else:\n",
    "            # For subsequent tokens, position ID should be the current sequence length\n",
    "            current_pos = input_ids.shape[1] - 1\n",
    "            position_ids = torch.tensor([[current_pos]], dtype=torch.long, device=x.device)\n",
    "        \n",
    "        # Get rotary embeddings - simplified approach\n",
    "        max_seq_len = 4096  # Use a reasonable max length\n",
    "        dummy_x = torch.zeros(batch_size, max_seq_len, x.shape[-1], device=x.device, dtype=x.dtype)\n",
    "        dummy_position_ids = torch.arange(max_seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=dummy_x, position_ids=dummy_position_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            x = compute(model, x, 3, cos, sin, k, tokens, attention_mask, kv_cache, position_ids, is_first_token)\n",
    "            x = model.model.norm(x)\n",
    "            torch.cuda.empty_cache()\n",
    "            x = model.lm_head(x)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Get the logits for the last token\n",
    "        logits = x[:, -1, :]\n",
    "        \n",
    "        # Use temperature and top-p sampling for better generation\n",
    "        temperature = 0.7\n",
    "        top_p = 0.9\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Apply top-p filtering\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample from the filtered distribution\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Check for end of sequence\n",
    "        if next_token.item() == tok.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        print(tok.decode(next_token[0], skip_special_tokens=True), end=\"\", flush=True)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    \n",
    "    print()  # New line at the end\n",
    "    \n",
    "    # Cleanup\n",
    "    input_ids.to('cpu')\n",
    "    model.model.embed_tokens.to('cpu', non_blocking=True)\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    kv_cache.clear()\n",
    "\n",
    "# Load model and tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", torch_dtype=torch.bfloat16, use_cache=True)\n",
    "layers = model.model.layers\n",
    "\n",
    "# Test the model\n",
    "while True:\n",
    "    question = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
    "    if question.lower() == 'quit':\n",
    "        break\n",
    "    start = time.time()\n",
    "    model_inference(question, 50)  # Reduced tokens for testing\n",
    "    print(f\"Time taken: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6961788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Input: hi\n",
      "Output: "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaRotaryEmbedding.forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 360\u001b[0m\n\u001b[1;32m    358\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 360\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 292\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, max_new_tokens)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    291\u001b[0m     layers_per_chunk \u001b[38;5;241m=\u001b[39m get_layers(model, \u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_per_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    298\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[1], line 175\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(model_instance, x, layers_per_chunk, rotary_emb, position_ids, kv_cache, past_seq_len, k, max_new_tokens)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layers_per_chunk \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_layers:\n\u001b[1;32m    174\u001b[0m     m \u001b[38;5;241m=\u001b[39m Module(model_instance\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers, \u001b[38;5;241m0\u001b[39m, total_layers)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 175\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m max_new_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    177\u001b[0m         m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, rotary_emb, position_ids, kv_cache, past_seq_len)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    167\u001b[0m     layer_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_idx \u001b[38;5;241m+\u001b[39m idx\n\u001b[0;32m--> 168\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mllama_layer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[1], line 151\u001b[0m, in \u001b[0;36mllama_layer_forward\u001b[0;34m(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\u001b[0m\n\u001b[1;32m    149\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    150\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m--> 151\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mllama_attention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m attn_output\n\u001b[1;32m    153\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36mllama_attention_forward\u001b[0;34m(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\u001b[0m\n\u001b[1;32m    104\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, num_key_value_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Apply rotary position embeddings using the passed rotary_emb module\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Handle KV caching\u001b[39;00m\n\u001b[1;32m    110\u001b[0m cached_k, cached_v, cache_seq_len \u001b[38;5;241m=\u001b[39m kv_cache\u001b[38;5;241m.\u001b[39mget_cached_kv(layer_idx, key_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaRotaryEmbedding.forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Use bfloat16 for better performance on modern GPUs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Estimate the number of layers that can fit on the GPU.\n",
    "    This is a heuristic and may need adjustment.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2  # Assuming bfloat16 (2 bytes per parameter)\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Ensure at least one layer can fit and we don't return zero\n",
    "    return max(1, (int(device_memory / param_size_GB) - drop) // 2)\n",
    "\n",
    "class KVCache:\n",
    "    \"\"\"A simple KV cache implementation on the CPU to save GPU memory.\"\"\"\n",
    "    def __init__(self, num_layers, num_key_value_heads, head_dim, max_seq_len=4096):\n",
    "        self.num_layers = num_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cache = {}\n",
    "    \n",
    "    def get_cache(self, layer_idx):\n",
    "        \"\"\"Get cache for a specific layer.\"\"\"\n",
    "        return self.cache.get(layer_idx, None)\n",
    "    \n",
    "    def update_cache(self, layer_idx, k, v, seq_pos):\n",
    "        \"\"\"Update cache for a specific layer at specific sequence position.\"\"\"\n",
    "        if layer_idx not in self.cache:\n",
    "            batch_size = k.shape[0]\n",
    "            self.cache[layer_idx] = {\n",
    "                \"k\": torch.zeros(batch_size, self.num_key_value_heads, self.max_seq_len, self.head_dim, \n",
    "                                 dtype=k.dtype, device='cpu'),\n",
    "                \"v\": torch.zeros(batch_size, self.num_key_value_heads, self.max_seq_len, self.head_dim, \n",
    "                                 dtype=v.dtype, device='cpu'),\n",
    "                \"seq_len\": 0\n",
    "            }\n",
    "        \n",
    "        cache_entry = self.cache[layer_idx]\n",
    "        current_seq_len = k.shape[2]\n",
    "        \n",
    "        cache_entry[\"k\"][:, :, seq_pos : seq_pos + current_seq_len, :] = k.detach().cpu()\n",
    "        cache_entry[\"v\"][:, :, seq_pos : seq_pos + current_seq_len, :] = v.detach().cpu()\n",
    "        cache_entry[\"seq_len\"] = seq_pos + current_seq_len\n",
    "    \n",
    "    def get_cached_kv(self, layer_idx, device):\n",
    "        \"\"\"Get cached k,v up to current sequence length.\"\"\"\n",
    "        if layer_idx not in self.cache:\n",
    "            return None, None, 0\n",
    "            \n",
    "        cache_entry = self.cache[layer_idx]\n",
    "        seq_len = cache_entry[\"seq_len\"]\n",
    "        \n",
    "        if seq_len == 0:\n",
    "            return None, None, 0\n",
    "            \n",
    "        k_cached = cache_entry[\"k\"][:, :, :seq_len, :].to(device)\n",
    "        v_cached = cache_entry[\"v\"][:, :, :seq_len, :].to(device)\n",
    "        \n",
    "        return k_cached, v_cached, seq_len\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Removed apply_rotary_pos_emb as it will be handled directly by the rotary_emb module\n",
    "\n",
    "def llama_attention_forward(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len=0):\n",
    "    \"\"\"Custom forward pass for Llama attention with KV caching\"\"\"\n",
    "    attn = layer.self_attn\n",
    "    \n",
    "    bsz, q_len, _ = hidden_states.shape\n",
    "    \n",
    "    # Hardcoded values based on Llama-3.2-1B model\n",
    "    num_heads = 32\n",
    "    num_key_value_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Project to Q, K, V\n",
    "    query_states = attn.q_proj(hidden_states)\n",
    "    key_states = attn.k_proj(hidden_states)\n",
    "    value_states = attn.v_proj(hidden_states)\n",
    "    \n",
    "    # Reshape for multi-head attention\n",
    "    query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    \n",
    "    # Apply rotary position embeddings using the passed rotary_emb module\n",
    "    query_states, key_states = rotary_emb(query_states, key_states, position_ids)\n",
    "    \n",
    "    # Handle KV caching\n",
    "    cached_k, cached_v, cache_seq_len = kv_cache.get_cached_kv(layer_idx, key_states.device)\n",
    "    \n",
    "    if cached_k is not None:\n",
    "        key_states = torch.cat([cached_k, key_states], dim=2)\n",
    "        value_states = torch.cat([cached_v, value_states], dim=2)\n",
    "    \n",
    "    kv_cache.update_cache(layer_idx, key_states[:, :, past_seq_len:, :], value_states[:, :, past_seq_len:, :], past_seq_len)\n",
    "    \n",
    "    # Repeat k/v heads if num_key_value_heads < num_heads (grouped query attention)\n",
    "    if num_key_value_heads != num_heads:\n",
    "        key_states = key_states.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "        value_states = value_states.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "    \n",
    "    # Compute attention\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "    \n",
    "    # Apply causal mask\n",
    "    seq_len_q = query_states.shape[2]\n",
    "    seq_len_k = key_states.shape[2]\n",
    "    \n",
    "    causal_mask = torch.full((seq_len_q, seq_len_k), float('-inf'), device=hidden_states.device, dtype=attn_weights.dtype)\n",
    "    causal_mask = torch.triu(causal_mask, diagonal=past_seq_len + 1)\n",
    "        \n",
    "    attn_weights = attn_weights + causal_mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    \n",
    "    # Reshape and apply output projection\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "    attn_output = attn.o_proj(attn_output)\n",
    "    \n",
    "    return attn_output\n",
    "\n",
    "def llama_layer_forward(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len=0):\n",
    "    \"\"\"Custom forward pass for a Llama layer\"\"\"\n",
    "    residual = hidden_states\n",
    "    hidden_states = layer.input_layernorm(hidden_states)\n",
    "    attn_output = llama_attention_forward(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\n",
    "    hidden_states = residual + attn_output\n",
    "    residual = hidden_states\n",
    "    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "    mlp_output = layer.mlp(hidden_states)\n",
    "    hidden_states = residual + mlp_output\n",
    "    return hidden_states\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        self.start_idx = i\n",
    "    \n",
    "    def forward(self, x, rotary_emb, position_ids, kv_cache, past_seq_len=0):\n",
    "        for idx, layer in enumerate(self.model):\n",
    "            layer_idx = self.start_idx + idx\n",
    "            x = llama_layer_forward(layer, x, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\n",
    "        return x\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, rotary_emb, position_ids, kv_cache, past_seq_len, k, max_new_tokens):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda', non_blocking=True)\n",
    "        x = m(x.to('cuda'), rotary_emb.to('cuda'), position_ids.to('cuda'), kv_cache, past_seq_len)\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_previous = None\n",
    "\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    rotary_emb.to('cuda', non_blocking=True),\n",
    "                    position_ids.to('cuda', non_blocking=True),\n",
    "                    kv_cache,\n",
    "                    past_seq_len\n",
    "                )\n",
    "\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        if rem > 0:\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "            i += l_next\n",
    "            rem -= l_next\n",
    "    return x\n",
    "\n",
    "def model_inference(question, max_new_tokens=50):\n",
    "    if not question.strip():\n",
    "        question = \"hi\"\n",
    "    \n",
    "    inputs = tok(question, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids'].to('cuda')\n",
    "    \n",
    "    print(f\"Input: {question}\")\n",
    "    print(\"Output: \", end=\"\", flush=True)\n",
    "    \n",
    "    num_layers = len(model.model.layers)\n",
    "    \n",
    "    # Hardcoded values for Llama-3.2-1B\n",
    "    num_key_value_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Create the KVCache with the correct hardcoded parameters\n",
    "    kv_cache = KVCache(\n",
    "        num_layers=num_layers,\n",
    "        num_key_value_heads=num_key_value_heads,\n",
    "        head_dim=head_dim,\n",
    "        max_seq_len=4096\n",
    "    )\n",
    "    \n",
    "    # Get the rotary_emb module from the main model.model object\n",
    "    # This is the correct way to access it, as it's a shared component\n",
    "    rotary_emb = model.model.rotary_emb.to('cuda', non_blocking=True)\n",
    "    \n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for k in range(max_new_tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        is_first_iteration = (k == 0)\n",
    "        \n",
    "        if is_first_iteration:\n",
    "            current_input_ids = input_ids\n",
    "            past_seq_len = 0\n",
    "        else:\n",
    "            current_input_ids = input_ids[:, -1:].contiguous()\n",
    "            past_seq_len = input_ids.shape[1] - 1\n",
    "        \n",
    "        hidden_states = model.model.embed_tokens(current_input_ids)\n",
    "        \n",
    "        seq_len = hidden_states.shape[1]\n",
    "        position_ids = torch.arange(past_seq_len, past_seq_len + seq_len, dtype=torch.long, device=hidden_states.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(hidden_states.shape[0], -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            layers_per_chunk = get_layers(model, 7)\n",
    "            hidden_states = compute(\n",
    "                model, hidden_states, layers_per_chunk, rotary_emb, position_ids, \n",
    "                kv_cache, past_seq_len, k, max_new_tokens\n",
    "            )\n",
    "            \n",
    "            hidden_states = model.model.norm(hidden_states)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            logits = model.lm_head(hidden_states)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        temperature = 0.7\n",
    "        top_p = 0.9\n",
    "        \n",
    "        if temperature > 0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "            next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        if next_token.item() == tok.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        token_text = tok.decode(next_token[0], skip_special_tokens=True)\n",
    "        print(token_text, end=\"\", flush=True)\n",
    "        generated_tokens.append(next_token.item())\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    model.model.embed_tokens.to('cpu', non_blocking=True)\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    rotary_emb.to('cpu', non_blocking=True) # Move rotary_emb back to CPU\n",
    "    torch.cuda.empty_cache()\n",
    "    kv_cache.clear()\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=None\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test the model\n",
    "question = \"hi\"\n",
    "start = time.time()\n",
    "model_inference(question, 50)\n",
    "print(f\"Time taken: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daea1348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: hi\n",
      "Output: "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 132\u001b[0m \u001b[43mcompute_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mcompute_stream\u001b[0;34m(model, tokenizer, input_ids, max_new_tokens)\u001b[0m\n\u001b[1;32m     92\u001b[0m end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m chunks\n\u001b[1;32m     93\u001b[0m chunk \u001b[38;5;241m=\u001b[39m LayerChunk(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers, start, \u001b[38;5;28mmin\u001b[39m(end, num_layers))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# offload\u001b[39;00m\n\u001b[1;32m     96\u001b[0m chunk\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m, in \u001b[0;36mLayerChunk.forward\u001b[0;34m(self, x, attn_mask, cos, sin, kv_cache, first_token)\u001b[0m\n\u001b[1;32m     46\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m i\n\u001b[1;32m     47\u001b[0m past \u001b[38;5;241m=\u001b[39m kv_cache\u001b[38;5;241m.\u001b[39mget(idx)\n\u001b[0;32m---> 48\u001b[0m out, present \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m     49\u001b[0m     x,\n\u001b[1;32m     50\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39m(cos, sin),\n\u001b[1;32m     51\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast,\n\u001b[1;32m     52\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     55\u001b[0m kv_cache\u001b[38;5;241m.\u001b[39mupdate(idx, present)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "\n",
    "# Enable TF32 on supported cards for faster matmuls\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def get_layers_per_chunk(model, drop_gb=2):\n",
    "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    # Estimate size of one layer in GB\n",
    "    example_layer = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in example_layer.parameters())\n",
    "    # Map dtype to bytes per element\n",
    "    dtype_size = torch.tensor([], dtype=model.dtype).element_size()\n",
    "    layer_size_gb = num_params * dtype_size / 1024**3\n",
    "    return max(1, int((total_mem_gb - drop_gb) // layer_size_gb))\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self, num_layers):\n",
    "        self.cache = [None] * num_layers\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.cache[idx]\n",
    "\n",
    "    def update(self, idx, present):\n",
    "        if self.cache[idx] is None:\n",
    "            self.cache[idx] = present\n",
    "        else:\n",
    "            k_old, v_old = self.cache[idx]\n",
    "            k_new, v_new = present\n",
    "            # concat on seq dim (-2)\n",
    "            self.cache[idx] = (torch.cat([k_old, k_new], dim=-2), torch.cat([v_old, v_new], dim=-2))\n",
    "\n",
    "    def clear(self):\n",
    "        self.cache = [None] * len(self.cache)\n",
    "\n",
    "class LayerChunk(nn.Module):\n",
    "    def __init__(self, layers, start, end):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers[start:end])\n",
    "        self.start = start\n",
    "\n",
    "    def forward(self, x, attn_mask, cos, sin, kv_cache, first_token):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            idx = self.start + i\n",
    "            past = kv_cache.get(idx)\n",
    "            out, present = layer(\n",
    "                x,\n",
    "                position_embeddings=(cos, sin),\n",
    "                past_key_value=past,\n",
    "                use_cache=True\n",
    "            )\n",
    "            x = out\n",
    "            kv_cache.update(idx, present)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_stream(model, tokenizer, input_ids, max_new_tokens=10):\n",
    "    device = torch.device('cuda')\n",
    "    batch, seq_len = input_ids.shape\n",
    "\n",
    "    # Prepare cache and move critical parts\n",
    "    num_layers = len(model.model.layers)\n",
    "    kv_cache = KVCache(num_layers)\n",
    "    model.model.embed_tokens.to(device)\n",
    "    model.model.norm.to(device)\n",
    "    model.lm_head.to(device)\n",
    "\n",
    "    rot_emb = model.model.rotary_emb\n",
    "    chunks = get_layers_per_chunk(model)\n",
    "\n",
    "    x = None\n",
    "    for step in range(max_new_tokens):\n",
    "        is_first = (step == 0)\n",
    "        cur_ids = input_ids if is_first else input_ids[:, -1:]\n",
    "        x = model.model.embed_tokens(cur_ids.to(device))\n",
    "\n",
    "                # Rotary embeddings\n",
    "        cur_seq_len = cur_ids.shape[1]\n",
    "        if is_first:\n",
    "            pos_ids = torch.arange(cur_seq_len, device=device).unsqueeze(0).expand(batch, -1)\n",
    "        else:\n",
    "            pos_ids = torch.tensor([[seq_len-1]], device=device).expand(batch, 1)\n",
    "        cos, sin = rot_emb(x=x, position_ids=pos_ids)\n",
    "\n",
    "        # Process in chunks\n",
    "        rem = num_layers\n",
    "        start = 0\n",
    "        attn_mask = torch.ones_like(cur_ids, device=device)\n",
    "        while rem > 0:\n",
    "            end = start + chunks\n",
    "            chunk = LayerChunk(model.model.layers, start, min(end, num_layers)).to(device)\n",
    "            x = chunk(x, attn_mask, cos, sin, kv_cache, is_first)\n",
    "            # offload\n",
    "            chunk.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            start += chunks\n",
    "            rem -= chunks\n",
    "\n",
    "        x = model.model.norm(x)\n",
    "        logits = model.lm_head(x)\n",
    "        nxt = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        print(tokenizer.decode(nxt[0]), end='', flush=True)\n",
    "        input_ids = torch.cat([input_ids.to(device), nxt], dim=-1)\n",
    "        seq_len += 1\n",
    "\n",
    "    print()\n",
    "    # Cleanup\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    kv_cache.clear()\n",
    "\n",
    "\n",
    "def main():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Single-shot prompt = \"hi\"\n",
    "    prompt = \"hi\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Output:\", end=' ')\n",
    "    start = time.time()\n",
    "    compute_stream(model, tokenizer, inputs.input_ids, max_new_tokens=10)\n",
    "    print(f\"Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d94296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
