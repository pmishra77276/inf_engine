{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3c95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d83bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cuda')\n",
    "question=\"hi\"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "out=model.generate(**prompt,max_new_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d57f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out=model.generate(**prompt,max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc7fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   6151,     11,    358,   1097,   3411,    369,   1063,   1520,\n",
       "           449,    264,   3488], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, I am looking for some help with a question\n"
     ]
    }
   ],
   "source": [
    "decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08669892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cpu')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47aff853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28b28405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in model.model.layers:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1334cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "bytes_per_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e186b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2265777587890625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=model.model.layers[0]\n",
    "num_params = sum(p.numel() for p in l.parameters())\n",
    "num_params\n",
    "param_size_bytes = num_params * 4\n",
    "param_size_MB = param_size_bytes / (1024 ** 3)\n",
    "param_size_MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679c19b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f41edcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec0798e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers=model.model.layers\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed31bb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers=layers[:-1]\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e69f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 4\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a34a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaed17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n",
      "Warning: Not enough GPU memory. Available: 7.61 GB. Falling back to 1 layer per chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_worker/__main__.py\", line 10, in <module>\n",
      "    from torch._inductor.async_compile import pre_fork_setup\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/async_compile.py\", line 17, in <module>\n",
      "    from torch._dynamo.device_interface import get_registered_device_interfaces\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/__init__.py\", line 13, in <module>\n",
      "    from . import config, convert_frame, eval_frame, resume_execution\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 52, in <module>\n",
      "    from torch._dynamo.symbolic_convert import TensorifyState\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 52, in <module>\n",
      "    from torch._dynamo.exc import TensorifyScalarRestartAnalysis\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 41, in <module>\n",
      "    from .utils import counters\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 69, in <module>\n",
      "    import torch.fx.experimental.symbolic_shapes\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 67, in <module>\n",
      "    from torch.utils._sympy.functions import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/utils/_sympy/functions.py\", line 9, in <module>\n",
      "    import sympy\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/__init__.py\", line 74, in <module>\n",
      "    from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/__init__.py\", line 124, in <module>\n",
      "    from .partfrac import apart, apart_list, assemble_partfrac_list\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/partfrac.py\", line 15, in <module>\n",
      "    def apart(f, x=None, full=False, **options):\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/utilities/decorator.py\", line 76, in xthreaded\n",
      "    return threaded_factory(func, False)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/utilities/decorator.py\", line 13, in threaded_factory\n",
      "    from sympy.matrices import MatrixBase\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/__init__.py\", line 8, in <module>\n",
      "    from .dense import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/dense.py\", line 14, in <module>\n",
      "    from .matrixbase import MatrixBase\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/matrixbase.py\", line 60, in <module>\n",
      "    from .solvers import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/solvers.py\", line 6, in <module>\n",
      "    from .eigen import _fuzzy_positive_definite\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/eigen.py\", line 15, in <module>\n",
      "    from sympy.polys.matrices.eigen import dom_eigenvects, dom_eigenvects_to_sympy\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/matrices/eigen.py\", line 8, in <module>\n",
      "    from ..agca.extensions import FiniteExtension\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/agca/__init__.py\", line 3, in <module>\n",
      "    from .homomorphisms import homomorphism\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/agca/homomorphisms.py\", line 10, in <module>\n",
      "    from sympy.polys.agca.modules import (Module, FreeModule, QuotientModule,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/agca/modules.py\", line 24, in <module>\n",
      "    from sympy.polys.agca.ideals import Ideal\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 237\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 237\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[28], line 190\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens, temperature, top_p)\u001b[0m\n\u001b[1;32m    188\u001b[0m l1 \u001b[38;5;241m=\u001b[39m get_layers(model, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Use the calculated l1 value, not a hardcoded number\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "Cell \u001b[0;32mIn[28], line 127\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mstream(stream_compute):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 127\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mm_current\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Check if this is the last chunk\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rem \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1432\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1426\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1427\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1428\u001b[0m             )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1213\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1211\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:598\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    595\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1059\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m   1057\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     put_code_state()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_utils_internal.py:97\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# This is not needed but we have it here to avoid having profile_compile_time\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# in stack traces when profiling is not enabled.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m    100\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    101\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:761\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    759\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39minstall_callbacks())\n\u001b[1;32m    760\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    764\u001b[0m     ConvertFrameReturn()\n\u001b[1;32m    765\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:797\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    795\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1422\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1419\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1420\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1422\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:257\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:715\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 715\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    717\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3498\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3498\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1337\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1337\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1338\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1246\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3699\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 3699\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3684\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3679\u001b[0m _step_logger()(\n\u001b[1;32m   3680\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   3681\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3682\u001b[0m )\n\u001b[1;32m   3683\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 3684\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   3688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3690\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3691\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   3694\u001b[0m )\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1144\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1141\u001b[0m append_prefix_insts()\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_replacements\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m   1148\u001b[0m )\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m   1151\u001b[0m     [\n\u001b[1;32m   1152\u001b[0m         PyCodegen(tx, overridden_sources\u001b[38;5;241m=\u001b[39moverridden_sources)\u001b[38;5;241m.\u001b[39mcreate_store(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     ]\n\u001b[1;32m   1157\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1437\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root, replaced_outputs)\u001b[0m\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1437\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1487\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1483\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1484\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1485\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1486\u001b[0m     ):\n\u001b[0;32m-> 1487\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1519\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1518\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1519\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1520\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:150\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/__init__.py:2347\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2088\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m   2082\u001b[0m     V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode),\n\u001b[1;32m   2083\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(tracing_context),\n\u001b[1;32m   2084\u001b[0m     compiled_autograd\u001b[38;5;241m.\u001b[39m_disable(),\n\u001b[1;32m   2085\u001b[0m     functorch_config\u001b[38;5;241m.\u001b[39mpatch(unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   2086\u001b[0m ):\n\u001b[1;32m   2087\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2088\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m            \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2097\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2098\u001b[0m         \u001b[38;5;66;03m# We will also shorten the traceback inside dynamo.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m         \u001b[38;5;66;03m# This is only useful if inductor is called directly with an FX graph.\u001b[39;00m\n\u001b[1;32m   2100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:101\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m--> 101\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1168\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local \u001b[38;5;129;01mor\u001b[39;00m remote:\n\u001b[1;32m   1167\u001b[0m     set_feature_use(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_remote_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, remote)\n\u001b[0;32m-> 1168\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mAOTAutogradCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdispatch_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m dispatch_and_compile()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:695\u001b[0m, in \u001b[0;36mAOTAutogradCache.load\u001b[0;34m(dispatch_and_compile, mod, args, aot_config, cudagraphs, local, remote)\u001b[0m\n\u001b[1;32m    693\u001b[0m fx_config: _CompileFxKwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m: cudagraphs}\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     cache_key, debug_lines \u001b[38;5;241m=\u001b[39m \u001b[43mautograd_cache_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m     entry: Optional[AOTAutogradCacheEntry] \u001b[38;5;241m=\u001b[39m AOTAutogradCache\u001b[38;5;241m.\u001b[39m_lookup(\n\u001b[1;32m    699\u001b[0m         cache_key, local, remote\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:331\u001b[0m, in \u001b[0;36mautograd_cache_key\u001b[0;34m(gm, example_inputs, config, fx_config)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m triton\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m<\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BypassAOTAutogradCache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAOTAutogradCache requires triton 3.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m details \u001b[38;5;241m=\u001b[39m \u001b[43mAOTAutogradCacheDetails\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m pickler \u001b[38;5;241m=\u001b[39m AOTAutogradCachePickler(gm)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# The prefix distinguishes among the other kinds of objects we cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:264\u001b[0m, in \u001b[0;36mAOTAutogradCacheDetails.__init__\u001b[0;34m(self, gm, example_inputs, aot_config, fx_config)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# FXGraphCache has constraints on what can be pickled in its inductor\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# config. Check that the gm is cacheable by inductor first,\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# and if it raises an exception, also bypass on our end.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     FxGraphCache\u001b[38;5;241m.\u001b[39m_check_can_cache(gm)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BypassFxGraphCache \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# Sometimes inductor configs are unpickleable and can fail\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BypassAOTAutogradCache \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:830\u001b[0m, in \u001b[0;36mFxGraphHashDetails.__init__\u001b[0;34m(self, gm, example_inputs, fx_kwargs, inputs_to_check)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_matmul_settings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    824\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_tf32,\n\u001b[1;32m    825\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_fp16_reduced_precision_reduction,\n\u001b[1;32m    826\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_bf16_reduced_precision_reduction,\n\u001b[1;32m    827\u001b[0m )\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Also hash on various system info (including the triton compiler version).\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_version \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_info \u001b[38;5;241m=\u001b[39m CacheBase\u001b[38;5;241m.\u001b[39mget_system()\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minductor_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39msave_config_portable()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:695\u001b[0m, in \u001b[0;36mtorch_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    692\u001b[0m                     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hasher\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_TORCH_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibfb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parutil\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parutil\u001b[38;5;241m.\u001b[39mget_file_contents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch/src_hash.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:688\u001b[0m, in \u001b[0;36mtorch_key.<locals>.get_code_hash\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m    686\u001b[0m hasher \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha256()\n\u001b[1;32m    687\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(torch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 688\u001b[0m \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m extra_files:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:664\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mispkg:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# need to also hash submodules\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule_search_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:664\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mispkg:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# need to also hash submodules\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule_search_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:664\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mispkg:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# need to also hash submodules\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule_search_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:654\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_code_hash\u001b[39m(\n\u001b[1;32m    652\u001b[0m     roots: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, prefix: \u001b[38;5;28mstr\u001b[39m, hasher: hashlib\u001b[38;5;241m.\u001b[39m_Hash\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkgutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    655\u001b[0m         spec \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmodule_finder\u001b[38;5;241m.\u001b[39mfind_spec(lib\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/pkgutil.py:130\u001b[0m, in \u001b[0;36miter_modules\u001b[0;34m(path, prefix)\u001b[0m\n\u001b[1;32m    128\u001b[0m yielded \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m importers:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, ispkg \u001b[38;5;129;01min\u001b[39;00m iter_importer_modules(i, prefix):\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m yielded:\n\u001b[1;32m    132\u001b[0m             yielded[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/pkgutil.py:168\u001b[0m, in \u001b[0;36m_iter_file_finder_modules\u001b[0;34m(importer, prefix)\u001b[0m\n\u001b[1;32m    166\u001b[0m modname \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     dircontents \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# ignore unreadable directories like import does\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     dircontents \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with three streams\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk on the load stream\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    rem -= layers_per_chunk\n",
    "\n",
    "    m_next = None\n",
    "    \n",
    "    while True:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        if rem > 0:\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        else:\n",
    "            m_next = None\n",
    "\n",
    "        # Wait for the current chunk to finish loading\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "\n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "\n",
    "        # Check if this is the last chunk\n",
    "        if rem <= 0:\n",
    "            break\n",
    "        \n",
    "        # Offload the current chunk asynchronously\n",
    "        with torch.cuda.stream(stream_offload):\n",
    "            m_current.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline and update counters\n",
    "        m_current = m_next\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    # The last m_current must be moved back to cpu explicitly\n",
    "    m_current.to('cpu')\n",
    "    del m_current\n",
    "    if m_next:\n",
    "        del m_next\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250, temperature=0.7, top_p=0.95):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            # Use the calculated l1 value, not a hardcoded number\n",
    "            x = compute(model, x, l1, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Apply temperature and Top-P sampling\n",
    "        logits = x[:, -1, :] / temperature\n",
    "        \n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        \n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Find the tokens that are in the nucleus\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        \n",
    "        # Shift the indices to the right to keep at least one token\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        # Mask out the logits that are not in the nucleus\n",
    "        logits[sorted_indices[sorted_indices_to_remove]] = -float('inf')\n",
    "        \n",
    "        # Sample a token from the nucleus\n",
    "        next_token = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f60ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d74102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae0c469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    # Initialize offload module to None for the first iteration\n",
    "    m_to_offload = None\n",
    "    \n",
    "    while i <= total_layers:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        # Wait for the current chunk to finish loading before computing\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        \n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, attention_mask)\n",
    "        \n",
    "        # Offload the previously used chunk (if it exists)\n",
    "        if m_to_offload:\n",
    "            # Wait for compute to finish before offloading to prevent race condition\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Handle the final cleanup outside the loop\n",
    "    # stream_compute.synchronize()\n",
    "    # stream_load.synchronize()\n",
    "    # stream_offload.synchronize()\n",
    "    \n",
    "    # Ensure the last computed module is offloaded as well\n",
    "    if m_to_offload:\n",
    "        m_to_offload.to('cpu')\n",
    "    if m_current: # Should be None but for safety\n",
    "        m_current.to('cpu')\n",
    "    if m_next:\n",
    "        m_next.to('cpu')\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e9db5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.forward() missing 2 required positional arguments: 'sin' and 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[17], line 142\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Forcing layers_per_chunk to 5 as requested to test pipelining\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     layers_per_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_per_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    145\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "Cell \u001b[0;32mIn[17], line 72\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mstream(stream_compute):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 72\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mm_current\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Offload the previously used chunk (if it exists)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m_to_offload:\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Wait for compute to finish before offloading to prevent race condition\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/inf_engine/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/inf_engine/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/inf_engine/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:736\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    733\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "\u001b[0;31mTypeError\u001b[0m: Module.forward() missing 2 required positional arguments: 'sin' and 'attention_mask'"
     ]
    }
   ],
   "source": [
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa1fd1",
   "metadata": {},
   "source": [
    "# 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "layers=model.model.layers\n",
    "\n",
    "\n",
    "# Usage\n",
    "# compiled_block = torch.compile(FusedBlock(layers[i:j]))\n",
    "\n",
    "# def compute(model, x, layers_per_chunk, cos, sin,k,max_new_tokens,attention_mask):\n",
    "#     total_layers = len(model.model.layers)\n",
    "#     gpu_stream = torch.cuda.current_stream()\n",
    "#     if layers_per_chunk >= total_layers:\n",
    "#         # model = model.to('cuda')\n",
    "#         # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "#         # return out.argmax(-1)\n",
    "#         i=0\n",
    "#         l=total_layers\n",
    "#         m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "#         x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         return x\n",
    "#     i = 0\n",
    "#     rem = total_layers\n",
    "#     while rem > 0:\n",
    "#         l = min(layers_per_chunk, rem)\n",
    "        \n",
    "#         m = Module(model.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "#         # torch.cuda.current_stream().wait_stream(gpu_stream)\n",
    "#         # m=torch.compile(m)\n",
    "#         # Run chunk\n",
    "#         with torch.no_grad():\n",
    "#             x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         m=m.to('cpu')\n",
    "#         torch.cuda.empty_cache()\n",
    "#         # update indices\n",
    "#         i += l\n",
    "#         rem -= l\n",
    "#     torch.cuda.current_stream().synchronize()\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i=0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True), \n",
    "                              cos.to('cuda', non_blocking=True), \n",
    "                              sin.to('cuda', non_blocking=True), \n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            m_current = m_current.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_current = m_current.to('cpu', non_blocking=True)\n",
    "            del m_current\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        torch.cuda.empty_cache() \n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec619479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda') \n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,1,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb2f9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.548299551010132\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2460c40",
   "metadata": {},
   "source": [
    "# other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Loading new layers (stream_load)\n",
    "    Worker 2: Computation (stream_compute)\n",
    "    Worker 3: Offloading old layers (stream_offload)\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    m_to_offload = None # This will hold the chunk from the previous iteration to be offloaded\n",
    "    m_current = None # This is the chunk currently being computed\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    while i <= total_layers + layers_per_chunk: # Loop until all layers are processed and the last chunk is offloaded\n",
    "        # 1. Load the next chunk on a separate stream if there are layers left\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        \n",
    "        # 2. Wait for the current chunk to finish loading, then compute\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # 3. Wait for the computation to finish, then offload the previous chunk\n",
    "        if m_to_offload:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "        \n",
    "        # Advance the pipeline to the next stage\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birbirbirbirbirbirbirbirbirbir\n",
      "Time taken for 10 tokens: 4.10 seconds\n",
      "\n",
      "Total time 4.095994472503662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ae734",
   "metadata": {},
   "source": [
    "12.1\n",
    "11.9\n",
    "11.5\n",
    "11.9\n",
    "10.9\n",
    "11.1\n",
    "10.3\n",
    "9.5\n",
    "9.0 (10)\n",
    "9.3 (11)\n",
    "10.5 (12)\n",
    "9.8 (13)\n",
    "9.9 (14)\n",
    "9.1 (15)\n",
    "\n",
    "1.9 (16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c91944",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=nn.ModuleList(layers[i:j+1])\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        model = model.to('cuda')\n",
    "        out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        return out.argmax(-1)\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l)\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cpu'), cos.to('cpu'), sin.to('cpu'))\n",
    "\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39589d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2278, -0.1952,  1.0512,  ...,  0.4470,  0.9027,  0.4017],\n",
       "         [-0.0171,  0.0437,  0.0796,  ..., -0.0353, -0.0933, -0.0494],\n",
       "         [-0.0076,  0.0305, -0.0894,  ..., -0.1101, -0.0858,  0.0215],\n",
       "         [-0.0955,  0.1930, -0.0288,  ..., -0.0590, -0.0255,  0.1503],\n",
       "         [-0.0718,  0.0508, -0.0462,  ...,  0.0025, -0.0664, -0.0365]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,7)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        # model = model.to('cuda')\n",
    "        # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        # return out.argmax(-1)\n",
    "        i=0\n",
    "        l=total_layers\n",
    "        m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "        x=m(x.to('cuda'),cos.to('cuda'),sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l).to('cuda')\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c7af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,get_layers(model),cos,sin)\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf313c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d5247463e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4072, -0.0109,  0.1410,  ...,  0.2553, -0.2387, -0.3508],\n",
       "         [ 0.1307, -0.0781,  0.1387,  ...,  0.4531,  0.1098, -0.2780],\n",
       "         [ 0.1409,  0.0029,  0.2776,  ...,  0.8693,  0.6018, -0.1494],\n",
       "         [-0.0449,  0.0310,  0.0482,  ...,  0.3607, -0.3491, -0.5668],\n",
       "         [-0.0326,  0.3774, -0.2075,  ...,  0.3641, -0.2429, -0.2585]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd69379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4545,  2.3900, -1.2563,  ..., -1.2801, -1.2815, -1.2818],\n",
       "         [-0.5335,  1.2898, -0.0679,  ..., -0.9111, -0.9123, -0.9122],\n",
       "         [ 0.2582,  0.9092,  1.3728,  ..., -1.5733, -1.5758, -1.5756],\n",
       "         [-1.0597,  2.9733,  0.6937,  ..., -0.5993, -0.6006, -0.6005],\n",
       "         [ 0.2114,  0.6242,  1.3618,  ..., -0.0651, -0.0660, -0.0650]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98977648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   3\n",
      "4   7\n",
      "8   11\n",
      "12   15\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=3\n",
    "l=4\n",
    "rem=16\n",
    "while(rem!=0):\n",
    "    if(rem>=l):\n",
    "        m=nn.Sequential(\n",
    "            layers[i:j]\n",
    "        )\n",
    "        print(i,\" \",j)\n",
    "        rem-=l\n",
    "        i+=l;j+=l\n",
    "    else:\n",
    "        m=nn.Sequential(\n",
    "            layers[i:i+rem-1]\n",
    "        )\n",
    "        print('else')\n",
    "        print(i,\" \",i+rem-1)\n",
    "        rem=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e4353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ModuleList(\n",
       "    (0-2): 3 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.Sequential(\n",
    "        layers[0:3]\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ccb6",
   "metadata": {},
   "source": [
    "if int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 > total layers  : load whole model into sequential model\n",
    "\n",
    "else load int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 layers into the seq \n",
    "\n",
    "keep track of the layers which went inside the seq model\n",
    "\n",
    "[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] if 6-2=4 layers are allowed then iterations = while i not>= number of layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.814697265625e-06\n",
      "0.4892578125\n",
      "0.4892578125\n"
     ]
    }
   ],
   "source": [
    "# LlamaRMSNorm((2048,), eps=1e-05)\n",
    "#     (rotary_emb): LlamaRotaryEmbedding()\n",
    "# print((sum(p.numel() for p in model.model.norm.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.lm_head.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.model.embed_tokens.parameters())*2)/(1024**3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa53c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=input()\n",
    "converstation=[\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are an assitant help user by solving their query\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"query-{question}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "# prompt.to('cpu'//)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95944161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   6151,   1268,    527,    220]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out=model.generate(**prompt,max_new_tokens=1000)\n",
    "# print(tok.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68deb58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79079cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=layers[i:j]\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "            print()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2181, -0.3543,  1.0459,  ...,  0.3998,  0.9639,  0.3820],\n",
       "         [ 0.0501, -0.0546, -0.0243,  ...,  0.1164, -0.1678,  0.0384],\n",
       "         [ 0.0353, -0.0942, -0.1205,  ..., -0.0287, -0.1063,  0.0035],\n",
       "         [ 0.0649,  0.0549, -0.0413,  ..., -0.0517, -0.0086,  0.0737],\n",
       "         [-0.0263,  0.0541, -0.0978,  ...,  0.0248, -0.0258, -0.0238]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,3)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6659f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
      "           8.4305e-04,  7.0190e-04],\n",
      "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
      "           9.5215e-03,  4.7363e-02],\n",
      "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
      "          -4.8828e-03,  1.2451e-02],\n",
      "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
      "          -1.5869e-02,  1.1520e-03],\n",
      "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
      "          -1.8677e-02, -3.5156e-02]]], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.5403,  0.7878,  0.9046,  0.9576,  0.9813,  0.9917,  0.9964,\n",
      "           0.9984,  0.9993,  0.9997,  0.9999,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.5403,  0.7878,  0.9046,\n",
      "           0.9576,  0.9813,  0.9917,  0.9964,  0.9984,  0.9993,  0.9997,\n",
      "           0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.4161,  0.2412,  0.6366,  0.8340,  0.9257,  0.9671,  0.9855,\n",
      "           0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.4161,  0.2412,  0.6366,\n",
      "           0.8340,  0.9257,  0.9671,  0.9855,  0.9936,  0.9972,  0.9988,\n",
      "           0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.9900, -0.4078,  0.2471,  0.6397,  0.8355,  0.9264,  0.9674,\n",
      "           0.9856,  0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.9900, -0.4078,  0.2471,\n",
      "           0.6397,  0.8355,  0.9264,  0.9674,  0.9856,  0.9936,  0.9972,\n",
      "           0.9988,  0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.6536, -0.8837, -0.1895,  0.3912,  0.7139,  0.8704,  0.9422,\n",
      "           0.9744,  0.9887,  0.9950,  0.9978,  0.9990,  0.9996,  0.9998,\n",
      "           0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.6536, -0.8837, -0.1895,\n",
      "           0.3912,  0.7139,  0.8704,  0.9422,  0.9744,  0.9887,  0.9950,\n",
      "           0.9978,  0.9990,  0.9996,  0.9998,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000]]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 8.4147e-01,  6.1596e-01,  4.2627e-01,  2.8809e-01,  1.9271e-01,\n",
      "           1.2833e-01,  8.5293e-02,  5.6639e-02,  3.7597e-02,  2.4953e-02,\n",
      "           1.6560e-02,  1.0989e-02,  7.2926e-03,  4.8394e-03,  3.2114e-03,\n",
      "           1.2905e-03,  4.2956e-04,  9.7083e-05,  1.9462e-05,  1.2915e-05,\n",
      "           8.5703e-06,  5.6872e-06,  3.7741e-06,  2.5045e-06,  1.6620e-06,\n",
      "           1.1029e-06,  7.3187e-07,  4.8567e-07,  3.2229e-07,  2.1387e-07,\n",
      "           1.4193e-07,  9.4183e-08,  8.4147e-01,  6.1596e-01,  4.2627e-01,\n",
      "           2.8809e-01,  1.9271e-01,  1.2833e-01,  8.5293e-02,  5.6639e-02,\n",
      "           3.7597e-02,  2.4953e-02,  1.6560e-02,  1.0989e-02,  7.2926e-03,\n",
      "           4.8394e-03,  3.2114e-03,  1.2905e-03,  4.2956e-04,  9.7083e-05,\n",
      "           1.9462e-05,  1.2915e-05,  8.5703e-06,  5.6872e-06,  3.7741e-06,\n",
      "           2.5045e-06,  1.6620e-06,  1.1029e-06,  7.3187e-07,  4.8567e-07,\n",
      "           3.2229e-07,  2.1387e-07,  1.4193e-07,  9.4183e-08],\n",
      "         [ 9.0930e-01,  9.7048e-01,  7.7121e-01,  5.5175e-01,  3.7819e-01,\n",
      "           2.5454e-01,  1.6997e-01,  1.1310e-01,  7.5141e-02,  4.9890e-02,\n",
      "           3.3115e-02,  2.1977e-02,  1.4585e-02,  9.6787e-03,  6.4228e-03,\n",
      "           2.5811e-03,  8.5911e-04,  1.9417e-04,  3.8923e-05,  2.5830e-05,\n",
      "           1.7141e-05,  1.1374e-05,  7.5481e-06,  5.0089e-06,  3.3239e-06,\n",
      "           2.2058e-06,  1.4637e-06,  9.7135e-07,  6.4459e-07,  4.2775e-07,\n",
      "           2.8385e-07,  1.8837e-07,  9.0930e-01,  9.7048e-01,  7.7121e-01,\n",
      "           5.5175e-01,  3.7819e-01,  2.5454e-01,  1.6997e-01,  1.1310e-01,\n",
      "           7.5141e-02,  4.9890e-02,  3.3115e-02,  2.1977e-02,  1.4585e-02,\n",
      "           9.6787e-03,  6.4228e-03,  2.5811e-03,  8.5911e-04,  1.9417e-04,\n",
      "           3.8923e-05,  2.5830e-05,  1.7141e-05,  1.1374e-05,  7.5481e-06,\n",
      "           5.0089e-06,  3.3239e-06,  2.2058e-06,  1.4637e-06,  9.7135e-07,\n",
      "           6.4459e-07,  4.2775e-07,  2.8385e-07,  1.8837e-07],\n",
      "         [ 1.4112e-01,  9.1309e-01,  9.6899e-01,  7.6862e-01,  5.4950e-01,\n",
      "           3.7654e-01,  2.5340e-01,  1.6919e-01,  1.1258e-01,  7.4796e-02,\n",
      "           4.9661e-02,  3.2963e-02,  2.1876e-02,  1.4518e-02,  9.6342e-03,\n",
      "           3.8716e-03,  1.2887e-03,  2.9125e-04,  5.8385e-05,  3.8744e-05,\n",
      "           2.5711e-05,  1.7062e-05,  1.1322e-05,  7.5134e-06,  4.9859e-06,\n",
      "           3.3087e-06,  2.1956e-06,  1.4570e-06,  9.6688e-07,  6.4162e-07,\n",
      "           4.2578e-07,  2.8255e-07,  1.4112e-01,  9.1309e-01,  9.6899e-01,\n",
      "           7.6862e-01,  5.4950e-01,  3.7654e-01,  2.5340e-01,  1.6919e-01,\n",
      "           1.1258e-01,  7.4796e-02,  4.9661e-02,  3.2963e-02,  2.1876e-02,\n",
      "           1.4518e-02,  9.6342e-03,  3.8716e-03,  1.2887e-03,  2.9125e-04,\n",
      "           5.8385e-05,  3.8744e-05,  2.5711e-05,  1.7062e-05,  1.1322e-05,\n",
      "           7.5134e-06,  4.9859e-06,  3.3087e-06,  2.1956e-06,  1.4570e-06,\n",
      "           9.6688e-07,  6.4162e-07,  4.2578e-07,  2.8255e-07],\n",
      "         [-7.5680e-01,  4.6814e-01,  9.8188e-01,  9.2033e-01,  7.0021e-01,\n",
      "           4.9232e-01,  3.3498e-01,  2.2474e-01,  1.4986e-01,  9.9656e-02,\n",
      "           6.6193e-02,  4.3944e-02,  2.9167e-02,  1.9356e-02,  1.2845e-02,\n",
      "           5.1622e-03,  1.7182e-03,  3.8833e-04,  7.7847e-05,  5.1659e-05,\n",
      "           3.4281e-05,  2.2749e-05,  1.5096e-05,  1.0018e-05,  6.6479e-06,\n",
      "           4.4115e-06,  2.9275e-06,  1.9427e-06,  1.2892e-06,  8.5550e-07,\n",
      "           5.6771e-07,  3.7673e-07, -7.5680e-01,  4.6814e-01,  9.8188e-01,\n",
      "           9.2033e-01,  7.0021e-01,  4.9232e-01,  3.3498e-01,  2.2474e-01,\n",
      "           1.4986e-01,  9.9656e-02,  6.6193e-02,  4.3944e-02,  2.9167e-02,\n",
      "           1.9356e-02,  1.2845e-02,  5.1622e-03,  1.7182e-03,  3.8833e-04,\n",
      "           7.7847e-05,  5.1659e-05,  3.4281e-05,  2.2749e-05,  1.5096e-05,\n",
      "           1.0018e-05,  6.6479e-06,  4.4115e-06,  2.9275e-06,  1.9427e-06,\n",
      "           1.2892e-06,  8.5550e-07,  5.6771e-07,  3.7673e-07]]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[117], line 11\u001b[0m, in \u001b[0;36mmodule.forward\u001b[0;34m(self, x, cos, sin)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sin)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m---> 11\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:287\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    286\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    291\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    292\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:61\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m---> 61\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m     62\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     63\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d3c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0131, -0.0472,  0.0412,  ..., -0.0090,  0.0760, -0.0241],\n",
       "         [-0.0173, -0.0174, -0.0555,  ...,  0.0072, -0.0251, -0.0046],\n",
       "         [-0.0093,  0.0029, -0.1033,  ..., -0.0247, -0.0518,  0.0019],\n",
       "         [ 0.0227,  0.0492, -0.0206,  ..., -0.0301,  0.0070,  0.0488],\n",
       "         [-0.0341,  0.0040, -0.0712,  ..., -0.0127, -0.0116,  0.0161]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=model.model.layers[0]\n",
    "m(x.to('cpu'),position_embeddings=(cos.to('cpu'),sin.to('cpu')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f23b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out=model(prompt['input_ids'].to('cpu'))\n",
    "m=layers[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d086ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d526b7b8e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(prompt['input_ids'].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextt=out['logits'][:,-1,:].argmax(dim=-1)\n",
    "nextt\n",
    "tok.decode(nextt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=prompt['input_ids']\n",
    "del prompt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480b343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "# del input_ids\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91acbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.decoder.embed_positions.to('cuda')\n",
    "# x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "# model.model.decoder.embed_positions.to('cpu')\n",
    "# # del model.model.decoder.embed_positions\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's max_position_embeddings: 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's max_position_embeddings: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b082e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386187b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "tokenizer=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "model=AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.2-1B',torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c247e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "prompt=tokenizer(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb.to(x.device)\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2000bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initial KV Extraction Using DynamicCache ===\n",
      " aca  wors  Ca  Ca \n",
      "Initial KV extraction time: 20.52s\n",
      "Final hidden state shape: torch.Size([1, 1, 128256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers.models.llama.modeling_llama import DynamicCache, Cache  # Correct imports\n",
    "\n",
    "# === KV Extractor ===\n",
    "def extract_and_reshape_kv(layer, x):\n",
    "    \"\"\"Extract KV using your approach and reshape for cache usage\"\"\"\n",
    "    attn = layer.self_attn\n",
    "    \n",
    "    # Your approach: Extract raw projections\n",
    "    k_raw = attn.k_proj(x).detach()  # [batch, seq_len, 512]\n",
    "    v_raw = attn.v_proj(x).detach()  # [batch, seq_len, 512]\n",
    "    \n",
    "    # Convert to cache format\n",
    "    bsz, seq_len, _ = x.shape\n",
    "    num_kv_heads = attn.num_key_value_heads  # 8 for your model\n",
    "    head_dim = attn.head_dim  # 64 for your model\n",
    "    \n",
    "    # Reshape for multi-head attention cache format\n",
    "    k_cache = k_raw.view(bsz, seq_len, num_kv_heads, head_dim).transpose(1, 2)  # [1, 8, seq_len, 64]\n",
    "    v_cache = v_raw.view(bsz, seq_len, num_kv_heads, head_dim).transpose(1, 2)  # [1, 8, seq_len, 64]\n",
    "    \n",
    "    return (k_cache, v_cache)\n",
    "\n",
    "\n",
    "\n",
    "# === Step 1: Initial Setup ===\n",
    "\n",
    "\n",
    "# === Step 2: Initialize DynamicCache ===\n",
    "# dynamic_cache = DynamicCache.with_empty_cache(model=model.model)\n",
    "dynamic_cache = DynamicCache()\n",
    "\n",
    "print(\"=== Initial KV Extraction Using DynamicCache ===\")\n",
    "model.model.rotary_emb.to('cuda')\n",
    "start = time.time()\n",
    "for _ in range(5):\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    # initial_input_ids = torch.tensor([[tokenizer.bos_token_id]], device='cuda')\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    seq_len = x.shape[1]\n",
    "    batch_size = x.shape[0]\n",
    "\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        # print(f\"--- Layer {layer_idx + 1} ---\")\n",
    "        \n",
    "        layer.to('cuda')\n",
    "        x = x.to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Extract KV\n",
    "            k_cache, v_cache = extract_and_reshape_kv(layer, x)\n",
    "\n",
    "            # Use Cache wrapper to update DynamicCache\n",
    "            dynamic_cache.update(\n",
    "                k_cache.cuda(),\n",
    "                v_cache.cuda(),\n",
    "                layer_idx=layer_idx  # must match layer index\n",
    "            )\n",
    "\n",
    "            # Forward pass using dynamic cache\n",
    "            out = layer(\n",
    "                x,\n",
    "                position_embeddings=(cos.to('cuda'), sin.to('cuda')),\n",
    "                past_key_value=dynamic_cache,\n",
    "                use_cache=True\n",
    "            )\n",
    "            x = out[0]\n",
    "\n",
    "        layer.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        model.model.norm.to('cuda')\n",
    "        x=model.model.norm(x)\n",
    "        model.model.norm.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x)\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x[:,-1,:]\n",
    "        \n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        # input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        input_ids=x1\n",
    "        # print(input_ids.shape)\n",
    "        print(tokenizer.decode(x1[0]),end=\" \")\n",
    "\n",
    "\n",
    "print(f\"\\nInitial KV extraction time: {time.time() - start:.2f}s\")\n",
    "print(f\"Final hidden state shape: {x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f1d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b42bffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.809312343597412\n"
     ]
    }
   ],
   "source": [
    "# seq_len = x.shape[1]\n",
    "# batch_size = x.shape[0]\n",
    "# position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "# rotary_emb = model.model.rotary_emb\n",
    "# cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "# cnt=0\n",
    "start=time.time()\n",
    "for _ in range(5):\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    input_ids.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    seq_len = x.shape[1]\n",
    "    batch_size = x.shape[0]\n",
    "    position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "    rotary_emb = model.model.rotary_emb.to(x.device)\n",
    "    cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "    cnt=0\n",
    "    for i in model.model.layers:\n",
    "        # print(x.shape)\n",
    "        # print(cos.shape)\n",
    "        # print(sin.shape)\n",
    "        cnt+=1\n",
    "        # print(cnt)\n",
    "        i.to('cuda')\n",
    "        x.to('cuda')\n",
    "        # print(x)\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x=i(x,position_embeddings=(cos,sin),use_cache=True)\n",
    "            # print(x)\n",
    "            # print(x)\n",
    "        i.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x=x[0]\n",
    "        # x.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "            # time.sleep(3)\n",
    "    with torch.no_grad():\n",
    "        model.model.norm.to('cuda')\n",
    "        x=model.model.norm(x)\n",
    "        model.model.norm.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x)\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x[:,-1,:]\n",
    "        torch.cat([input_ids,x1],dim=-1)\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        # print(tok.decode(x1[0]),end=\" \")\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18251d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f129a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7762, -5.0166, 13.8534,  ..., -1.9152,  1.8989, -3.0755],\n",
       "         [-0.2314,  1.3026,  1.7797,  ..., -0.2708, -1.0364, -0.2083],\n",
       "         [ 0.2276,  0.8936,  0.8521,  ..., -0.0246, -1.4944, -0.0458],\n",
       "         [ 0.0748,  1.2204,  0.5945,  ..., -0.7305, -1.4607, -0.6370],\n",
       "         [ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc202f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1caa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.model.norm.to('cuda')\n",
    "    x=model.model.norm(x)\n",
    "    model.model.norm.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cuda')\n",
    "    x=model.lm_head(x)\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca9e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560056fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe40c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   6151,   1268,    527,    220,     17]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([input_ids,x1],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc43b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 "
     ]
    }
   ],
   "source": [
    "x1=x[:,-1,:]\n",
    "x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "print(tok.decode(x1[0]),end=\" \")\n",
    "# print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cd9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device=torch.device('cuda:0')\n",
    "props=torch.cuda.get_device_properties('cuda')\n",
    "props.total_memory/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a20b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import DynamicCache\n",
    "def model_inference(question, tokens=250):\n",
    "    # --- Phase 1: Prefill the KV cache with the entire prompt ---\n",
    "    prompt = tokenizer(question, return_tensors='pt').to('cuda')\n",
    "    input_ids = prompt['input_ids']\n",
    "    \n",
    "    dynamic_cache = DynamicCache()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Process the entire prompt at once to get the initial embeddings\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        model.model.rotary_emb.to('cuda')\n",
    "        cos, sin = model.model.rotary_emb(x=x, position_ids=position_ids)\n",
    "        \n",
    "        # This loop now correctly passes the cache to the layer and lets the layer update it\n",
    "        for layer_idx, layer in enumerate(model.model.layers):\n",
    "            layer.to('cuda')\n",
    "            \n",
    "            out = layer(\n",
    "                x,\n",
    "                position_embeddings=(cos, sin),\n",
    "                past_key_value=dynamic_cache, # pass the cache\n",
    "                use_cache=True               # enable caching\n",
    "            )\n",
    "            x = out[0]\n",
    "            \n",
    "            layer.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get the last hidden state and the last token ID from the prefill pass\n",
    "    with torch.no_grad():\n",
    "        model.model.norm.to('cuda')\n",
    "        x = model.model.norm(x)\n",
    "        model.model.norm.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        model.lm_head.to('cuda')\n",
    "        logits = model.lm_head(x)\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    next_token_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "    print(tokenizer.decode(input_ids[0]), end=\"\")\n",
    "    print(tokenizer.decode(next_token_id[0]), end=\"\")\n",
    "    \n",
    "    # --- Phase 2: Autoregressive generation loop ---\n",
    "    # We now pass only the single new token and the cache\n",
    "    current_input_ids = next_token_id\n",
    "    \n",
    "    for _ in range(tokens):\n",
    "        with torch.no_grad():\n",
    "            # Get the new position ID, based on the current length of the cache\n",
    "            current_position_id = torch.tensor([[dynamic_cache.get_seq_length()]], device='cuda')\n",
    "            \n",
    "            model.model.embed_tokens.to('cuda')\n",
    "            x = model.model.embed_tokens(current_input_ids)\n",
    "            \n",
    "            model.model.rotary_emb.to('cuda')\n",
    "            cos, sin = model.model.rotary_emb(x=x, position_ids=current_position_id)\n",
    "            \n",
    "            # Offloading loop for the single new token\n",
    "            for layer_idx, layer in enumerate(model.model.layers):\n",
    "                layer.to('cuda')\n",
    "                \n",
    "                out = layer(\n",
    "                    x,\n",
    "                    position_embeddings=(cos, sin),\n",
    "                    past_key_value=dynamic_cache,\n",
    "                    use_cache=True\n",
    "                )\n",
    "                x = out[0]\n",
    "                \n",
    "                layer.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Final Norm and Head for the new token\n",
    "            model.model.norm.to('cuda')\n",
    "            x = model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            model.lm_head.to('cuda')\n",
    "            logits = model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            next_token_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "            print(tokenizer.decode(next_token_id[0]), end=\"\")\n",
    "            \n",
    "            current_input_ids = next_token_id\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501122ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>Hi, I'm a 20 year old girl who loves"
     ]
    }
   ],
   "source": [
    "dc=model_inference(\"Hi\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e823e27",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LlamaAttention' object has no attribute 'num_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m layer \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m, layer\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mhead_dim)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaAttention' object has no attribute 'num_heads'"
     ]
    }
   ],
   "source": [
    "layer = model.model.layers[0]\n",
    "print(layer.self_attn.num_heads, layer.self_attn.head_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "822a4e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 24, 64])\n"
     ]
    }
   ],
   "source": [
    "print(dc[3][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9202f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(prompt_text, tokens=250):\n",
    "    # Initial tokenization\n",
    "    prompt = tok(prompt_text, return_tensors='pt')\n",
    "    input_ids = prompt['input_ids'].to('cuda')  # [1, seq_len]\n",
    "    del prompt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # Run only the latest token through embed layer and model\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for layer in model.model.layers:\n",
    "            layer.to('cuda')\n",
    "            x = x.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                x = layer(x, position_embeddings=(cos, sin))\n",
    "            layer.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x = x[0]  # Remove tuple\n",
    "\n",
    "        # Final norm and lm_head\n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x = model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            model.lm_head.to('cuda')\n",
    "            logits = model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]  # Only the last token matters\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # shape [1,1]\n",
    "\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Decode and print latest token\n",
    "        print(tok.decode(next_token[0]), end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "506a694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am a new member here and I am very25.85783076286316\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"Hi how are you ?\",10)\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d98000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_oov(text, tokenizer, max_token_id=2048, oov_token='[OOV]'):\n",
    "    # First, add [OOV] token to tokenizer if not present\n",
    "    if oov_token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': [oov_token]})\n",
    "\n",
    "    oov_id = tokenizer.convert_tokens_to_ids(oov_token)\n",
    "\n",
    "    # Raw tokenization\n",
    "    encoding = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "\n",
    "    # Replace tokens  max_token_id with OOV\n",
    "    processed_ids = []\n",
    "    for token_id in input_ids:\n",
    "        if token_id >= max_token_id:\n",
    "            processed_ids.append(oov_id)\n",
    "        else:\n",
    "            processed_ids.append(token_id)\n",
    "\n",
    "    return torch.tensor([processed_ids]), encoding['attention_mask']\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    try:\n",
    "        for _ in range(tokens):\n",
    "            # prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "            # input_ids = prompt['input_ids']\n",
    "            # attention_mask = prompt['attention_mask']\n",
    "            input_ids, attention_mask = tokenize_with_oov(question, tok)\n",
    "\n",
    "            del prompt\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Truncate if too long\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            max_positions = model.config.max_position_embeddings\n",
    "            if seq_len > max_positions:\n",
    "                input_ids = input_ids[:, -max_positions:]\n",
    "                attention_mask = attention_mask[:, -max_positions:]\n",
    "                seq_len = input_ids.shape[1]\n",
    "\n",
    "            # Embeddings\n",
    "            model.model.decoder.embed_tokens.to('cpu')\n",
    "            x = model.model.decoder.embed_tokens(input_ids)\n",
    "\n",
    "            # Positional IDs\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device='cpu')\n",
    "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            model.model.decoder.embed_positions.to('cpu')\n",
    "            x = x + model.model.decoder.embed_positions(position_ids)\n",
    "\n",
    "            # Decoder layers\n",
    "            for layer in model.model.decoder.layers:\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                with torch.no_grad():\n",
    "                    x = layer(x, attention_mask=attention_mask.to(torch.float32))[0]\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Final LM head\n",
    "            model.lm_head.to('cpu')\n",
    "            logits = model.lm_head(x.to('cpu'))\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Decode\n",
    "            next_token_id = logits.argmax(-1)\n",
    "            next_token_text = tok.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            print(next_token_text, end=\" \")\n",
    "            question += \" \" + next_token_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n[Exception Caught]\")\n",
    "        print(\"Question So Far:\", question)\n",
    "        prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "        input_ids = prompt['input_ids']\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        y = torch.arange(x.shape[1]).unsqueeze(0)\n",
    "        print(\"Embedding Shape:\", x[0].shape)\n",
    "        print(\"Position ID Shape:\", y.shape)\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicyconservancy how are you 're post .''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicyconservancy\n",
      "\n",
      "torch.Size([35, 768])\n",
      "torch.Size([1, 35])\n",
      "index out of range in self\n"
     ]
    }
   ],
   "source": [
    "model_inference(\"how are you 're post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Ensure model components are initially on CPU\n",
    "    model.model.decoder.embed_tokens.to('cpu')\n",
    "    model.model.decoder.embed_positions.to('cpu')\n",
    "    for layer in model.model.decoder.layers:\n",
    "        layer.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    \n",
    "    max_position_embeddings = model.config.max_position_embeddings\n",
    "    print(f\"Model's max_position_embeddings: {max_position_embeddings}\")\n",
    "\n",
    "    # Initialize input_ids_for_next_step. This will be the tensor that grows.\n",
    "    input_ids_for_next_step = tok(question, return_tensors='pt')['input_ids'] \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # The 'x' variable for the current iteration is the input_ids_for_next_step\n",
    "        current_input_ids = input_ids_for_next_step \n",
    "        current_seq_len = current_input_ids.shape[1]\n",
    "\n",
    "        # --- Truncation ---\n",
    "        if current_seq_len > max_position_embeddings:\n",
    "            current_input_ids = current_input_ids[:, current_seq_len - max_position_embeddings:]\n",
    "            print(f\"Warning: Sequence truncated from {current_seq_len} to {current_input_ids.shape[1]} for max_position_embeddings.\")\n",
    "            current_seq_len = current_input_ids.shape[1] # Update after truncation\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # Use current_input_ids for embedding lookup\n",
    "        x_embeddings = model.model.decoder.embed_tokens(current_input_ids.to('cpu')) # x_embeddings is on CPU\n",
    "        model.model.decoder.embed_tokens.to('cpu') # Move layer back\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Add Positional Embeddings\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        try:\n",
    "            position_offset = model.model.decoder.embed_positions.offset\n",
    "        except AttributeError:\n",
    "            position_offset = 2 # Common default for OPT\n",
    "\n",
    "        pos_ids = torch.arange(position_offset, position_offset + current_seq_len, device='cpu').unsqueeze(0) \n",
    "        \n",
    "        # The main `x` variable throughout the layer processing will be `current_token_embeddings_plus_pos`\n",
    "        current_token_embeddings_plus_pos = x_embeddings + model.model.decoder.embed_positions(pos_ids) \n",
    "        model.model.decoder.embed_positions.to('cpu') \n",
    "        del x_embeddings \n",
    "        del pos_ids \n",
    "        gc.collect()\n",
    "\n",
    "        # --- Decoder Layers Loop ---\n",
    "        # Rename 'x' to something that reflects its current state (embeddings in, embeddings out)\n",
    "        x_current_layer_output = current_token_embeddings_plus_pos # Start of layer processing\n",
    "        for j, layer in enumerate(model.model.decoder.layers):\n",
    "            try:\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    x_current_layer_output = layer(x_current_layer_output)[0]\n",
    "\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "                \n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder layer {j}: {e}\")\n",
    "                break\n",
    "        else: \n",
    "            # --- LM Head and Token Generation ---\n",
    "            model.lm_head.to('cpu')\n",
    "            # The input to lm_head is x_current_layer_output (the final embeddings from decoder)\n",
    "            logits = model.lm_head(x_current_layer_output.to('cpu')) \n",
    "            model.lm_head.to('cpu') \n",
    "            \n",
    "            logits = logits.to('cpu') # Ensure logits are on CPU\n",
    "            gc.collect()\n",
    "\n",
    "            # Get the predicted token ID (on CPU)\n",
    "            # Take the last token's prediction from batch 0\n",
    "            predicted_token_id = logits.argmax(-1)[:, -1].item() \n",
    "\n",
    "            decoded_token = tok.decode(predicted_token_id)\n",
    "            print(decoded_token, end=\" \")\n",
    "            \n",
    "            # --- Prepare input_ids_for_next_step for the next iteration ---\n",
    "            # Append the new token ID to the input_ids_for_next_step tensor (on CPU)\n",
    "            new_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long, device='cpu')\n",
    "            \n",
    "            # This is the correct concatenation for input_ids\n",
    "            input_ids_for_next_step = torch.cat((input_ids_for_next_step, new_token_tensor), dim=1)\n",
    "            \n",
    "            # Update the 'question' string for printing and future reference\n",
    "            question = question + \" \" + decoded_token\n",
    "            \n",
    "        if 'predicted_token_id' not in locals(): # If inner loop broke, stop outer loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    for i in range(tokens):\n",
    "        prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "        input_ids=prompt['input_ids']\n",
    "        del prompt\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cuda')\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # del model.model.decoder.embed_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_positions.to('cuda')\n",
    "        x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        # del model.model.decoder.embed_positions\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in model.model.decoder.layers:\n",
    "            try:\n",
    "                i.to('cuda')\n",
    "                x=x.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    x=i(x)[0]\n",
    "                i.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                x=x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                # time.sleep(3)\n",
    "            except:\n",
    "                print(x)\n",
    "                break\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x.to('cuda'))\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x.argmax(-1)\n",
    "        print(tok.decode(x1[0]),end=\" \")\n",
    "        question=question+\" \"+tok.decode(x1[0])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35778bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. your retarded're .''. you retarded're .dylib mom retard.''. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:76\u001b[0m, in \u001b[0;36mOPTLearnedPositionalEmbedding.forward\u001b[0;34m(self, attention_mask, past_key_values_length, position_ids)\u001b[0m\n\u001b[1;32m     74\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids[:, past_key_values_length:]\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m]),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         question\u001b[38;5;241m=\u001b[39mquestion\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43me\u001b[49m:\n\u001b[1;32m     48\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mtok(question,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mprompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "model_inference(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "cnt=0\n",
    "for i in model.model.decoder.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    try:\n",
    "        i.to('cuda')\n",
    "        x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            x=i(x)[0]\n",
    "        i.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # time.sleep(3)\n",
    "    except:\n",
    "        print(cnt)\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.to('cuda')\n",
    "x=model.lm_head(x.to('cuda'))\n",
    "model.lm_head.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e12f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d35ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d2228",
   "metadata": {},
   "source": [
    "# 3 Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e7274f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first chunk\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        # Wait for transfer to complete before starting computation\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "        # Move pointers\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        # Offload previous chunk to CPU (if exists)\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If this is the last chunk, break\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            # Offload the final chunk\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Load next chunk\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "       \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06103c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0819 09:52:38.438194 1123664 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 10.8710618019104\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "model.to('cpu')\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80987286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.613088607788086\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5596c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 09:47:38.365181 121776 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 18.63120675086975\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45178336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8292bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.665896415710449\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf827a7c",
   "metadata": {},
   "source": [
    "# 4 Workers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "056ee507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import DynamicCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf8ca5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.53.2\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Requirement already satisfied: filelock in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.53.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.53.2) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.53.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.53.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.53.2) (2025.8.3)\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.55.0\n",
      "    Uninstalling transformers-4.55.0:\n",
      "      Successfully uninstalled transformers-4.55.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "vllm 0.10.1.1 requires transformers>=4.55.0, but you have transformers 4.53.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed transformers-4.53.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.53.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc92640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "from transformers.models.llama.modeling_llama import DynamicCache\n",
    "\n",
    "os.environ[\"FLASH_ATTENTION_FORCE_DISABLED\"] = \"1\"\n",
    "torch.set_num_threads(6)\n",
    "torch.set_num_interop_threads(3)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           trust_remote_code=True,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "model.eval()\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer=tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b243c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu efficient\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory/param_size_GB)-drop)//2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    # @torch.compile\n",
    "    def forward(self, x, cos, sin,dynamic_cache,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin),past_key_value=dynamic_cache,use_cache=True,)[0]\n",
    "        return x\n",
    "tokenizer=tok\n",
    "layers=model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, dynamic_cache,max_new_tokens, attention_mask,modules):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    # stream_transfer3 = torch.cuda.Stream()  # New 3rd stream\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        with torch.no_grad():\n",
    "            x = m(x, cos, sin,dynamic_cache,attention_mask)\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "        return x\n",
    "\n",
    "    # i = 0\n",
    "    \n",
    "    m_current = None\n",
    "    m_current2=None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "    m_previous2=None\n",
    "    rem = len(modules)\n",
    "    i=0\n",
    "    if rem > 0:\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            full_mod = modules[i]  # This is already your Module wrapper\n",
    "            total_layers_in_mod = len(full_mod.model)\n",
    "            half = total_layers_in_mod // 2\n",
    "\n",
    "            # m_next = Module(full_mod.model, 0, half).to('cuda', non_blocking=True)\n",
    "        # with torch.cuda.stream(stream_transfer2):\n",
    "            m_next2 = Module(full_mod.model, half, total_layers_in_mod).to('cuda', non_blocking=True)\n",
    "        i += 1\n",
    "        rem -= 1\n",
    "    stream_compute.wait_stream(stream_transfer)\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        m_previous = m_previous2\n",
    "        m_previous2=m_current\n",
    "        m_current = m_next\n",
    "        # m_current2 = m_next2\n",
    "        m_next=m_next2\n",
    "        m_next2 = None\n",
    "        \n",
    "        \n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                if(m_current!=None):\n",
    "                    x = m_current(\n",
    "                        x,\n",
    "                        cos,\n",
    "                        sin,\n",
    "                        dynamic_cache,\n",
    "                        attention_mask\n",
    "                    )\n",
    "        if m_previous is not None:\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous.to('cpu',non_blocking=True)\n",
    "        # if m_previous2 is not None:\n",
    "        #     with torch.cuda.stream(stream_offload):\n",
    "        #         m_previous2.to('cpu',non_blocking=True)\n",
    "        torch.cuda.empty_cache()\n",
    "        if rem == 0 and m_next is None and m_next2 is None :\n",
    "            if k==max_new_tokens-1:\n",
    "                stream_compute.synchronize()\n",
    "                with torch.cuda.stream(stream_offload):\n",
    "                    m_current = m_current.to('cpu', non_blocking=True)\n",
    "                stream_offload.synchronize()\n",
    "                torch.cuda.empty_cache()\n",
    "                break\n",
    "            break\n",
    "        if rem > 0:\n",
    "            with torch.cuda.stream(stream_transfer):\n",
    "                full_mod = modules[i]  # This is already your Module wrapper\n",
    "                total_layers_in_mod = len(full_mod.model)\n",
    "                half = total_layers_in_mod // 2\n",
    "\n",
    "                # m_next = Module(full_mod.model, 0, half).to('cuda', non_blocking=True)\n",
    "            # with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(full_mod.model, half, total_layers_in_mod).to('cuda', non_blocking=True)\n",
    "            i += 1\n",
    "            rem -= 1\n",
    "    return x\n",
    "\n",
    "\n",
    "# Initialize graph variables as function attributes\n",
    "\n",
    "\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu',non_blocking=True)\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "    model.model.norm.to('cuda',non_blocking=True)\n",
    "    model.lm_head.to('cuda',non_blocking=True)\n",
    "    rotary_emb = model.model.rotary_emb.to('cuda',non_blocking=True)\n",
    "    dynamic_cache = DynamicCache()\n",
    "    current_input_ids = input_ids\n",
    "    modules=[]\n",
    "    rem1=len(model.model.layers)\n",
    "    i=0\n",
    "    while(rem1>=1):\n",
    "        l=min(1,rem1)\n",
    "        modules.append(Module(model.model.layers,i,i+l).to('cpu',non_blocking=True))\n",
    "        i+=l\n",
    "        rem1-=l\n",
    "    for k in range(tokens):\n",
    "        if k==0:\n",
    "            seq_len = current_input_ids.shape[1]\n",
    "            batch_size = current_input_ids.shape[0]\n",
    "            current_position_id=torch.arange(seq_len, dtype=torch.long, device=current_input_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        else:\n",
    "            current_position_id = torch.tensor([[dynamic_cache.get_seq_length()]], device='cuda')\n",
    "        x = model.model.embed_tokens(current_input_ids)\n",
    "        cos, sin = rotary_emb(x=x, position_ids=current_position_id)\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,dynamic_cache,tokens,attention_mask,modules)\n",
    "            x=model.model.norm(x)\n",
    "            logits=model.lm_head(x)\n",
    "        next_token_id = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        print(tokenizer.decode(next_token_id[0]), end=\"\")\n",
    "        current_input_ids = next_token_id\n",
    "    model.model.embed_tokens.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.model.norm.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    rotary_emb.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ea912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " me about the best way to get to the beach from the airport\n",
      "I am planning a trip to the beach in the next few weeks and I am wondering what is the best way to get to the beach from the airport."
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"tell\", 100)\n",
    "print()\n",
    "print(time.time()-start) #2640 2474 2172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6925b027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.55.0\n",
      "  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.55.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers==4.55.0) (1.1.8)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.55.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.55.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.55.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.55.0) (2025.8.3)\n",
      "Downloading transformers-4.55.0-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.55.4\n",
      "    Uninstalling transformers-4.55.4:\n",
      "      Successfully uninstalled transformers-4.55.4\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed transformers-4.55.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.55.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "364270ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", i am new to this forum and i am trying to get a better understanding of the different types of data that can be used in a regression model. i am trying to understand the difference between the following types of data:\n",
      "1) nominal data (e.g. gender, race, etc.)\n",
      "2) ordinal data (e.g. age, income, etc.)\n",
      "3) interval data (e.g. weight, height, etc.)\n",
      "4) ratio data (e.g. income, weight, etc\n",
      "24.25179409980774\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"hello\",100)\n",
    "print()\n",
    "print(time.time()-start) #2640 2474 2172"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c09375",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f1464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 16 layers into chunks of 4 layers each...\n",
      "Building chunk 0: layers 0 to 3\n",
      "Building chunk 1: layers 4 to 7\n",
      "Building chunk 2: layers 8 to 11\n",
      "Building chunk 3: layers 12 to 15\n",
      "Successfully built 4 chunks\n",
      ", I'm a new user and I'm trying to get my first project to work. I'm using the latest version of the Arduino IDE and the latest version of the Arduino IDE. I'm using the Arduino IDE to compile the code and upload it to the Arduino board. I'm using the Arduino IDE to compile the code and upload it to the Arduino board. I'm using the Arduino IDE to compile the code and upload it to the Arduino board. I'm using the Arduino IDE to compile the"
     ]
    }
   ],
   "source": [
    "model_chunker = ModelChunker(model, layers_per_chunk=4)  # 4 layers per chunk\n",
    "\n",
    "# Now you can run inference multiple times without rebuilding chunks\n",
    "model_inference_optimized(model_chunker, \"Hi\", tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92509193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1932328/870409792.py\", line 2, in <module>\n",
      "    from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py\", line 23, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 43, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/generation/utils.py\", line 62, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/generation/candidate_generator.py\", line 29, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pandas/__init__.py\", line 26, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/asyncio/base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/asyncio/base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1932328/870409792.py\", line 2, in <module>\n",
      "    from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py\", line 23, in <module>\n",
      "    from .auto_factory import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 43, in <module>\n",
      "    from ...generation import GenerationMixin\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2292, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/utils/import_utils.py\", line 2320, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/generation/utils.py\", line 62, in <module>\n",
      "    from .candidate_generator import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/generation/candidate_generator.py\", line 29, in <module>\n",
      "    from sklearn.metrics import roc_curve\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._metadata_requests import _MetadataRequester, _routing_enabled\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sklearn/utils/fixes.py\", line 20, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pandas/core/api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "from transformers.models.llama.modeling_llama import DynamicCache\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                          #  use_cache=True,\n",
    "                                          #  attn_implementation=\"eager\",\n",
    "                                          #  force_install=True\n",
    "                                          #  rope_scaling={'type':\"llama3\",'factor':32.0}\n",
    "                                          #  offload_folder='/offload_nvm',\n",
    "                                          #  offload_state_dict=True,\n",
    "                                          #  max_memory={\"cpu\":'0GB'},\n",
    "                                           quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    for p in module.parameters():\n",
    "        if p.device.type == 'cpu':\n",
    "            p.data = p.data.pin_memory()\n",
    "            \n",
    "# Test if your parameters are actually pinned\n",
    "# def check_if_pinned(model):\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.device.type == 'cpu':\n",
    "#             print(f\"{name}: pinned = {param.is_pinned()}\")\n",
    "\n",
    "# check_if_pinned(model)  # Should show True for all parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3b7682e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mName: transformers\n",
      "Version: 4.55.4\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache 2.0 License\n",
      "Location: /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages\n",
      "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
      "Required-by: compressed-tensors, sentence-transformers, vllm, xgrammar\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405bb477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 2652 (18525.2 MB)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip cache purge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe5040ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        # print(self.model.device)\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            # print(layer.device)\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first two chunks\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "        \n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "    while True:\n",
    "    # Wait for previous transfer to complete before compute\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        # Move pointers forward\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    cos.to('cuda', non_blocking=True),\n",
    "                    sin.to('cuda', non_blocking=True),\n",
    "                    attention_mask.to('cuda', non_blocking=True)\n",
    "                )\n",
    "\n",
    "        # Offload the previous chunk after compute is done\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If no more chunks left to load after this, finish\n",
    "        if rem == 0 and m_next is None and m_next2 is None:\n",
    "            # Wait for compute to complete\n",
    "            stream_compute.synchronize()\n",
    "\n",
    "            # Offload the current module\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Start preloading next-next chunk if any\n",
    "        if rem > 0:\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            stream_transfer2.wait_stream(stream_transfer)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "    return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu',non_blocking=True)\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "    model.model.norm.to('cuda',non_blocking=True)\n",
    "    model.lm_head.to('cuda',non_blocking=True)\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            # l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda',non_blocking=True)\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda',non_blocking=True)\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fedf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 09:31:51.247132 5599 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am interested in [Project 1]Design and analysis of a 2D Poisson's equation for the given data. I have a problem with the Poisson's equation. I have a problem with the Poisson's equation. I have a problem with the Poisson's equation. I have a problem with the Poisson's equation. I have a problem with the Poisson's equation. I have a problem with the Poisson's equation. I have a problem with the Po\n",
      "Total time 42.96747446060181\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"Hello\",100)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9005d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model, drop=2):\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024 ** 3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory / param_size_GB) - drop) // 2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    # @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers = model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        # time.sleep(5.01)  # delay 1\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        # time.sleep(5.01)  # delay 2\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            # time.sleep(5.01)  # delay 3\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        # time.sleep(5.01)  # delay 4\n",
    "\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # time.sleep(5.01)  # delay 5\n",
    "\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            # time.sleep(5.01)  # delay 6\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        # time.sleep(5.01)  # delay 7\n",
    "\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            # time.sleep(5.01)  # delay 8\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            # time.sleep(5.01)  # delay 9\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            # time.sleep(8)  # delay 10\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            # time.sleep(8)  # delay 11\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        if rem > 0:\n",
    "            # print(\"runing test\")\n",
    "            time.sleep(0.01)  # delay 12\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            time.sleep(0.01)  # delay 13\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    prompt = tok(question, return_tensors='pt').to('cpu', non_blocking=True)\n",
    "    input_ids = prompt['input_ids'].to('cuda', non_blocking=True)\n",
    "    attention_mask = prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    # time.sleep(5.01)  # delay 14\n",
    "\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "\n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # time.sleep(5.01)  # delay 15\n",
    "\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "        # time.sleep(5.01)  # delay 16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            x = compute(model, x, 3, cos, sin, k, tokens, attention_mask)\n",
    "            # time.sleep(5.01)  # delay 17\n",
    "\n",
    "            x = model.model.norm(x)\n",
    "            # time.sleep(5.01)  # delay 18\n",
    "\n",
    "            x = model.lm_head(x)\n",
    "            # time.sleep(5.01)/  # delay 19\n",
    "\n",
    "        x1 = x[:, -1, :]\n",
    "        x1 = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        print(tok.decode(x1[0]), end=\"\")\n",
    "        input_ids = torch.cat([input_ids, x1], dim=-1)\n",
    "        # time.sleep(5.01)  # delay 20\n",
    "\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236073b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " guys, please tell me this is a good deal\n",
      "Total time 10.169853687286377\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",100)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f06784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting scipy==1.10.1\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: numpy, scipy\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 1.25.2\n",
      "\u001b[2K    Uninstalling numpy-1.25.2:\n",
      "\u001b[2K      Successfully uninstalled numpy-1.25.2\n",
      "\u001b[2K  Attempting uninstall: scipy\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: scipy 1.11.1[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling scipy-1.11.1:0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m1/2\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled scipy-1.11.1\u001b[0m \u001b[32m1/2\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m2/2\u001b[0m [scipy]32m1/2\u001b[0m [scipy]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "faiss-cpu 1.11.0 requires numpy<3.0,>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
      "langchain-community 0.3.27 requires numpy>=1.26.2; python_version < \"3.13\", but you have numpy 1.24.3 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3 scipy-1.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting transformers==4.53.2\n",
      "  Using cached transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting filelock (from transformers==4.53.2)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers==4.53.2)\n",
      "  Using cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers==4.53.2)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting packaging>=20.0 (from transformers==4.53.2)\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers==4.53.2)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.53.2)\n",
      "  Using cached regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers==4.53.2)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.53.2)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers==4.53.2)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers==4.53.2)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2)\n",
      "  Using cached typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2)\n",
      "  Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers==4.53.2)\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers==4.53.2)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers==4.53.2)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers==4.53.2)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "Using cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "Using cached hf_xet-1.1.7-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "Using cached regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (152 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: urllib3, typing-extensions, tqdm, safetensors, regex, pyyaml, packaging, numpy, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: urllib3\n",
      "\u001b[2K    Found existing installation: urllib3 2.5.0\n",
      "\u001b[2K    Uninstalling urllib3-2.5.0:\n",
      "\u001b[2K      Successfully uninstalled urllib3-2.5.0\n",
      "\u001b[2K  Attempting uninstall: typing-extensions\n",
      "\u001b[2K    Found existing installation: typing_extensions 4.12.2\n",
      "\u001b[2K    Uninstalling typing_extensions-4.12.2:\n",
      "\u001b[2K      Successfully uninstalled typing_extensions-4.12.2\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: tqdm\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: tqdm 4.67.1\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling tqdm-4.67.1:\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled tqdm-4.67.1\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: safetensors\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Found existing installation: safetensors 0.5.3\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K    Uninstalling safetensors-0.5.3:\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K      Successfully uninstalled safetensors-0.5.3\u001b[0m \u001b[32m 1/18\u001b[0m [typing-extensions]\n",
      "\u001b[2K  Attempting uninstall: regex[90m\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]s]\n",
      "\u001b[2K    Found existing installation: regex 2024.11.6\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K    Uninstalling regex-2024.11.6:\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K      Successfully uninstalled regex-2024.11.6\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K  Attempting uninstall: pyyaml\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K    Found existing installation: PyYAML 6.0.2\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K    Uninstalling PyYAML-6.0.2:\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K      Successfully uninstalled PyYAML-6.0.2\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K  Attempting uninstall: packaging\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K    Found existing installation: packaging 24.2\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K    Uninstalling packaging-24.2:\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K      Successfully uninstalled packaging-24.2\u001b[0m \u001b[32m 3/18\u001b[0m [safetensors]\n",
      "\u001b[2K  Attempting uninstall: numpym\u001b[0m\u001b[90m\u001b[0m \u001b[32m 6/18\u001b[0m [packaging]\n",
      "\u001b[2K    Found existing installation: numpy 1.24.3\u001b[0m \u001b[32m 6/18\u001b[0m [packaging]\n",
      "\u001b[2K    Uninstalling numpy-1.24.3:m\u001b[90m\u001b[0m \u001b[32m 6/18\u001b[0m [packaging]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.24.3\u001b[0m \u001b[32m 6/18\u001b[0m [packaging]\n",
      "\u001b[2K  Attempting uninstall: idna[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: idna 3.10\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling idna-3.10:m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled idna-3.10\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: hf-xet[0m\u001b[90m\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: hf-xet 1.1.5\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling hf-xet-1.1.5:[0m\u001b[90m\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled hf-xet-1.1.5\u001b[0m \u001b[32m 7/18\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: fsspec0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: fsspec 2024.6.1\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling fsspec-2024.6.1:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2024.6.1\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K  Attempting uninstall: filelockm\u001b[0m\u001b[90m\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: filelock 3.13.1\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling filelock-3.13.1:\u001b[0m\u001b[90m\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled filelock-3.13.1\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K  Attempting uninstall: charset_normalizerm\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: charset-normalizer 3.3.2\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling charset-normalizer-3.3.2:m\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled charset-normalizer-3.3.2\u001b[0m \u001b[32m 9/18\u001b[0m [hf-xet]\n",
      "\u001b[2K  Attempting uninstall: certifi\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: certifi 2025.7.9\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling certifi-2025.7.9:\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled certifi-2025.7.90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: requests0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: requests 2.32.4m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling requests-2.32.4:m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled requests-2.32.490m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\u001b[0m\u001b[90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.34.4\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.34.4:\u001b[0m\u001b[90m\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.34.4\u001b[0m \u001b[32m12/18\u001b[0m [charset_normalizer]\n",
      "\u001b[2K  Attempting uninstall: tokenizers\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m15/18\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.4m\u001b[90m\u001b[0m \u001b[32m15/18\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.4:\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m15/18\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.4[0m\u001b[90m\u001b[0m \u001b[32m15/18\u001b[0m [huggingface-hub]\n",
      "\u001b[2K  Attempting uninstall: transformers\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m16/18\u001b[0m [tokenizers]]\n",
      "\u001b[2K    Found existing installation: transformers 4.53.2m\u001b[90m\u001b[0m \u001b[32m16/18\u001b[0m [tokenizers]\n",
      "\u001b[2K    Uninstalling transformers-4.53.2:\u001b[0m\u001b[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m17/18\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.53.2[91m\u001b[0m\u001b[90m\u001b[0m \u001b[32m17/18\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m18/18\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-core 0.3.68 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "scipy 1.10.1 requires numpy<1.27.0,>=1.19.5, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2025.8.3 charset_normalizer-3.4.2 filelock-3.17.0 fsspec-2025.7.0 hf-xet-1.1.7 huggingface-hub-0.34.4 idna-3.10 numpy-2.2.6 packaging-25.0 pyyaml-6.0.2 regex-2025.7.34 requests-2.32.4 safetensors-0.6.2 tokenizers-0.21.4 tqdm-4.67.1 transformers-4.53.2 typing-extensions-4.14.1 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.3 scipy==1.10.1\n",
    "!pip install transformers==4.53.2 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0b03c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
