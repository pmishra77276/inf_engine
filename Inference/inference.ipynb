{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3c95a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d83bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cuda')\n",
    "question=\"hi\"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "out=model.generate(**prompt,max_new_tokens=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24d57f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out=model.generate(**prompt,max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccc7fb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,   6151,     11,    358,   1097,   3411,    369,   1063,   1520,\n",
       "           449,    264,   3488], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ef9e3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, I am looking for some help with a question\n"
     ]
    }
   ],
   "source": [
    "# If out is a tensor (e.g., shape [1, seq_len])\n",
    "decoded = tok.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08669892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to('cpu')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47aff853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaRotaryEmbedding()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.rotary_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28b28405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "for i in model.model.layers:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1334cbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = torch.bfloat16\n",
    "bytes_per_param = torch.tensor([], dtype=dtype).element_size()\n",
    "bytes_per_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e186b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2265777587890625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=model.model.layers[0]\n",
    "num_params = sum(p.numel() for p in l.parameters())\n",
    "num_params\n",
    "param_size_bytes = num_params * 4\n",
    "param_size_MB = param_size_bytes / (1024 ** 3)\n",
    "param_size_MB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "679c19b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f41edcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec0798e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers=model.model.layers\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed31bb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers=layers[:-1]\n",
    "len(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e69f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 4\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7a34a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eaed17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n",
      "Warning: Not enough GPU memory. Available: 7.61 GB. Falling back to 1 layer per chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_worker/__main__.py\", line 10, in <module>\n",
      "    from torch._inductor.async_compile import pre_fork_setup\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/async_compile.py\", line 17, in <module>\n",
      "    from torch._dynamo.device_interface import get_registered_device_interfaces\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/__init__.py\", line 13, in <module>\n",
      "    from . import config, convert_frame, eval_frame, resume_execution\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py\", line 52, in <module>\n",
      "    from torch._dynamo.symbolic_convert import TensorifyState\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py\", line 52, in <module>\n",
      "    from torch._dynamo.exc import TensorifyScalarRestartAnalysis\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/exc.py\", line 41, in <module>\n",
      "    from .utils import counters\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/utils.py\", line 69, in <module>\n",
      "    import torch.fx.experimental.symbolic_shapes\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/fx/experimental/symbolic_shapes.py\", line 67, in <module>\n",
      "    from torch.utils._sympy.functions import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/utils/_sympy/functions.py\", line 9, in <module>\n",
      "    import sympy\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/__init__.py\", line 74, in <module>\n",
      "    from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/__init__.py\", line 124, in <module>\n",
      "    from .partfrac import apart, apart_list, assemble_partfrac_list\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/partfrac.py\", line 15, in <module>\n",
      "    def apart(f, x=None, full=False, **options):\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/utilities/decorator.py\", line 76, in xthreaded\n",
      "    return threaded_factory(func, False)\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/utilities/decorator.py\", line 13, in threaded_factory\n",
      "    from sympy.matrices import MatrixBase\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/__init__.py\", line 8, in <module>\n",
      "    from .dense import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/dense.py\", line 14, in <module>\n",
      "    from .matrixbase import MatrixBase\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/matrixbase.py\", line 60, in <module>\n",
      "    from .solvers import (\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/solvers.py\", line 6, in <module>\n",
      "    from .eigen import _fuzzy_positive_definite\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/matrices/eigen.py\", line 15, in <module>\n",
      "    from sympy.polys.matrices.eigen import dom_eigenvects, dom_eigenvects_to_sympy\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/matrices/eigen.py\", line 8, in <module>\n",
      "    from ..agca.extensions import FiniteExtension\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/agca/__init__.py\", line 3, in <module>\n",
      "    from .homomorphisms import homomorphism\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/agca/homomorphisms.py\", line 10, in <module>\n",
      "    from sympy.polys.agca.modules import (Module, FreeModule, QuotientModule,\n",
      "  File \"/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/sympy/polys/agca/modules.py\", line 24, in <module>\n",
      "    from sympy.polys.agca.ideals import Ideal\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 879, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1012, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 672, in _compile_bytecode\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 237\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting inference...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 237\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n",
      "Cell \u001b[0;32mIn[28], line 190\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens, temperature, top_p)\u001b[0m\n\u001b[1;32m    188\u001b[0m l1 \u001b[38;5;241m=\u001b[39m get_layers(model, \u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# Use the calculated l1 value, not a hardcoded number\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "Cell \u001b[0;32mIn[28], line 127\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mstream(stream_compute):\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 127\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mm_current\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Check if this is the last chunk\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rem \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1432\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1426\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1427\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1428\u001b[0m             )\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1431\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1213\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1211\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1216\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:598\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    595\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1059\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m   1057\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1059\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m     put_code_state()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_utils_internal.py:97\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# This is not needed but we have it here to avoid having profile_compile_time\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# in stack traces when profiling is not enabled.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m    100\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    101\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:761\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    759\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39minstall_callbacks())\n\u001b[1;32m    760\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    764\u001b[0m     ConvertFrameReturn()\n\u001b[1;32m    765\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:797\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    795\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1422\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1419\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1420\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1422\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:257\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:715\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 715\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    717\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3498\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 3498\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1337\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_point \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer\n\u001b[0;32m-> 1337\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1338\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1246\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3699\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3698\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 3699\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3684\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3679\u001b[0m _step_logger()(\n\u001b[1;32m   3680\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   3681\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3682\u001b[0m )\n\u001b[1;32m   3683\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 3684\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   3688\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3690\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3691\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3693\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   3694\u001b[0m )\n\u001b[1;32m   3695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1144\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1141\u001b[0m append_prefix_insts()\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m# optimization to generate better code in a common case\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[0;32m-> 1144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mreversed\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstack_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_replacements\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m     \u001b[38;5;241m+\u001b[39m [create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNPACK_SEQUENCE\u001b[39m\u001b[38;5;124m\"\u001b[39m, arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(stack_values))]\n\u001b[1;32m   1148\u001b[0m )\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;66;03m# restore all the live local vars\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_output_instructions(\n\u001b[1;32m   1151\u001b[0m     [\n\u001b[1;32m   1152\u001b[0m         PyCodegen(tx, overridden_sources\u001b[38;5;241m=\u001b[39moverridden_sources)\u001b[38;5;241m.\u001b[39mcreate_store(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1156\u001b[0m     ]\n\u001b[1;32m   1157\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1437\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root, replaced_outputs)\u001b[0m\n\u001b[1;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1437\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1447\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1487\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1483\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1484\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1485\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1486\u001b[0m     ):\n\u001b[0;32m-> 1487\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1519\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1518\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1519\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1520\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:150\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/__init__.py:2347\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2088\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\n\u001b[1;32m   2082\u001b[0m     V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode),\n\u001b[1;32m   2083\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(tracing_context),\n\u001b[1;32m   2084\u001b[0m     compiled_autograd\u001b[38;5;241m.\u001b[39m_disable(),\n\u001b[1;32m   2085\u001b[0m     functorch_config\u001b[38;5;241m.\u001b[39mpatch(unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m   2086\u001b[0m ):\n\u001b[1;32m   2087\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2088\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2091\u001b[0m \u001b[43m            \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2093\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2094\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2095\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2096\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2097\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2098\u001b[0m         \u001b[38;5;66;03m# We will also shorten the traceback inside dynamo.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m         \u001b[38;5;66;03m# This is only useful if inductor is called directly with an FX graph.\u001b[39;00m\n\u001b[1;32m   2100\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:101\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m--> 101\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1168\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local \u001b[38;5;129;01mor\u001b[39;00m remote:\n\u001b[1;32m   1167\u001b[0m     set_feature_use(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_remote_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, remote)\n\u001b[0;32m-> 1168\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mAOTAutogradCache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdispatch_and_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1178\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m dispatch_and_compile()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:695\u001b[0m, in \u001b[0;36mAOTAutogradCache.load\u001b[0;34m(dispatch_and_compile, mod, args, aot_config, cudagraphs, local, remote)\u001b[0m\n\u001b[1;32m    693\u001b[0m fx_config: _CompileFxKwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudagraphs\u001b[39m\u001b[38;5;124m\"\u001b[39m: cudagraphs}\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m     cache_key, debug_lines \u001b[38;5;241m=\u001b[39m \u001b[43mautograd_cache_key\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m     entry: Optional[AOTAutogradCacheEntry] \u001b[38;5;241m=\u001b[39m AOTAutogradCache\u001b[38;5;241m.\u001b[39m_lookup(\n\u001b[1;32m    699\u001b[0m         cache_key, local, remote\n\u001b[1;32m    700\u001b[0m     )\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m entry \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:331\u001b[0m, in \u001b[0;36mautograd_cache_key\u001b[0;34m(gm, example_inputs, config, fx_config)\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m triton\u001b[38;5;241m.\u001b[39m__version__ \u001b[38;5;241m<\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m BypassAOTAutogradCache(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAOTAutogradCache requires triton 3.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 331\u001b[0m details \u001b[38;5;241m=\u001b[39m \u001b[43mAOTAutogradCacheDetails\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m pickler \u001b[38;5;241m=\u001b[39m AOTAutogradCachePickler(gm)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# The prefix distinguishes among the other kinds of objects we cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/autograd_cache.py:264\u001b[0m, in \u001b[0;36mAOTAutogradCacheDetails.__init__\u001b[0;34m(self, gm, example_inputs, aot_config, fx_config)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# FXGraphCache has constraints on what can be pickled in its inductor\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# config. Check that the gm is cacheable by inductor first,\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;66;03m# and if it raises an exception, also bypass on our end.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     FxGraphCache\u001b[38;5;241m.\u001b[39m_check_can_cache(gm)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfx_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BypassFxGraphCache \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# Sometimes inductor configs are unpickleable and can fail\u001b[39;00m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BypassAOTAutogradCache \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:830\u001b[0m, in \u001b[0;36mFxGraphHashDetails.__init__\u001b[0;34m(self, gm, example_inputs, fx_kwargs, inputs_to_check)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda_matmul_settings \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    824\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_tf32,\n\u001b[1;32m    825\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_fp16_reduced_precision_reduction,\n\u001b[1;32m    826\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmatmul\u001b[38;5;241m.\u001b[39mallow_bf16_reduced_precision_reduction,\n\u001b[1;32m    827\u001b[0m )\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Also hash on various system info (including the triton compiler version).\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtorch_version \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msystem_info \u001b[38;5;241m=\u001b[39m CacheBase\u001b[38;5;241m.\u001b[39mget_system()\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minductor_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39msave_config_portable()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:695\u001b[0m, in \u001b[0;36mtorch_key\u001b[0;34m()\u001b[0m\n\u001b[1;32m    692\u001b[0m                     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    693\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m hasher\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[0;32m--> 695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_TORCH_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlibfb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parutil\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m parutil\u001b[38;5;241m.\u001b[39mget_file_contents(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch/src_hash.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:688\u001b[0m, in \u001b[0;36mtorch_key.<locals>.get_code_hash\u001b[0;34m(root)\u001b[0m\n\u001b[1;32m    686\u001b[0m hasher \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha256()\n\u001b[1;32m    687\u001b[0m hasher\u001b[38;5;241m.\u001b[39mupdate(torch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m--> 688\u001b[0m \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m extra_files:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path):\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:664\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mispkg:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# need to also hash submodules\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule_search_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:664\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mispkg:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# need to also hash submodules\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule_search_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:664\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    661\u001b[0m     hasher\u001b[38;5;241m.\u001b[39mupdate(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mispkg:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;66;03m# need to also hash submodules\u001b[39;00m\n\u001b[0;32m--> 664\u001b[0m     \u001b[43mbuild_code_hash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmodule_search_locations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhasher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/codecache.py:654\u001b[0m, in \u001b[0;36mbuild_code_hash\u001b[0;34m(roots, prefix, hasher)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_code_hash\u001b[39m(\n\u001b[1;32m    652\u001b[0m     roots: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, prefix: \u001b[38;5;28mstr\u001b[39m, hasher: hashlib\u001b[38;5;241m.\u001b[39m_Hash\n\u001b[1;32m    653\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkgutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    655\u001b[0m         spec \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmodule_finder\u001b[38;5;241m.\u001b[39mfind_spec(lib\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/pkgutil.py:130\u001b[0m, in \u001b[0;36miter_modules\u001b[0;34m(path, prefix)\u001b[0m\n\u001b[1;32m    128\u001b[0m yielded \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m importers:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, ispkg \u001b[38;5;129;01min\u001b[39;00m iter_importer_modules(i, prefix):\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m yielded:\n\u001b[1;32m    132\u001b[0m             yielded[name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/pkgutil.py:168\u001b[0m, in \u001b[0;36m_iter_file_finder_modules\u001b[0;34m(importer, prefix)\u001b[0m\n\u001b[1;32m    166\u001b[0m modname \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m     dircontents \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# ignore unreadable directories like import does\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     dircontents \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with three streams\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk on the load stream\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    rem -= layers_per_chunk\n",
    "\n",
    "    m_next = None\n",
    "    \n",
    "    while True:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        if rem > 0:\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        else:\n",
    "            m_next = None\n",
    "\n",
    "        # Wait for the current chunk to finish loading\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "\n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "\n",
    "        # Check if this is the last chunk\n",
    "        if rem <= 0:\n",
    "            break\n",
    "        \n",
    "        # Offload the current chunk asynchronously\n",
    "        with torch.cuda.stream(stream_offload):\n",
    "            m_current.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline and update counters\n",
    "        m_current = m_next\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    # The last m_current must be moved back to cpu explicitly\n",
    "    m_current.to('cpu')\n",
    "    del m_current\n",
    "    if m_next:\n",
    "        del m_next\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250, temperature=0.7, top_p=0.95):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            # Use the calculated l1 value, not a hardcoded number\n",
    "            x = compute(model, x, l1, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Apply temperature and Top-P sampling\n",
    "        logits = x[:, -1, :] / temperature\n",
    "        \n",
    "        # Sort logits in descending order\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        \n",
    "        # Calculate cumulative probabilities\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        \n",
    "        # Find the tokens that are in the nucleus\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        \n",
    "        # Shift the indices to the right to keep at least one token\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        # Mask out the logits that are not in the nucleus\n",
    "        logits[sorted_indices[sorted_indices_to_remove]] = -float('inf')\n",
    "        \n",
    "        # Sample a token from the nucleus\n",
    "        next_token = torch.multinomial(torch.softmax(logits, dim=-1), num_samples=1)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f60ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Enable CUDA and Tensor Core optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "# Corrected the missing closing quote in the tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             # quantization_config=bnb_config\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d74102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Computation\n",
    "    Worker 2: Loading new layers\n",
    "    Worker 3: Offloading old layers\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    m_current = None\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    # Initialize offload module to None for the first iteration\n",
    "    m_to_offload = None\n",
    "    \n",
    "    while i <= total_layers:\n",
    "        # Load the next chunk if there are layers remaining\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        # Wait for the current chunk to finish loading before computing\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        \n",
    "        # Compute on the current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # Offload the previously used chunk (if it exists)\n",
    "        if m_to_offload:\n",
    "            # Wait for compute to finish before offloading to prevent race condition\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "\n",
    "        # Advance the pipeline\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Handle the final cleanup outside the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    # Ensure the last computed module is offloaded as well\n",
    "    if m_to_offload:\n",
    "        m_to_offload.to('cpu')\n",
    "    if m_current: # Should be None but for safety\n",
    "        m_current.to('cpu')\n",
    "    if m_next:\n",
    "        m_next.to('cpu')\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9db5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centerslyphlyphlyphlyphlyphlyphlyphlyphlyph\n",
      "Time taken for 10 tokens: 9.55 seconds\n",
      "\n",
      "Total time 9.552647829055786\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting inference...\")\n",
    "start = time.time()\n",
    "model_inference(\"HI\", 10)\n",
    "print()\n",
    "print(\"Total time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa1fd1",
   "metadata": {},
   "source": [
    "# 2 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76b6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fb926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_layers(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80fb67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(model.model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3aaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "layers=model.model.layers\n",
    "\n",
    "\n",
    "# Usage\n",
    "# compiled_block = torch.compile(FusedBlock(layers[i:j]))\n",
    "\n",
    "# def compute(model, x, layers_per_chunk, cos, sin,k,max_new_tokens,attention_mask):\n",
    "#     total_layers = len(model.model.layers)\n",
    "#     gpu_stream = torch.cuda.current_stream()\n",
    "#     if layers_per_chunk >= total_layers:\n",
    "#         # model = model.to('cuda')\n",
    "#         # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "#         # return out.argmax(-1)\n",
    "#         i=0\n",
    "#         l=total_layers\n",
    "#         m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "#         x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         return x\n",
    "#     i = 0\n",
    "#     rem = total_layers\n",
    "#     while rem > 0:\n",
    "#         l = min(layers_per_chunk, rem)\n",
    "        \n",
    "#         m = Module(model.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "#         # torch.cuda.current_stream().wait_stream(gpu_stream)\n",
    "#         # m=torch.compile(m)\n",
    "#         # Run chunk\n",
    "#         with torch.no_grad():\n",
    "#             x=m(x.to('cuda', non_blocking=True),cos.to('cuda',non_blocking=True),sin.to('cuda',non_blocking=True),attention_mask.to('cuda'))\n",
    "#         if(k==max_new_tokens-1):\n",
    "#             m=m.to('cpu')\n",
    "#             torch.cuda.empty_cache()\n",
    "#         m=m.to('cpu')\n",
    "#         torch.cuda.empty_cache()\n",
    "#         # update indices\n",
    "#         i += l\n",
    "#         rem -= l\n",
    "#     torch.cuda.current_stream().synchronize()\n",
    "#     return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i=0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True), \n",
    "                              cos.to('cuda', non_blocking=True), \n",
    "                              sin.to('cuda', non_blocking=True), \n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            m_current = m_current.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_current = m_current.to('cpu', non_blocking=True)\n",
    "            del m_current\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        torch.cuda.empty_cache() \n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec619479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda') \n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,1,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfb2f9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e865b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.548299551010132\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2460c40",
   "metadata": {},
   "source": [
    "# other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             use_cache=True,\n",
    "                                             )\n",
    "model.to('cpu')\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0bce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Calculates the maximum number of layers that can fit on the GPU.\n",
    "    This function has been improved to account for static model parts\n",
    "    and provide a more robust chunk size calculation.\n",
    "\n",
    "    Args:\n",
    "        model: The model instance.\n",
    "        drop (int): A memory buffer in GB to leave on the GPU.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of layers per chunk.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2 # Using bfloat16 for correct size calculation\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Adjusting for static model parts which are loaded once\n",
    "    static_mem_gb = 0\n",
    "    if hasattr(model.model, 'embed_tokens'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.embed_tokens.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model.model, 'norm'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.model.norm.parameters()) * 2 / (1024**3)\n",
    "    if hasattr(model, 'lm_head'):\n",
    "        static_mem_gb += sum(p.numel() for p in model.lm_head.parameters()) * 2 / (1024**3)\n",
    "\n",
    "    available_gpu_memory_for_layers_gb = device_memory - drop - static_mem_gb\n",
    "    if available_gpu_memory_for_layers_gb <= 0:\n",
    "        print(f\"Warning: Not enough GPU memory. Available: {device_memory:.2f} GB. Falling back to 1 layer per chunk.\")\n",
    "        return 1\n",
    "    \n",
    "    if param_size_GB == 0:\n",
    "        return 1\n",
    "\n",
    "    return max(1, int(available_gpu_memory_for_layers_gb / param_size_GB))\n",
    "\n",
    "\n",
    "class Module(nn.Module):\n",
    "    \"\"\"A wrapper for a chunk of layers.\"\"\"\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "    \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            # Pass the attention mask to the layer\n",
    "            x = layer(x, position_embeddings=(cos, sin), attention_mask=attention_mask)[0]\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    \"\"\"\n",
    "    Computes a forward pass using a three-worker pipeline with three explicit streams.\n",
    "    Worker 1: Loading new layers (stream_load)\n",
    "    Worker 2: Computation (stream_compute)\n",
    "    Worker 3: Offloading old layers (stream_offload)\n",
    "    \"\"\"\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_load = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    # Case 1: All layers fit on the GPU, no offloading needed.\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            del m\n",
    "        return x\n",
    "\n",
    "    # Case 2: Pipelined offloading with a robust, explicit three-stage loop.\n",
    "    i = 0\n",
    "    m_to_offload = None # This will hold the chunk from the previous iteration to be offloaded\n",
    "    m_current = None # This is the chunk currently being computed\n",
    "    \n",
    "    # Priming the pipeline: Load the first chunk\n",
    "    with torch.cuda.stream(stream_load):\n",
    "        m_current = Module(model_instance.model.layers, i, i + layers_per_chunk).to('cuda', non_blocking=True)\n",
    "    i += layers_per_chunk\n",
    "    \n",
    "    while i <= total_layers + layers_per_chunk: # Loop until all layers are processed and the last chunk is offloaded\n",
    "        # 1. Load the next chunk on a separate stream if there are layers left\n",
    "        m_next = None\n",
    "        if i < total_layers:\n",
    "            l_next = min(layers_per_chunk, total_layers - i)\n",
    "            with torch.cuda.stream(stream_load):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "        \n",
    "        # 2. Wait for the current chunk to finish loading, then compute\n",
    "        stream_compute.wait_stream(stream_load)\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x, cos, sin, attention_mask)\n",
    "        \n",
    "        # 3. Wait for the computation to finish, then offload the previous chunk\n",
    "        if m_to_offload:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_to_offload.to('cpu', non_blocking=True)\n",
    "        \n",
    "        # Advance the pipeline to the next stage\n",
    "        m_to_offload = m_current\n",
    "        m_current = m_next\n",
    "        i += layers_per_chunk\n",
    "\n",
    "    # Final cleanup after the loop\n",
    "    stream_compute.synchronize()\n",
    "    stream_load.synchronize()\n",
    "    stream_offload.synchronize()\n",
    "    \n",
    "    del m_to_offload, m_current, m_next\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    start_total_time = time.time()\n",
    "    \n",
    "    model.to('cpu')\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "    input_ids = prompt['input_ids'].to('cuda')\n",
    "    # Correctly move attention mask to CUDA and convert its dtype\n",
    "    attention_mask = prompt['attention_mask'].to('cuda')\n",
    "    del prompt\n",
    "\n",
    "    # Move static parts of the model to GPU once\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    generated_text_tokens = [] \n",
    "\n",
    "    for k in range(tokens):\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        \n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "\n",
    "        # Create a 4D attention mask with the correct dtype\n",
    "        causal_mask = torch.triu(torch.ones((1, seq_len, seq_len), device=x.device, dtype=torch.bool), diagonal=1)\n",
    "        causal_mask = ~causal_mask\n",
    "        causal_mask = causal_mask.view(1, 1, seq_len, seq_len)\n",
    "        causal_mask = causal_mask * attention_mask.view(1, 1, 1, seq_len)\n",
    "        causal_mask = (causal_mask == 0).float() * torch.finfo(x.dtype).min\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Forcing layers_per_chunk to 5 as requested to test pipelining\n",
    "            layers_per_chunk = 5\n",
    "            x = compute(model, x, layers_per_chunk, cos, sin, k, tokens, causal_mask)\n",
    "            \n",
    "            x = model.model.norm(x)\n",
    "            x = model.lm_head(x)\n",
    "        \n",
    "        # Reverting to the original greedy decoding using argmax as requested\n",
    "        x1 = x[:,-1,:]\n",
    "        next_token = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        \n",
    "        print(tok.decode(next_token[0]), end=\"\")\n",
    "        generated_text_tokens.append(tok.decode(next_token[0]))\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token.to('cuda')], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones(attention_mask.shape[0], 1).to('cuda')], dim=-1)\n",
    "    \n",
    "    # Final cleanup at the very end\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache() \n",
    "    \n",
    "    end_total_time = time.time()\n",
    "    print(f\"\\nTime taken for {tokens} tokens: {end_total_time - start_total_time:.2f} seconds\")\n",
    "    return \"\".join(generated_text_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6fb1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "birbirbirbirbirbirbirbirbirbir\n",
      "Time taken for 10 tokens: 4.10 seconds\n",
      "\n",
      "Total time 4.095994472503662\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ae734",
   "metadata": {},
   "source": [
    "12.1\n",
    "11.9\n",
    "11.5\n",
    "11.9\n",
    "10.9\n",
    "11.1\n",
    "10.3\n",
    "9.5\n",
    "9.0 (10)\n",
    "9.3 (11)\n",
    "10.5 (12)\n",
    "9.8 (13)\n",
    "9.9 (14)\n",
    "9.1 (15)\n",
    "\n",
    "1.9 (16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c91944",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b454b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=nn.ModuleList(layers[i:j+1])\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3a3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        model = model.to('cuda')\n",
    "        out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        return out.argmax(-1)\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l)\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cpu'), cos.to('cpu'), sin.to('cpu'))\n",
    "\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c6d6f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39589d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfb529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2278, -0.1952,  1.0512,  ...,  0.4470,  0.9027,  0.4017],\n",
       "         [-0.0171,  0.0437,  0.0796,  ..., -0.0353, -0.0933, -0.0494],\n",
       "         [-0.0076,  0.0305, -0.0894,  ..., -0.1101, -0.0858,  0.0215],\n",
       "         [-0.0955,  0.1930, -0.0288,  ..., -0.0590, -0.0255,  0.1503],\n",
       "         [-0.0718,  0.0508, -0.0462,  ...,  0.0025, -0.0664, -0.0365]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,7)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06aaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "input_ids=prompt['input_ids']\n",
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    def forward(self, x, cos, sin):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "def compute(model, x, layers_per_chunk, cos, sin):\n",
    "    total_layers = len(model.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        # model = model.to('cuda')\n",
    "        # out = model({'inputs_embeds': x})['logits'][:, -1, :]\n",
    "        # return out.argmax(-1)\n",
    "        i=0\n",
    "        l=total_layers\n",
    "        m=Module(model.model.layers,i,i+l).to('cuda')\n",
    "        x=m(x.to('cuda'),cos.to('cuda'),sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        return x\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    while rem > 0:\n",
    "        l = min(layers_per_chunk, rem)\n",
    "        m = Module(model.model.layers, i, i + l).to('cuda')\n",
    "\n",
    "        # Run chunk\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'))\n",
    "        m=m.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # update indices\n",
    "        i += l\n",
    "        rem -= l\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c7af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4012,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,get_layers(model),cos,sin)\n",
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf313c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d5247463e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4072, -0.0109,  0.1410,  ...,  0.2553, -0.2387, -0.3508],\n",
       "         [ 0.1307, -0.0781,  0.1387,  ...,  0.4531,  0.1098, -0.2780],\n",
       "         [ 0.1409,  0.0029,  0.2776,  ...,  0.8693,  0.6018, -0.1494],\n",
       "         [-0.0449,  0.0310,  0.0482,  ...,  0.3607, -0.3491, -0.5668],\n",
       "         [-0.0326,  0.3774, -0.2075,  ...,  0.3641, -0.2429, -0.2585]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=compute(model,x,4,cos,sin)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd69379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4545,  2.3900, -1.2563,  ..., -1.2801, -1.2815, -1.2818],\n",
       "         [-0.5335,  1.2898, -0.0679,  ..., -0.9111, -0.9123, -0.9122],\n",
       "         [ 0.2582,  0.9092,  1.3728,  ..., -1.5733, -1.5758, -1.5756],\n",
       "         [-1.0597,  2.9733,  0.6937,  ..., -0.5993, -0.6006, -0.6005],\n",
       "         [ 0.2114,  0.6242,  1.3618,  ..., -0.0651, -0.0660, -0.0650]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.norm.to('cpu')\n",
    "x=model.model.norm(y)\n",
    "model.model.norm.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "model.lm_head.to('cpu')\n",
    "x=model.lm_head(x)\n",
    "# tok.decode(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98977648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   3\n",
      "4   7\n",
      "8   11\n",
      "12   15\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "j=3\n",
    "l=4\n",
    "rem=16\n",
    "while(rem!=0):\n",
    "    if(rem>=l):\n",
    "        m=nn.Sequential(\n",
    "            layers[i:j]\n",
    "        )\n",
    "        print(i,\" \",j)\n",
    "        rem-=l\n",
    "        i+=l;j+=l\n",
    "    else:\n",
    "        m=nn.Sequential(\n",
    "            layers[i:i+rem-1]\n",
    "        )\n",
    "        print('else')\n",
    "        print(i,\" \",i+rem-1)\n",
    "        rem=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31e4353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ModuleList(\n",
       "    (0-2): 3 x LlamaDecoderLayer(\n",
       "      (self_attn): LlamaAttention(\n",
       "        (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      )\n",
       "      (mlp): LlamaMLP(\n",
       "        (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=nn.Sequential(\n",
    "        layers[0:3]\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd3ccb6",
   "metadata": {},
   "source": [
    "if int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 > total layers  : load whole model into sequential model\n",
    "\n",
    "else load int((torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)/param_size_MB)-2 layers into the seq \n",
    "\n",
    "keep track of the layers which went inside the seq model\n",
    "\n",
    "[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] if 6-2=4 layers are allowed then iterations = while i not>= number of layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.814697265625e-06\n",
      "0.4892578125\n",
      "0.4892578125\n"
     ]
    }
   ],
   "source": [
    "# LlamaRMSNorm((2048,), eps=1e-05)\n",
    "#     (rotary_emb): LlamaRotaryEmbedding()\n",
    "# print((sum(p.numel() for p in model.model.norm.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.lm_head.parameters())*2)/(1024**3))\n",
    "# print((sum(p.numel() for p in model.model.embed_tokens.parameters())*2)/(1024**3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889b1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa53c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=input()\n",
    "converstation=[\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are an assitant help user by solving their query\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":f\"query-{question}\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16aff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"hi how are \"\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "# prompt=tok.apply_chat_template(converstation)\n",
    "prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "# prompt.to('cpu'//)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95944161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,   6151,   1268,    527,    220]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out=model.generate(**prompt,max_new_tokens=1000)\n",
    "# print(tok.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68deb58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt['input_ids'].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79079cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class module(nn.Module):\n",
    "    def __init__(self,layers,i,j):\n",
    "        super().__init__()\n",
    "        self.model=layers[i:j]\n",
    "    def forward(self,x,cos,sin):\n",
    "        for l in self.model:\n",
    "            x=l(x,position_embeddings=(cos,sin))[0]\n",
    "            print()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373bfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2181, -0.3543,  1.0459,  ...,  0.3998,  0.9639,  0.3820],\n",
       "         [ 0.0501, -0.0546, -0.0243,  ...,  0.1164, -0.1678,  0.0384],\n",
       "         [ 0.0353, -0.0942, -0.1205,  ..., -0.0287, -0.1063,  0.0035],\n",
       "         [ 0.0649,  0.0549, -0.0413,  ..., -0.0517, -0.0086,  0.0737],\n",
       "         [-0.0263,  0.0541, -0.0978,  ...,  0.0248, -0.0258, -0.0238]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=module(layers,0,3)\n",
    "m(x.to('cpu'),cos.to('cpu'),sin.to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6659f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fd1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
      "           8.4305e-04,  7.0190e-04],\n",
      "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
      "           9.5215e-03,  4.7363e-02],\n",
      "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
      "          -4.8828e-03,  1.2451e-02],\n",
      "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
      "          -1.5869e-02,  1.1520e-03],\n",
      "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
      "          -1.8677e-02, -3.5156e-02]]], grad_fn=<ToCopyBackward0>)\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [ 0.5403,  0.7878,  0.9046,  0.9576,  0.9813,  0.9917,  0.9964,\n",
      "           0.9984,  0.9993,  0.9997,  0.9999,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  0.5403,  0.7878,  0.9046,\n",
      "           0.9576,  0.9813,  0.9917,  0.9964,  0.9984,  0.9993,  0.9997,\n",
      "           0.9999,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.4161,  0.2412,  0.6366,  0.8340,  0.9257,  0.9671,  0.9855,\n",
      "           0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.4161,  0.2412,  0.6366,\n",
      "           0.8340,  0.9257,  0.9671,  0.9855,  0.9936,  0.9972,  0.9988,\n",
      "           0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.9900, -0.4078,  0.2471,  0.6397,  0.8355,  0.9264,  0.9674,\n",
      "           0.9856,  0.9936,  0.9972,  0.9988,  0.9995,  0.9998,  0.9999,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.9900, -0.4078,  0.2471,\n",
      "           0.6397,  0.8355,  0.9264,  0.9674,  0.9856,  0.9936,  0.9972,\n",
      "           0.9988,  0.9995,  0.9998,  0.9999,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000],\n",
      "         [-0.6536, -0.8837, -0.1895,  0.3912,  0.7139,  0.8704,  0.9422,\n",
      "           0.9744,  0.9887,  0.9950,  0.9978,  0.9990,  0.9996,  0.9998,\n",
      "           0.9999,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000, -0.6536, -0.8837, -0.1895,\n",
      "           0.3912,  0.7139,  0.8704,  0.9422,  0.9744,  0.9887,  0.9950,\n",
      "           0.9978,  0.9990,  0.9996,  0.9998,  0.9999,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "           1.0000]]])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "         [ 8.4147e-01,  6.1596e-01,  4.2627e-01,  2.8809e-01,  1.9271e-01,\n",
      "           1.2833e-01,  8.5293e-02,  5.6639e-02,  3.7597e-02,  2.4953e-02,\n",
      "           1.6560e-02,  1.0989e-02,  7.2926e-03,  4.8394e-03,  3.2114e-03,\n",
      "           1.2905e-03,  4.2956e-04,  9.7083e-05,  1.9462e-05,  1.2915e-05,\n",
      "           8.5703e-06,  5.6872e-06,  3.7741e-06,  2.5045e-06,  1.6620e-06,\n",
      "           1.1029e-06,  7.3187e-07,  4.8567e-07,  3.2229e-07,  2.1387e-07,\n",
      "           1.4193e-07,  9.4183e-08,  8.4147e-01,  6.1596e-01,  4.2627e-01,\n",
      "           2.8809e-01,  1.9271e-01,  1.2833e-01,  8.5293e-02,  5.6639e-02,\n",
      "           3.7597e-02,  2.4953e-02,  1.6560e-02,  1.0989e-02,  7.2926e-03,\n",
      "           4.8394e-03,  3.2114e-03,  1.2905e-03,  4.2956e-04,  9.7083e-05,\n",
      "           1.9462e-05,  1.2915e-05,  8.5703e-06,  5.6872e-06,  3.7741e-06,\n",
      "           2.5045e-06,  1.6620e-06,  1.1029e-06,  7.3187e-07,  4.8567e-07,\n",
      "           3.2229e-07,  2.1387e-07,  1.4193e-07,  9.4183e-08],\n",
      "         [ 9.0930e-01,  9.7048e-01,  7.7121e-01,  5.5175e-01,  3.7819e-01,\n",
      "           2.5454e-01,  1.6997e-01,  1.1310e-01,  7.5141e-02,  4.9890e-02,\n",
      "           3.3115e-02,  2.1977e-02,  1.4585e-02,  9.6787e-03,  6.4228e-03,\n",
      "           2.5811e-03,  8.5911e-04,  1.9417e-04,  3.8923e-05,  2.5830e-05,\n",
      "           1.7141e-05,  1.1374e-05,  7.5481e-06,  5.0089e-06,  3.3239e-06,\n",
      "           2.2058e-06,  1.4637e-06,  9.7135e-07,  6.4459e-07,  4.2775e-07,\n",
      "           2.8385e-07,  1.8837e-07,  9.0930e-01,  9.7048e-01,  7.7121e-01,\n",
      "           5.5175e-01,  3.7819e-01,  2.5454e-01,  1.6997e-01,  1.1310e-01,\n",
      "           7.5141e-02,  4.9890e-02,  3.3115e-02,  2.1977e-02,  1.4585e-02,\n",
      "           9.6787e-03,  6.4228e-03,  2.5811e-03,  8.5911e-04,  1.9417e-04,\n",
      "           3.8923e-05,  2.5830e-05,  1.7141e-05,  1.1374e-05,  7.5481e-06,\n",
      "           5.0089e-06,  3.3239e-06,  2.2058e-06,  1.4637e-06,  9.7135e-07,\n",
      "           6.4459e-07,  4.2775e-07,  2.8385e-07,  1.8837e-07],\n",
      "         [ 1.4112e-01,  9.1309e-01,  9.6899e-01,  7.6862e-01,  5.4950e-01,\n",
      "           3.7654e-01,  2.5340e-01,  1.6919e-01,  1.1258e-01,  7.4796e-02,\n",
      "           4.9661e-02,  3.2963e-02,  2.1876e-02,  1.4518e-02,  9.6342e-03,\n",
      "           3.8716e-03,  1.2887e-03,  2.9125e-04,  5.8385e-05,  3.8744e-05,\n",
      "           2.5711e-05,  1.7062e-05,  1.1322e-05,  7.5134e-06,  4.9859e-06,\n",
      "           3.3087e-06,  2.1956e-06,  1.4570e-06,  9.6688e-07,  6.4162e-07,\n",
      "           4.2578e-07,  2.8255e-07,  1.4112e-01,  9.1309e-01,  9.6899e-01,\n",
      "           7.6862e-01,  5.4950e-01,  3.7654e-01,  2.5340e-01,  1.6919e-01,\n",
      "           1.1258e-01,  7.4796e-02,  4.9661e-02,  3.2963e-02,  2.1876e-02,\n",
      "           1.4518e-02,  9.6342e-03,  3.8716e-03,  1.2887e-03,  2.9125e-04,\n",
      "           5.8385e-05,  3.8744e-05,  2.5711e-05,  1.7062e-05,  1.1322e-05,\n",
      "           7.5134e-06,  4.9859e-06,  3.3087e-06,  2.1956e-06,  1.4570e-06,\n",
      "           9.6688e-07,  6.4162e-07,  4.2578e-07,  2.8255e-07],\n",
      "         [-7.5680e-01,  4.6814e-01,  9.8188e-01,  9.2033e-01,  7.0021e-01,\n",
      "           4.9232e-01,  3.3498e-01,  2.2474e-01,  1.4986e-01,  9.9656e-02,\n",
      "           6.6193e-02,  4.3944e-02,  2.9167e-02,  1.9356e-02,  1.2845e-02,\n",
      "           5.1622e-03,  1.7182e-03,  3.8833e-04,  7.7847e-05,  5.1659e-05,\n",
      "           3.4281e-05,  2.2749e-05,  1.5096e-05,  1.0018e-05,  6.6479e-06,\n",
      "           4.4115e-06,  2.9275e-06,  1.9427e-06,  1.2892e-06,  8.5550e-07,\n",
      "           5.6771e-07,  3.7673e-07, -7.5680e-01,  4.6814e-01,  9.8188e-01,\n",
      "           9.2033e-01,  7.0021e-01,  4.9232e-01,  3.3498e-01,  2.2474e-01,\n",
      "           1.4986e-01,  9.9656e-02,  6.6193e-02,  4.3944e-02,  2.9167e-02,\n",
      "           1.9356e-02,  1.2845e-02,  5.1622e-03,  1.7182e-03,  3.8833e-04,\n",
      "           7.7847e-05,  5.1659e-05,  3.4281e-05,  2.2749e-05,  1.5096e-05,\n",
      "           1.0018e-05,  6.6479e-06,  4.4115e-06,  2.9275e-06,  1.9427e-06,\n",
      "           1.2892e-06,  8.5550e-07,  5.6771e-07,  3.7673e-07]]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[117], line 11\u001b[0m, in \u001b[0;36mmodule.forward\u001b[0;34m(self, x, cos, sin)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sin)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[0;32m---> 11\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:287\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, Optional[\u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mFloatTensor, torch\u001b[38;5;241m.\u001b[39mFloatTensor]]]:\n\u001b[1;32m    286\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 287\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(\n\u001b[1;32m    291\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    292\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    300\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:61\u001b[0m, in \u001b[0;36mLlamaRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[0;32m---> 61\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\n\u001b[1;32m     62\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     63\u001b[0m     variance \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d3c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0131, -0.0472,  0.0412,  ..., -0.0090,  0.0760, -0.0241],\n",
       "         [-0.0173, -0.0174, -0.0555,  ...,  0.0072, -0.0251, -0.0046],\n",
       "         [-0.0093,  0.0029, -0.1033,  ..., -0.0247, -0.0518,  0.0019],\n",
       "         [ 0.0227,  0.0492, -0.0206,  ..., -0.0301,  0.0070,  0.0488],\n",
       "         [-0.0341,  0.0040, -0.0712,  ..., -0.0127, -0.0116,  0.0161]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=model.model.layers[0]\n",
    "m(x.to('cpu'),position_embeddings=(cos.to('cpu'),sin.to('cpu')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f23b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377f44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x LlamaDecoderLayer(\n",
       "    (self_attn): LlamaAttention(\n",
       "      (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "      (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "      (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    )\n",
       "    (mlp): LlamaMLP(\n",
       "      (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out=model(prompt['input_ids'].to('cpu'))\n",
    "m=layers[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d086ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       grad_fn=<UnsafeViewBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x71d526b7b8e0>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(prompt['input_ids'].to('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0ddbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextt=out['logits'][:,-1,:].argmax(dim=-1)\n",
    "nextt\n",
    "tok.decode(nextt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059c40e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=prompt['input_ids']\n",
    "del prompt\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480b343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fadee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens.to('cuda')\n",
    "x = model.model.embed_tokens(input_ids)\n",
    "input_ids.to('cpu')\n",
    "# del input_ids\n",
    "torch.cuda.empty_cache()\n",
    "model.model.embed_tokens.to('cpu')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa8ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.8381e-03,  3.3264e-03, -9.8877e-03,  ..., -1.7700e-03,\n",
       "           8.4305e-04,  7.0190e-04],\n",
       "         [-4.0039e-02,  2.8320e-02,  2.8076e-02,  ...,  1.8311e-02,\n",
       "           9.5215e-03,  4.7363e-02],\n",
       "         [-7.2937e-03,  4.3335e-03,  3.0029e-02,  ...,  8.0109e-05,\n",
       "          -4.8828e-03,  1.2451e-02],\n",
       "         [-1.6785e-03,  1.7334e-02,  3.9551e-02,  ..., -1.1047e-02,\n",
       "          -1.5869e-02,  1.1520e-03],\n",
       "         [ 5.0964e-03, -2.4902e-02,  3.6133e-02,  ..., -1.6602e-02,\n",
       "          -1.8677e-02, -3.5156e-02]]], device='cuda:0',\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91acbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.decoder.embed_positions.to('cuda')\n",
    "# x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "# model.model.decoder.embed_positions.to('cpu')\n",
    "# # del model.model.decoder.embed_positions\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c6a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's max_position_embeddings: 131072\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model's max_position_embeddings: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b082e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f0ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "seq_len = x.shape[1]\n",
    "batch_size = x.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "rotary_emb = model.model.rotary_emb\n",
    "cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "cnt=0\n",
    "for i in model.model.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    i.to('cuda')\n",
    "    x.to('cuda')\n",
    "    # print(x)\n",
    "    with torch.no_grad():\n",
    "        x=i(x,position_embeddings=(cos,sin))\n",
    "        # print(x)\n",
    "    i.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    x=x[0]\n",
    "    # x.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "        # time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18251d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 2048])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f129a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.7762, -5.0166, 13.8534,  ..., -1.9152,  1.8989, -3.0755],\n",
       "         [-0.2314,  1.3026,  1.7797,  ..., -0.2708, -1.0364, -0.2083],\n",
       "         [ 0.2276,  0.8936,  0.8521,  ..., -0.0246, -1.4944, -0.0458],\n",
       "         [ 0.0748,  1.2204,  0.5945,  ..., -0.7305, -1.4607, -0.6370],\n",
       "         [ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc202f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6602,  1.3918, -0.1689,  ..., -1.1154, -1.7489, -0.1802]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1caa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.model.norm.to('cuda')\n",
    "    x=model.model.norm(x)\n",
    "    model.model.norm.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cuda')\n",
    "    x=model.lm_head(x)\n",
    "    model.lm_head.to('cpu')\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f836c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca9e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 7.0544,  9.0268, 13.3233,  ..., -3.7595, -3.7596, -3.7596],\n",
       "         [17.6009,  9.2103,  9.4629,  ..., -0.6160, -0.6167, -0.6170],\n",
       "         [12.9647,  9.7460,  8.3662,  ..., -0.2670, -0.2677, -0.2679],\n",
       "         [14.1844, 10.6885,  6.5441,  ...,  0.4011,  0.4009,  0.4002],\n",
       "         [ 7.1437,  4.0489,  5.2485,  ...,  1.8494,  1.8495,  1.8485]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560056fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprompt\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe40c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   6151,   1268,    527,    220,     17]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([input_ids,x1],dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc43b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 "
     ]
    }
   ],
   "source": [
    "x1=x[:,-1,:]\n",
    "x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "print(tok.decode(x1[0]),end=\" \")\n",
    "# print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9cd9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.6064453125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device=torch.device('cuda:0')\n",
    "props=torch.cuda.get_device_properties('cuda')\n",
    "props.total_memory/(1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a20b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda')\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    del prompt\n",
    "    for i in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        input_ids.to('cpu')\n",
    "        # del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # Safe check\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        cnt=0\n",
    "        for i in model.model.layers:\n",
    "            cnt+=1\n",
    "            # print(cnt)\n",
    "            i.to('cuda')\n",
    "            x.to('cuda')\n",
    "            # print(x)\n",
    "            with torch.no_grad():\n",
    "                x=i(x,position_embeddings=(cos,sin))\n",
    "                # print(x)\n",
    "            i.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x=x[0]\n",
    "            # x.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1) \n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501122ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I'm a new user and I'm trying"
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi\",10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(prompt_text, tokens=250):\n",
    "    # Initial tokenization\n",
    "    prompt = tok(prompt_text, return_tensors='pt')\n",
    "    input_ids = prompt['input_ids'].to('cuda')  # [1, seq_len]\n",
    "    del prompt\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # Run only the latest token through embed layer and model\n",
    "        model.model.embed_tokens.to('cuda')\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        model.model.embed_tokens.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        rotary_emb = model.model.rotary_emb\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "\n",
    "        # Transformer blocks\n",
    "        for layer in model.model.layers:\n",
    "            layer.to('cuda')\n",
    "            x = x.to('cuda')\n",
    "            with torch.no_grad():\n",
    "                x = layer(x, position_embeddings=(cos, sin))\n",
    "            layer.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            x = x[0]  # Remove tuple\n",
    "\n",
    "        # Final norm and lm_head\n",
    "        with torch.no_grad():\n",
    "            model.model.norm.to('cuda')\n",
    "            x = model.model.norm(x)\n",
    "            model.model.norm.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            model.lm_head.to('cuda')\n",
    "            logits = model.lm_head(x)\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        next_token_logits = logits[:, -1, :]  # Only the last token matters\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # shape [1,1]\n",
    "\n",
    "        # Append predicted token\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Decode and print latest token\n",
    "        print(tok.decode(next_token[0]), end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a694b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I am very happy to meet you all. I am a very friendly person and I love to meet new people. I am a very good listener and I am very easy going. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I am a very good singer and I love to sing. I am a very good swimmer and I love to swim. I am a very good gardener and I love to garden. I am a very good cook and I love to cook. I am a very good dancer and I love to dance. I"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi how are you ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(prompt_text, tokens)\u001b[0m\n\u001b[1;32m     39\u001b[0m     model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head(x)\n\u001b[0;32m---> 41\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     44\u001b[0m next_token_logits \u001b[38;5;241m=\u001b[39m logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# Only the last token matters\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1355\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:942\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 942\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    945\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1341\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1335\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1336\u001b[0m             device,\n\u001b[1;32m   1337\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1338\u001b[0m             non_blocking,\n\u001b[1;32m   1339\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1340\u001b[0m         )\n\u001b[0;32m-> 1341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_inference(\"Hi how are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d98000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_oov(text, tokenizer, max_token_id=2048, oov_token='[OOV]'):\n",
    "    # First, add [OOV] token to tokenizer if not present\n",
    "    if oov_token not in tokenizer.get_vocab():\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': [oov_token]})\n",
    "\n",
    "    oov_id = tokenizer.convert_tokens_to_ids(oov_token)\n",
    "\n",
    "    # Raw tokenization\n",
    "    encoding = tokenizer(text, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = encoding['input_ids'][0]\n",
    "\n",
    "    # Replace tokens ≥ max_token_id with OOV\n",
    "    processed_ids = []\n",
    "    for token_id in input_ids:\n",
    "        if token_id >= max_token_id:\n",
    "            processed_ids.append(oov_id)\n",
    "        else:\n",
    "            processed_ids.append(token_id)\n",
    "\n",
    "    return torch.tensor([processed_ids]), encoding['attention_mask']\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    try:\n",
    "        for _ in range(tokens):\n",
    "            # prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "            # input_ids = prompt['input_ids']\n",
    "            # attention_mask = prompt['attention_mask']\n",
    "            input_ids, attention_mask = tokenize_with_oov(question, tok)\n",
    "\n",
    "            del prompt\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Truncate if too long\n",
    "            batch_size, seq_len = input_ids.shape\n",
    "            max_positions = model.config.max_position_embeddings\n",
    "            if seq_len > max_positions:\n",
    "                input_ids = input_ids[:, -max_positions:]\n",
    "                attention_mask = attention_mask[:, -max_positions:]\n",
    "                seq_len = input_ids.shape[1]\n",
    "\n",
    "            # Embeddings\n",
    "            model.model.decoder.embed_tokens.to('cpu')\n",
    "            x = model.model.decoder.embed_tokens(input_ids)\n",
    "\n",
    "            # Positional IDs\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device='cpu')\n",
    "            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "            model.model.decoder.embed_positions.to('cpu')\n",
    "            x = x + model.model.decoder.embed_positions(position_ids)\n",
    "\n",
    "            # Decoder layers\n",
    "            for layer in model.model.decoder.layers:\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                with torch.no_grad():\n",
    "                    x = layer(x, attention_mask=attention_mask.to(torch.float32))[0]\n",
    "                layer.to('cpu')\n",
    "                x = x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Final LM head\n",
    "            model.lm_head.to('cpu')\n",
    "            logits = model.lm_head(x.to('cpu'))\n",
    "            model.lm_head.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Decode\n",
    "            next_token_id = logits.argmax(-1)\n",
    "            next_token_text = tok.decode(next_token_id[0], skip_special_tokens=True)\n",
    "            print(next_token_text, end=\" \")\n",
    "            question += \" \" + next_token_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n[Exception Caught]\")\n",
    "        print(\"Question So Far:\", question)\n",
    "        prompt = tok(question, return_tensors='pt').to('cpu')\n",
    "        input_ids = prompt['input_ids']\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        y = torch.arange(x.shape[1]).unsqueeze(0)\n",
    "        print(\"Embedding Shape:\", x[0].shape)\n",
    "        print(\"Position ID Shape:\", y.shape)\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc1b9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy how are you 're post .''. you you'rewepost navigation .''. your you'reTheze is It.''..''.'re'reiresolicy姫conservancy\n",
      "\n",
      "torch.Size([35, 768])\n",
      "torch.Size([1, 35])\n",
      "index out of range in self\n"
     ]
    }
   ],
   "source": [
    "model_inference(\"how are you 're post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa955c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Ensure model components are initially on CPU\n",
    "    model.model.decoder.embed_tokens.to('cpu')\n",
    "    model.model.decoder.embed_positions.to('cpu')\n",
    "    for layer in model.model.decoder.layers:\n",
    "        layer.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    \n",
    "    max_position_embeddings = model.config.max_position_embeddings\n",
    "    print(f\"Model's max_position_embeddings: {max_position_embeddings}\")\n",
    "\n",
    "    # Initialize input_ids_for_next_step. This will be the tensor that grows.\n",
    "    input_ids_for_next_step = tok(question, return_tensors='pt')['input_ids'] \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for _ in range(tokens):\n",
    "        # The 'x' variable for the current iteration is the input_ids_for_next_step\n",
    "        current_input_ids = input_ids_for_next_step \n",
    "        current_seq_len = current_input_ids.shape[1]\n",
    "\n",
    "        # --- Truncation ---\n",
    "        if current_seq_len > max_position_embeddings:\n",
    "            current_input_ids = current_input_ids[:, current_seq_len - max_position_embeddings:]\n",
    "            print(f\"Warning: Sequence truncated from {current_seq_len} to {current_input_ids.shape[1]} for max_position_embeddings.\")\n",
    "            current_seq_len = current_input_ids.shape[1] # Update after truncation\n",
    "\n",
    "        # --- Embeddings ---\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # Use current_input_ids for embedding lookup\n",
    "        x_embeddings = model.model.decoder.embed_tokens(current_input_ids.to('cpu')) # x_embeddings is on CPU\n",
    "        model.model.decoder.embed_tokens.to('cpu') # Move layer back\n",
    "        gc.collect()\n",
    "        \n",
    "        # 2. Add Positional Embeddings\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        try:\n",
    "            position_offset = model.model.decoder.embed_positions.offset\n",
    "        except AttributeError:\n",
    "            position_offset = 2 # Common default for OPT\n",
    "\n",
    "        pos_ids = torch.arange(position_offset, position_offset + current_seq_len, device='cpu').unsqueeze(0) \n",
    "        \n",
    "        # The main `x` variable throughout the layer processing will be `current_token_embeddings_plus_pos`\n",
    "        current_token_embeddings_plus_pos = x_embeddings + model.model.decoder.embed_positions(pos_ids) \n",
    "        model.model.decoder.embed_positions.to('cpu') \n",
    "        del x_embeddings \n",
    "        del pos_ids \n",
    "        gc.collect()\n",
    "\n",
    "        # --- Decoder Layers Loop ---\n",
    "        # Rename 'x' to something that reflects its current state (embeddings in, embeddings out)\n",
    "        x_current_layer_output = current_token_embeddings_plus_pos # Start of layer processing\n",
    "        for j, layer in enumerate(model.model.decoder.layers):\n",
    "            try:\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "\n",
    "                with torch.no_grad():\n",
    "                    x_current_layer_output = layer(x_current_layer_output)[0]\n",
    "\n",
    "                layer.to('cpu')\n",
    "                x_current_layer_output = x_current_layer_output.to('cpu') \n",
    "                \n",
    "                gc.collect()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in decoder layer {j}: {e}\")\n",
    "                break\n",
    "        else: \n",
    "            # --- LM Head and Token Generation ---\n",
    "            model.lm_head.to('cpu')\n",
    "            # The input to lm_head is x_current_layer_output (the final embeddings from decoder)\n",
    "            logits = model.lm_head(x_current_layer_output.to('cpu')) \n",
    "            model.lm_head.to('cpu') \n",
    "            \n",
    "            logits = logits.to('cpu') # Ensure logits are on CPU\n",
    "            gc.collect()\n",
    "\n",
    "            # Get the predicted token ID (on CPU)\n",
    "            # Take the last token's prediction from batch 0\n",
    "            predicted_token_id = logits.argmax(-1)[:, -1].item() \n",
    "\n",
    "            decoded_token = tok.decode(predicted_token_id)\n",
    "            print(decoded_token, end=\" \")\n",
    "            \n",
    "            # --- Prepare input_ids_for_next_step for the next iteration ---\n",
    "            # Append the new token ID to the input_ids_for_next_step tensor (on CPU)\n",
    "            new_token_tensor = torch.tensor([[predicted_token_id]], dtype=torch.long, device='cpu')\n",
    "            \n",
    "            # This is the correct concatenation for input_ids\n",
    "            input_ids_for_next_step = torch.cat((input_ids_for_next_step, new_token_tensor), dim=1)\n",
    "            \n",
    "            # Update the 'question' string for printing and future reference\n",
    "            question = question + \" \" + decoded_token\n",
    "            \n",
    "        if 'predicted_token_id' not in locals(): # If inner loop broke, stop outer loop\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(question,tokens=250):\n",
    "    for i in range(tokens):\n",
    "        prompt=tok(question,return_tensors='pt').to('cuda')\n",
    "        input_ids=prompt['input_ids']\n",
    "        del prompt\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cuda')\n",
    "        x = model.model.decoder.embed_tokens(input_ids)\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_tokens.to('cpu')\n",
    "        # del model.model.decoder.embed_tokens\n",
    "        torch.cuda.empty_cache()\n",
    "        model.model.decoder.embed_positions.to('cuda')\n",
    "        x = x + model.model.decoder.embed_positions(torch.arange(x.shape[1]).unsqueeze(0).to('cuda'))\n",
    "        model.model.decoder.embed_positions.to('cpu')\n",
    "        # del model.model.decoder.embed_positions\n",
    "        torch.cuda.empty_cache()\n",
    "        for i in model.model.decoder.layers:\n",
    "            try:\n",
    "                i.to('cuda')\n",
    "                x=x.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    x=i(x)[0]\n",
    "                i.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                x=x.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "                # time.sleep(3)\n",
    "            except:\n",
    "                print(x)\n",
    "                break\n",
    "        model.lm_head.to('cuda')\n",
    "        x=model.lm_head(x.to('cuda'))\n",
    "        model.lm_head.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x1=x.argmax(-1)\n",
    "        print(tok.decode(x1[0]),end=\" \")\n",
    "        question=question+\" \"+tok.decode(x1[0])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35778bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".''. your retarded're .''. you retarded're .dylib mom retard.''. "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 21\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_positions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39membed_positions\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py:76\u001b[0m, in \u001b[0;36mOPTLearnedPositionalEmbedding.forward\u001b[0;34m(self, attention_mask, past_key_values_length, position_ids)\u001b[0m\n\u001b[1;32m     74\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m position_ids[:, past_key_values_length:]\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 47\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(tok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m]),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m         question\u001b[38;5;241m=\u001b[39mquestion\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mtok\u001b[38;5;241m.\u001b[39mdecode(x1[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43me\u001b[49m:\n\u001b[1;32m     48\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mtok(question,return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     49\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mprompt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "model_inference(\"How are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce7bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "cnt=0\n",
    "for i in model.model.decoder.layers:\n",
    "    cnt+=1\n",
    "    print(cnt)\n",
    "    try:\n",
    "        i.to('cuda')\n",
    "        x.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            x=i(x)[0]\n",
    "        i.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        x.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "        # time.sleep(3)\n",
    "    except:\n",
    "        print(cnt)\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33d7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lm_head.to('cuda')\n",
    "x=model.lm_head(x.to('cuda'))\n",
    "model.lm_head.to('cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e12f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=x.argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d35ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' hi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(x1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d2228",
   "metadata": {},
   "source": [
    "# 3 Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7274f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return int(device_memory/param_size_GB)-drop\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda')\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first chunk\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        # Wait for transfer to complete before starting computation\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "        # Move pointers\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        # Offload previous chunk to CPU (if exists)\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If this is the last chunk, break\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            # Offload the final chunk\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Load next chunk\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "\n",
    "        i += l_next\n",
    "        rem -= l_next\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu')\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda')\n",
    "    model.model.norm.to('cuda')\n",
    "    model.lm_head.to('cuda')\n",
    "    # x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda')\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda')\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda')\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu')\n",
    "            # torch.cuda.empty_cache()\n",
    "       \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8292bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", I am a 19 year old girl who\n",
      "Total time 4.665896415710449\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf827a7c",
   "metadata": {},
   "source": [
    "# 4 Workers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc92640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig\n",
    "from torch import nn\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Add at the beginning of your script:\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            # bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "            # bnb_4bit_use_double_quant=True,\n",
    "            # bnb_8bit_quant_type='nf4',\n",
    "        )\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\",\n",
    "                                           torch_dtype=torch.bfloat16,\n",
    "                                           use_cache=True,\n",
    "                                          #  offload_folder='/offload_nvm',\n",
    "                                          #  offload_state_dict=True,\n",
    "                                          #  max_memory={\"cpu\":'0GB'},\n",
    "                                        #    quantization_config=bnb_config\n",
    "                                           )\n",
    "\n",
    "# print(model)\n",
    "tok=AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edfeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: torch.Size([1, 3, 512])\n",
      "v: torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "inputs = tokenizer(\"This is\", return_tensors=\"pt\").to(\"cuda\")\n",
    "kv_store = {}\n",
    "\n",
    "def capture_kv(layer, inp, out):\n",
    "    x = inp[0]\n",
    "    attn = layer.self_attn\n",
    "    kv_store[0] = {\n",
    "        \"k\": attn.k_proj(x).detach().cpu(),\n",
    "        \"v\": attn.v_proj(x).detach().cpu()\n",
    "    }\n",
    "\n",
    "layer = model.model.layers[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "    pos_ids = torch.arange(x.shape[1], device=x.device).unsqueeze(0)\n",
    "    cos, sin = model.model.rotary_emb(x, pos_ids)\n",
    "\n",
    "    capture_kv(layer, (x,), None)\n",
    "    _ = layer(x, position_ids=pos_ids, position_embeddings=(cos, sin), use_cache=False)\n",
    "\n",
    "for k, v in kv_store[0].items():\n",
    "    print(f\"{k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "def get_layers(model,drop=2):\n",
    "    device_memory=(torch.cuda.get_device_properties('cuda').total_memory)/(1024**3)\n",
    "    l=model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory/param_size_GB)-drop)//2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        # print(self.model.device)\n",
    "       \n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin,attention_mask):\n",
    "        for layer in self.model:\n",
    "            # print(layer.device)\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "            # x = layer(x)[0]\n",
    "        return x\n",
    "\n",
    "layers=model.model.layers\n",
    "def slice_past(past_list, start, end):\n",
    "    return past_list[start:end] if past_list else None\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda',non_blocking=True)\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first two chunks\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "        \n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "    while True:\n",
    "    # Wait for previous transfer to complete before compute\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        # Move pointers forward\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    cos.to('cuda', non_blocking=True),\n",
    "                    sin.to('cuda', non_blocking=True),\n",
    "                    attention_mask.to('cuda', non_blocking=True)\n",
    "                )\n",
    "\n",
    "        # Offload the previous chunk after compute is done\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If no more chunks left to load after this, finish\n",
    "        if rem == 0 and m_next is None and m_next2 is None:\n",
    "            # Wait for compute to complete\n",
    "            stream_compute.synchronize()\n",
    "\n",
    "            # Offload the current module\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Start preloading next-next chunk if any\n",
    "        if rem > 0:\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            stream_transfer2.wait_stream(stream_transfer)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "    return x\n",
    "    # while True:\n",
    "    #     # Wait for transfer to complete before starting computation\n",
    "    #     stream_compute.wait_stream(stream_transfer)\n",
    "        \n",
    "    #     # Move pointers\n",
    "    #     m_previous = m_current\n",
    "    #     m_current = m_next\n",
    "    #     m_next = m_next2\n",
    "    #     m_next2 = None\n",
    "\n",
    "    #     # Compute on current chunk\n",
    "    #     with torch.cuda.stream(stream_compute):\n",
    "    #         with torch.no_grad():\n",
    "    #             x = m_current(x.to('cuda', non_blocking=True),\n",
    "    #                           cos.to('cuda', non_blocking=True),\n",
    "    #                           sin.to('cuda', non_blocking=True),\n",
    "    #                           attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "    #     # Offload previous chunk to CPU (if exists)\n",
    "    #     if m_previous is not None:\n",
    "    #         stream_offload.wait_stream(stream_compute)  # Wait for computation to finish before offloading\n",
    "    #         with torch.cuda.stream(stream_offload):\n",
    "    #             m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "    #             del m_previous\n",
    "    #         torch.cuda.empty_cache()\n",
    "\n",
    "    #     # If this is the last chunk, break\n",
    "    #     if rem == 0:\n",
    "    #         stream_compute.synchronize()\n",
    "    #         # Offload the final chunk\n",
    "    #         with torch.cuda.stream(stream_offload):\n",
    "    #             m_current = m_current.to('cpu', non_blocking=True)\n",
    "    #         stream_offload.synchronize()\n",
    "    #         torch.cuda.empty_cache()\n",
    "    #         break\n",
    "\n",
    "    #     # Load next chunk (2 steps ahead)\n",
    "    #     # if rem > 0:\n",
    "    #     #     time.sleep(0.01) \n",
    "    #     #     l_next2 = min(layers_per_chunk, rem)\n",
    "    #     #     with torch.cuda.stream(stream_transfer2):\n",
    "    #     #         m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "    #     #     time.sleep(0.1)\n",
    "    #     #     i += l_next2\n",
    "    #     #     rem -= l_next2\n",
    "    #     if rem > 0:\n",
    "    #         l_next2 = min(layers_per_chunk, rem)\n",
    "\n",
    "    #         # Create CUDA events for synchronization\n",
    "    #         transfer_start = torch.cuda.Event(blocking=False)\n",
    "    #         transfer_end = torch.cuda.Event(blocking=True)  # blocking=True ensures CPU waits for it\n",
    "\n",
    "    #         # Record the start of the transfer on the stream\n",
    "    #         stream_transfer2.record_event(transfer_start)\n",
    "\n",
    "    #         with torch.cuda.stream(stream_transfer2):\n",
    "    #             m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "    #             stream_transfer2.record_event(transfer_end)  # Record when transfer is done\n",
    "\n",
    "    #         # Wait until transfer is completed before proceeding\n",
    "    #         transfer_end.synchronize()  # 100% guaranteed sync\n",
    "\n",
    "    #         # Advance the layer index\n",
    "    #         i += l_next2\n",
    "    #         rem -= l_next2\n",
    "\n",
    "    # return x\n",
    "\n",
    "def model_inference(question,tokens=250):\n",
    "    prompt=tok(question,return_tensors='pt').to('cpu',non_blocking=True)\n",
    "    input_ids=prompt['input_ids'].to('cuda',non_blocking=True)\n",
    "    attention_mask=prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "    model.model.norm.to('cuda',non_blocking=True)\n",
    "    model.lm_head.to('cuda',non_blocking=True)\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "   \n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cuda',non_blocking=True)\n",
    "        # if i!=0:\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "       \n",
    "        cos, sin = rotary_emb(x=x,position_ids=position_ids)\n",
    "        # input_ids.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "        # model.model.embed_tokens.to('cpu')\n",
    "        # torch.cuda.empty_cache()\n",
    "       \n",
    "        cnt=0\n",
    "        with torch.no_grad():\n",
    "            l1=get_layers(model,7)\n",
    "            x=compute(model,x,2,cos,sin,k,tokens,attention_mask)\n",
    "            # model.model.norm.to('cuda',non_blocking=True)\n",
    "            x=model.model.norm(x)\n",
    "            # model.model.norm.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "            # model.lm_head.to('cuda',non_blocking=True)\n",
    "            x=model.lm_head(x)\n",
    "            # model.lm_head.to('cpu',non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        x1=x[:,-1,:]\n",
    "        x1=torch.argmax(x1,dim=-1,keepdim=True)\n",
    "        print(tok.decode(x1[0]),end=\"\")\n",
    "        input_ids=torch.cat([input_ids,x1],dim=-1)\n",
    "        # x1=x.argmax(-1)\n",
    "        # print(tok.decode(x1[0]),end=\"\\n\")\n",
    "        # question=question+\" \"+tok.decode(x1[0])\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu',non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fedf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "W0807 16:48:55.794943 7950 site-packages/torch/_inductor/utils.py:1250] [4/0_1] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI how are you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "Cell \u001b[0;32mIn[2], line 213\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    212\u001b[0m     l1\u001b[38;5;241m=\u001b[39mget_layers(model,\u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m--> 213\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# model.model.norm.to('cuda',non_blocking=True)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     x\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "Cell \u001b[0;32mIn[2], line 79\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mstream(stream_compute):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 79\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mm_current\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[43msin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Offload the previous chunk after compute is done\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m m_previous \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, cos, sin, attention_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cos, sin,attention_mask):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;66;03m# print(layer.device)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;66;03m# x = layer(x, position_embeddings=(cos, sin))[0]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:290\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:235\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    233\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 235\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[1;32m    236\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m apply_rotary_pos_emb(query_states, key_states, cos, sin)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI how are you\",50)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layers(model, drop=2):\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024 ** 3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory / param_size_GB) - drop) // 2\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "\n",
    "    @torch.compile\n",
    "    def forward(self, x, cos, sin, attention_mask):\n",
    "        for layer in self.model:\n",
    "            x = layer(x, position_embeddings=(cos, sin))[0]\n",
    "        return x\n",
    "\n",
    "layers = model.model.layers\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        time.sleep(5.01)  # delay 1\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'))\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        time.sleep(5.01)  # delay 2\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            time.sleep(5.01)  # delay 3\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        time.sleep(5.01)  # delay 4\n",
    "\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        time.sleep(5.01)  # delay 5\n",
    "\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            time.sleep(5.01)  # delay 6\n",
    "            with torch.no_grad():\n",
    "                x = m_current(x.to('cuda', non_blocking=True),\n",
    "                              cos.to('cuda', non_blocking=True),\n",
    "                              sin.to('cuda', non_blocking=True),\n",
    "                              attention_mask.to('cuda', non_blocking=True))\n",
    "\n",
    "        time.sleep(5.01)  # delay 7\n",
    "\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            time.sleep(5.01)  # delay 8\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            time.sleep(5.01)  # delay 9\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            time.sleep(8)  # delay 10\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "            stream_offload.synchronize()\n",
    "            time.sleep(8)  # delay 11\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        if rem > 0:\n",
    "            print(\"runing test\")\n",
    "            time.sleep(15)  # delay 12\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            time.sleep(15)  # delay 13\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "\n",
    "    return x\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    prompt = tok(question, return_tensors='pt').to('cpu', non_blocking=True)\n",
    "    input_ids = prompt['input_ids'].to('cuda', non_blocking=True)\n",
    "    attention_mask = prompt['attention_mask']\n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    time.sleep(5.01)  # delay 14\n",
    "\n",
    "    x = model.model.embed_tokens(input_ids)\n",
    "    rotary_emb = model.model.rotary_emb\n",
    "\n",
    "    del prompt\n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        time.sleep(5.01)  # delay 15\n",
    "\n",
    "        x = model.model.embed_tokens(input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        cos, sin = rotary_emb(x=x, position_ids=position_ids)\n",
    "        time.sleep(5.01)  # delay 16\n",
    "\n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            x = compute(model, x, 9, cos, sin, k, tokens, attention_mask)\n",
    "            time.sleep(5.01)  # delay 17\n",
    "\n",
    "            x = model.model.norm(x)\n",
    "            time.sleep(5.01)  # delay 18\n",
    "\n",
    "            x = model.lm_head(x)\n",
    "            time.sleep(5.01)  # delay 19\n",
    "\n",
    "        x1 = x[:, -1, :]\n",
    "        x1 = torch.argmax(x1, dim=-1, keepdim=True)\n",
    "        print(tok.decode(x1[0]), end=\"\")\n",
    "        input_ids = torch.cat([input_ids, x1], dim=-1)\n",
    "        time.sleep(5.01)  # delay 20\n",
    "\n",
    "    input_ids.to('cpu')\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236073b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0805 11:36:46.319881 52923 site-packages/torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/lowering.py:7007: UserWarning: \n",
      "Online softmax is disabled on the fly since Inductor decides to\n",
      "split the reduction. Cut an issue to PyTorch if this is an\n",
      "important use case and you want to speed it up with online\n",
      "softmax.\n",
      "\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n",
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:2177: UserWarning: NVIDIA T1000 8GB does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI nonetheless"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m start\u001b[38;5;241m=\u001b[39mtime\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time\u001b[39m\u001b[38;5;124m\"\u001b[39m,time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart)\n",
      "Cell \u001b[0;32mIn[3], line 130\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, tokens)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(tokens):\n\u001b[1;32m    129\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5.01\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# delay 15\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     x \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens(input_ids)\n\u001b[1;32m    133\u001b[0m     seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "model_inference(\"HI\",10)\n",
    "print()\n",
    "print(\"Total time\",time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7659fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Input: Hello! How are you today?\n",
      "Output: "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaAttention' object has no attribute 'num_key_value_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 418\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    417\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 418\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 300\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, max_new_tokens)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Get attention config from the model\u001b[39;00m\n\u001b[1;32m    297\u001b[0m sample_attn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mself_attn\n\u001b[1;32m    298\u001b[0m kv_cache \u001b[38;5;241m=\u001b[39m KVCache(\n\u001b[1;32m    299\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[0;32m--> 300\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39m\u001b[43msample_attn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_key_value_heads\u001b[49m,  \u001b[38;5;66;03m# Use KV heads for cache\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     head_dim\u001b[38;5;241m=\u001b[39msample_attn\u001b[38;5;241m.\u001b[39mhead_dim,\n\u001b[1;32m    302\u001b[0m     max_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# Move necessary components to GPU\u001b[39;00m\n\u001b[1;32m    306\u001b[0m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LlamaAttention' object has no attribute 'num_key_value_heads'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    return (int(device_memory / param_size_GB) - drop) // 2\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self, num_layers):\n",
    "        self.num_layers = num_layers\n",
    "        # Store KV cache on CPU to save GPU memory\n",
    "        self.cache = {}\n",
    "        \n",
    "    def get_cache(self, layer_idx):\n",
    "        \"\"\"Get cache for a specific layer\"\"\"\n",
    "        return self.cache.get(layer_idx, None)\n",
    "    \n",
    "    def update_cache(self, layer_idx, k, v):\n",
    "        \"\"\"Update cache for a specific layer\"\"\"\n",
    "        if layer_idx not in self.cache:\n",
    "            self.cache[layer_idx] = {\"k\": k.detach().cpu(), \"v\": v.detach().cpu()}\n",
    "        else:\n",
    "            # Concatenate new k,v with existing cache along sequence dimension\n",
    "            existing_k = self.cache[layer_idx][\"k\"]\n",
    "            existing_v = self.cache[layer_idx][\"v\"]\n",
    "            self.cache[layer_idx][\"k\"] = torch.cat([existing_k, k.detach().cpu()], dim=2)  # Fixed: dim=2 for seq_len\n",
    "            self.cache[layer_idx][\"v\"] = torch.cat([existing_v, v.detach().cpu()], dim=2)  # Fixed: dim=2 for seq_len\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    \"\"\"Apply rotary positional embeddings to query and key tensors\"\"\"\n",
    "    batch_size = q.shape[0]\n",
    "    q_seq_len = q.shape[2]\n",
    "    k_seq_len = k.shape[2]\n",
    "    \n",
    "    # Get cos/sin for query positions\n",
    "    if position_ids.shape[1] == q_seq_len:\n",
    "        # Use provided position_ids directly\n",
    "        cos_q = cos[:, position_ids.squeeze(0), :].unsqueeze(1)  # [batch_size, 1, seq_len, head_dim]\n",
    "        sin_q = sin[:, position_ids.squeeze(0), :].unsqueeze(1)\n",
    "    else:\n",
    "        # Single token case\n",
    "        pos_idx = position_ids.squeeze()\n",
    "        cos_q = cos[:, pos_idx:pos_idx+1, :].unsqueeze(1)\n",
    "        sin_q = sin[:, pos_idx:pos_idx+1, :].unsqueeze(1)\n",
    "    \n",
    "    # Apply to query\n",
    "    q_embed = (q * cos_q) + (rotate_half(q) * sin_q)\n",
    "    \n",
    "    # For keys, handle full sequence\n",
    "    if k_seq_len == q_seq_len:\n",
    "        # Same sequence length, use same cos/sin\n",
    "        cos_k = cos_q\n",
    "        sin_k = sin_q\n",
    "    else:\n",
    "        # Different sequence length (cached keys), create full position sequence\n",
    "        start_pos = k_seq_len - q_seq_len\n",
    "        full_positions = torch.arange(start_pos, k_seq_len, device=k.device, dtype=torch.long)\n",
    "        cos_k = cos[:, full_positions, :].unsqueeze(1)\n",
    "        sin_k = sin[:, full_positions, :].unsqueeze(1)\n",
    "    \n",
    "    k_embed = (k * cos_k) + (rotate_half(k) * sin_k)\n",
    "    \n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def capture_and_apply_kv(layer, x, layer_idx, kv_cache, position_embeddings, attention_mask, position_ids, is_first_token=False):\n",
    "    \"\"\"Capture K,V values and apply KV caching for a layer\"\"\"\n",
    "    cos, sin = position_embeddings\n",
    "    \n",
    "    # PRE-NORM: Apply input layer norm first\n",
    "    hidden_states = layer.input_layernorm(x)\n",
    "    \n",
    "    # Get attention module\n",
    "    attn = layer.self_attn\n",
    "    \n",
    "    # Model dimensions for Llama-3.2-1B\n",
    "    num_heads = 32\n",
    "    head_dim = 64\n",
    "    num_key_value_heads = 8\n",
    "    \n",
    "    # Compute Q, K, V projections\n",
    "    q = attn.q_proj(hidden_states)\n",
    "    k_new = attn.k_proj(hidden_states)\n",
    "    v_new = attn.v_proj(hidden_states)\n",
    "    \n",
    "    bsz, seq_len, _ = q.shape\n",
    "    \n",
    "    # Reshape for multi-head attention\n",
    "    q = q.view(bsz, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "    k_new = k_new.view(bsz, seq_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    v_new = v_new.view(bsz, seq_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    \n",
    "    # Handle KV caching - fixed to avoid position embedding issues\n",
    "    if is_first_token:\n",
    "        # First token - no existing cache, apply RoPE to all tokens\n",
    "        k_full = k_new\n",
    "        v_full = v_new\n",
    "        kv_cache.update_cache(layer_idx, k_new, v_new)\n",
    "        \n",
    "        # Apply rotary embeddings to q and k\n",
    "        if cos is not None and sin is not None:\n",
    "            q, k_full = apply_rotary_pos_emb(q, k_full, cos, sin, position_ids)\n",
    "    else:\n",
    "        # Get existing cache and concatenate\n",
    "        cached = kv_cache.get_cache(layer_idx)\n",
    "        if cached is not None:\n",
    "            k_cached = cached[\"k\"].to(k_new.device)\n",
    "            v_cached = cached[\"v\"].to(v_new.device)\n",
    "            \n",
    "            # Apply RoPE to new k tokens only\n",
    "            if cos is not None and sin is not None:\n",
    "                q, k_new = apply_rotary_pos_emb(q, k_new, cos, sin, position_ids)\n",
    "            \n",
    "            # Concatenate cached (already has RoPE) with new (just applied RoPE)\n",
    "            k_full = torch.cat([k_cached, k_new], dim=2)\n",
    "            v_full = torch.cat([v_cached, v_new], dim=2)\n",
    "            \n",
    "            # Update cache with the new tokens\n",
    "            kv_cache.update_cache(layer_idx, k_new, v_new)\n",
    "        else:\n",
    "            # No cache, treat as first token\n",
    "            k_full = k_new\n",
    "            v_full = v_new\n",
    "            kv_cache.update_cache(layer_idx, k_new, v_new)\n",
    "            if cos is not None and sin is not None:\n",
    "                q, k_full = apply_rotary_pos_emb(q, k_full, cos, sin, position_ids)\n",
    "    \n",
    "    # Expand k,v for grouped query attention (GQA: 8 KV heads -> 32 Q heads)\n",
    "    if num_key_value_heads != num_heads:\n",
    "        k_full = k_full.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "        v_full = v_full.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    attn_weights = torch.matmul(q, k_full.transpose(2, 3)) / torch.sqrt(torch.tensor(head_dim, dtype=q.dtype, device=q.device))\n",
    "    full_seq_len = k_full.shape[2]\n",
    "    # Apply causal mask - only mask future tokens\n",
    "    if full_seq_len > seq_len:\n",
    "        # We have cached tokens, create appropriate mask\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, full_seq_len, device=q.device, dtype=torch.bool))\n",
    "    else:\n",
    "        causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=q.device, dtype=torch.bool))\n",
    "    \n",
    "    # Apply the mask\n",
    "    attn_weights = attn_weights.masked_fill(~causal_mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "    \n",
    "    # Apply softmax\n",
    "    attn_weights = torch.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    attn_output = torch.matmul(attn_weights, v_full)\n",
    "    \n",
    "    # Reshape back to [batch_size, seq_len, hidden_size]\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, seq_len, -1)\n",
    "    \n",
    "    # Apply output projection\n",
    "    attn_output = attn.o_proj(attn_output)\n",
    "    \n",
    "    # Add residual connection\n",
    "    hidden_states = x + attn_output\n",
    "    \n",
    "    # POST-NORM: Apply post attention layer norm\n",
    "    residual = hidden_states\n",
    "    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "    \n",
    "    # Apply MLP\n",
    "    mlp_output = layer.mlp(hidden_states)\n",
    "    \n",
    "    # Add residual connection\n",
    "    hidden_states = residual + mlp_output\n",
    "    \n",
    "    return hidden_states\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        self.start_idx = i\n",
    "    \n",
    "    def forward(self, x, cos, sin, attention_mask, kv_cache, position_ids, is_first_token=False):\n",
    "        for idx, layer in enumerate(self.model):\n",
    "            layer_idx = self.start_idx + idx\n",
    "            x = capture_and_apply_kv(layer, x, layer_idx, kv_cache, (cos, sin), attention_mask, position_ids, is_first_token)\n",
    "        return x\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, cos, sin, k, max_new_tokens, attention_mask, kv_cache, position_ids, is_first_token=False):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_transfer2 = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    if layers_per_chunk >= total_layers:\n",
    "        i = 0\n",
    "        l = total_layers\n",
    "        m = Module(model_instance.model.layers, i, i + l).to('cuda', non_blocking=True)\n",
    "        x = m(x.to('cuda'), cos.to('cuda'), sin.to('cuda'), attention_mask.to('cuda'), kv_cache, position_ids.to('cuda'), is_first_token)\n",
    "        if k == max_new_tokens - 1:\n",
    "            m = m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_next2 = None\n",
    "    m_previous = None\n",
    "\n",
    "    # Load first two chunks\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "        \n",
    "        if rem > 0:\n",
    "            l_second = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_second).to('cuda', non_blocking=True)\n",
    "            i += l_second\n",
    "            rem -= l_second\n",
    "\n",
    "    while True:\n",
    "        # Wait for previous transfer to complete before compute\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "\n",
    "        # Move pointers forward\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = m_next2\n",
    "        m_next2 = None\n",
    "\n",
    "        # Compute on current chunk\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    cos.to('cuda', non_blocking=True),\n",
    "                    sin.to('cuda', non_blocking=True),\n",
    "                    attention_mask.to('cuda', non_blocking=True),\n",
    "                    kv_cache,\n",
    "                    position_ids.to('cuda', non_blocking=True),\n",
    "                    is_first_token\n",
    "                )\n",
    "\n",
    "        # Offload the previous chunk after compute is done\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous = m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # If no more chunks left to load after this, finish\n",
    "        if rem == 0 and m_next is None and m_next2 is None:\n",
    "            # Wait for compute to complete\n",
    "            stream_compute.synchronize()\n",
    "\n",
    "            # Offload the current module\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current = m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        # Start preloading next-next chunk if any\n",
    "        if rem > 0:\n",
    "            l_next2 = min(layers_per_chunk, rem)\n",
    "            stream_transfer2.wait_stream(stream_transfer)\n",
    "            with torch.cuda.stream(stream_transfer2):\n",
    "                m_next2 = Module(model_instance.model.layers, i, i + l_next2).to('cuda', non_blocking=True)\n",
    "            i += l_next2\n",
    "            rem -= l_next2\n",
    "    return x\n",
    "\n",
    "def model_inference(question, tokens=250):\n",
    "    # Add proper prompt formatting for Llama\n",
    "    if not question.strip():\n",
    "        question = \"Hello! How are you today?\"\n",
    "    \n",
    "    prompt = tok(question, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = prompt['input_ids'].to('cuda', non_blocking=True)\n",
    "    attention_mask = prompt['attention_mask'].to('cuda', non_blocking=True)\n",
    "    \n",
    "    print(f\"Input: {question}\")\n",
    "    print(\"Output: \", end=\"\")\n",
    "    \n",
    "    # Initialize KV cache\n",
    "    num_layers = len(model.model.layers)\n",
    "    kv_cache = KVCache(num_layers)\n",
    "    \n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    \n",
    "    rotary_emb = model.model.rotary_emb\n",
    "    \n",
    "    for k in range(tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Determine if this is the first token generation\n",
    "        is_first_token = (k == 0)\n",
    "        \n",
    "        # For subsequent tokens, only process the last token\n",
    "        if not is_first_token:\n",
    "            current_input_ids = input_ids[:, -1:].contiguous()\n",
    "        else:\n",
    "            current_input_ids = input_ids\n",
    "            \n",
    "        x = model.model.embed_tokens(current_input_ids)\n",
    "        seq_len = x.shape[1]\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Position IDs for current token(s)\n",
    "        if is_first_token:\n",
    "            position_ids = torch.arange(seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        else:\n",
    "            # For subsequent tokens, position ID should be the current sequence length\n",
    "            current_pos = input_ids.shape[1] - 1\n",
    "            position_ids = torch.tensor([[current_pos]], dtype=torch.long, device=x.device)\n",
    "        \n",
    "        # Get rotary embeddings - simplified approach\n",
    "        max_seq_len = 4096  # Use a reasonable max length\n",
    "        dummy_x = torch.zeros(batch_size, max_seq_len, x.shape[-1], device=x.device, dtype=x.dtype)\n",
    "        dummy_position_ids = torch.arange(max_seq_len, dtype=torch.long, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        cos, sin = rotary_emb(x=dummy_x, position_ids=dummy_position_ids)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            l1 = get_layers(model, 7)\n",
    "            x = compute(model, x, 3, cos, sin, k, tokens, attention_mask, kv_cache, position_ids, is_first_token)\n",
    "            x = model.model.norm(x)\n",
    "            torch.cuda.empty_cache()\n",
    "            x = model.lm_head(x)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Get the logits for the last token\n",
    "        logits = x[:, -1, :]\n",
    "        \n",
    "        # Use temperature and top-p sampling for better generation\n",
    "        temperature = 0.7\n",
    "        top_p = 0.9\n",
    "        \n",
    "        # Apply temperature\n",
    "        logits = logits / temperature\n",
    "        \n",
    "        # Apply top-p filtering\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "        # Sample from the filtered distribution\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Check for end of sequence\n",
    "        if next_token.item() == tok.eos_token_id:\n",
    "            break\n",
    "            \n",
    "        print(tok.decode(next_token[0], skip_special_tokens=True), end=\"\", flush=True)\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    \n",
    "    print()  # New line at the end\n",
    "    \n",
    "    # Cleanup\n",
    "    input_ids.to('cpu')\n",
    "    model.model.embed_tokens.to('cpu', non_blocking=True)\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    torch.cuda.empty_cache()\n",
    "    kv_cache.clear()\n",
    "\n",
    "# Load model and tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\", torch_dtype=torch.bfloat16, use_cache=True)\n",
    "layers = model.model.layers\n",
    "\n",
    "# Test the model\n",
    "while True:\n",
    "    question = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
    "    if question.lower() == 'quit':\n",
    "        break\n",
    "    start = time.time()\n",
    "    model_inference(question, 50)  # Reduced tokens for testing\n",
    "    print(f\"Time taken: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6961788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded successfully!\n",
      "Input: hi\n",
      "Output: "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LlamaRotaryEmbedding.forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 361\u001b[0m\n\u001b[1;32m    359\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 361\u001b[0m \u001b[43mmodel_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime taken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 292\u001b[0m, in \u001b[0;36mmodel_inference\u001b[0;34m(question, max_new_tokens)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    291\u001b[0m     layers_per_chunk \u001b[38;5;241m=\u001b[39m get_layers(model, \u001b[38;5;241m7\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_per_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    298\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[29], line 175\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(model_instance, x, layers_per_chunk, rotary_emb, position_ids, kv_cache, past_seq_len, k, max_new_tokens)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layers_per_chunk \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m total_layers:\n\u001b[1;32m    174\u001b[0m     m \u001b[38;5;241m=\u001b[39m Module(model_instance\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers, \u001b[38;5;241m0\u001b[39m, total_layers)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 175\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m==\u001b[39m max_new_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    177\u001b[0m         m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[29], line 168\u001b[0m, in \u001b[0;36mModule.forward\u001b[0;34m(self, x, rotary_emb, position_ids, kv_cache, past_seq_len)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    167\u001b[0m     layer_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_idx \u001b[38;5;241m+\u001b[39m idx\n\u001b[0;32m--> 168\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mllama_layer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[0;32mIn[29], line 151\u001b[0m, in \u001b[0;36mllama_layer_forward\u001b[0;34m(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\u001b[0m\n\u001b[1;32m    149\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    150\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[0;32m--> 151\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mllama_attention_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m attn_output\n\u001b[1;32m    153\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "Cell \u001b[0;32mIn[29], line 107\u001b[0m, in \u001b[0;36mllama_attention_forward\u001b[0;34m(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\u001b[0m\n\u001b[1;32m    104\u001b[0m value_states \u001b[38;5;241m=\u001b[39m value_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, num_key_value_heads, head_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Apply rotary position embeddings using the passed rotary_emb module\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# Handle KV caching\u001b[39;00m\n\u001b[1;32m    110\u001b[0m cached_k, cached_v, cache_seq_len \u001b[38;5;241m=\u001b[39m kv_cache\u001b[38;5;241m.\u001b[39mget_cached_kv(layer_idx, key_states\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaRotaryEmbedding.forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Use bfloat16 for better performance on modern GPUs\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def get_layers(model, drop=2):\n",
    "    \"\"\"\n",
    "    Estimate the number of layers that can fit on the GPU.\n",
    "    This is a heuristic and may need adjustment.\n",
    "    \"\"\"\n",
    "    device_memory = (torch.cuda.get_device_properties('cuda').total_memory) / (1024**3)\n",
    "    l = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in l.parameters())\n",
    "    param_size_bytes = num_params * 2  # Assuming bfloat16 (2 bytes per parameter)\n",
    "    param_size_GB = param_size_bytes / (1024 ** 3)\n",
    "    \n",
    "    # Ensure at least one layer can fit and we don't return zero\n",
    "    return max(1, (int(device_memory / param_size_GB) - drop) // 2)\n",
    "\n",
    "class KVCache:\n",
    "    \"\"\"A simple KV cache implementation on the CPU to save GPU memory.\"\"\"\n",
    "    def __init__(self, num_layers, num_key_value_heads, head_dim, max_seq_len=4096):\n",
    "        self.num_layers = num_layers\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.cache = {}\n",
    "    \n",
    "    def get_cache(self, layer_idx):\n",
    "        \"\"\"Get cache for a specific layer.\"\"\"\n",
    "        return self.cache.get(layer_idx, None)\n",
    "    \n",
    "    def update_cache(self, layer_idx, k, v, seq_pos):\n",
    "        \"\"\"Update cache for a specific layer at specific sequence position.\"\"\"\n",
    "        if layer_idx not in self.cache:\n",
    "            batch_size = k.shape[0]\n",
    "            self.cache[layer_idx] = {\n",
    "                \"k\": torch.zeros(batch_size, self.num_key_value_heads, self.max_seq_len, self.head_dim, \n",
    "                                 dtype=k.dtype, device='cpu'),\n",
    "                \"v\": torch.zeros(batch_size, self.num_key_value_heads, self.max_seq_len, self.head_dim, \n",
    "                                 dtype=v.dtype, device='cpu'),\n",
    "                \"seq_len\": 0\n",
    "            }\n",
    "        \n",
    "        cache_entry = self.cache[layer_idx]\n",
    "        current_seq_len = k.shape[2]\n",
    "        \n",
    "        cache_entry[\"k\"][:, :, seq_pos : seq_pos + current_seq_len, :] = k.detach().cpu()\n",
    "        cache_entry[\"v\"][:, :, seq_pos : seq_pos + current_seq_len, :] = v.detach().cpu()\n",
    "        cache_entry[\"seq_len\"] = seq_pos + current_seq_len\n",
    "    \n",
    "    def get_cached_kv(self, layer_idx, device):\n",
    "        \"\"\"Get cached k,v up to current sequence length.\"\"\"\n",
    "        if layer_idx not in self.cache:\n",
    "            return None, None, 0\n",
    "            \n",
    "        cache_entry = self.cache[layer_idx]\n",
    "        seq_len = cache_entry[\"seq_len\"]\n",
    "        \n",
    "        if seq_len == 0:\n",
    "            return None, None, 0\n",
    "            \n",
    "        k_cached = cache_entry[\"k\"][:, :, :seq_len, :].to(device)\n",
    "        v_cached = cache_entry[\"v\"][:, :, :seq_len, :].to(device)\n",
    "        \n",
    "        return k_cached, v_cached, seq_len\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache.\"\"\"\n",
    "        self.cache.clear()\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Removed apply_rotary_pos_emb as it will be handled directly by the rotary_emb module\n",
    "\n",
    "def llama_attention_forward(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len=0):\n",
    "    \"\"\"Custom forward pass for Llama attention with KV caching\"\"\"\n",
    "    attn = layer.self_attn\n",
    "    \n",
    "    bsz, q_len, _ = hidden_states.shape\n",
    "    \n",
    "    # Hardcoded values based on Llama-3.2-1B model\n",
    "    num_heads = 32\n",
    "    num_key_value_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Project to Q, K, V\n",
    "    query_states = attn.q_proj(hidden_states)\n",
    "    key_states = attn.k_proj(hidden_states)\n",
    "    value_states = attn.v_proj(hidden_states)\n",
    "    \n",
    "    # Reshape for multi-head attention\n",
    "    query_states = query_states.view(bsz, q_len, num_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, num_key_value_heads, head_dim).transpose(1, 2)\n",
    "    \n",
    "    # Apply rotary position embeddings using the passed rotary_emb module\n",
    "    query_states, key_states = rotary_emb(query_states, key_states, position_ids)\n",
    "    \n",
    "    # Handle KV caching\n",
    "    cached_k, cached_v, cache_seq_len = kv_cache.get_cached_kv(layer_idx, key_states.device)\n",
    "    \n",
    "    if cached_k is not None:\n",
    "        key_states = torch.cat([cached_k, key_states], dim=2)\n",
    "        value_states = torch.cat([cached_v, value_states], dim=2)\n",
    "    \n",
    "    kv_cache.update_cache(layer_idx, key_states[:, :, past_seq_len:, :], value_states[:, :, past_seq_len:, :], past_seq_len)\n",
    "    \n",
    "    # Repeat k/v heads if num_key_value_heads < num_heads (grouped query attention)\n",
    "    if num_key_value_heads != num_heads:\n",
    "        key_states = key_states.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "        value_states = value_states.repeat_interleave(num_heads // num_key_value_heads, dim=1)\n",
    "    \n",
    "    # Compute attention\n",
    "    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(head_dim)\n",
    "    \n",
    "    # Apply causal mask\n",
    "    seq_len_q = query_states.shape[2]\n",
    "    seq_len_k = key_states.shape[2]\n",
    "    \n",
    "    causal_mask = torch.full((seq_len_q, seq_len_k), float('-inf'), device=hidden_states.device, dtype=attn_weights.dtype)\n",
    "    causal_mask = torch.triu(causal_mask, diagonal=past_seq_len + 1)\n",
    "        \n",
    "    attn_weights = attn_weights + causal_mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    \n",
    "    # Reshape and apply output projection\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)\n",
    "    attn_output = attn.o_proj(attn_output)\n",
    "    \n",
    "    return attn_output\n",
    "\n",
    "def llama_layer_forward(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len=0):\n",
    "    \"\"\"Custom forward pass for a Llama layer\"\"\"\n",
    "    residual = hidden_states\n",
    "    hidden_states = layer.input_layernorm(hidden_states)\n",
    "    attn_output = llama_attention_forward(layer, hidden_states, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\n",
    "    hidden_states = residual + attn_output\n",
    "    residual = hidden_states\n",
    "    hidden_states = layer.post_attention_layernorm(hidden_states)\n",
    "    mlp_output = layer.mlp(hidden_states)\n",
    "    hidden_states = residual + mlp_output\n",
    "    return hidden_states\n",
    "\n",
    "class Module(nn.Module):\n",
    "    def __init__(self, layers, i, j):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList(layers[i:j])\n",
    "        self.start_idx = i\n",
    "    \n",
    "    def forward(self, x, rotary_emb, position_ids, kv_cache, past_seq_len=0):\n",
    "        for idx, layer in enumerate(self.model):\n",
    "            layer_idx = self.start_idx + idx\n",
    "            x = llama_layer_forward(layer, x, rotary_emb, position_ids, kv_cache, layer_idx, past_seq_len)\n",
    "        return x\n",
    "\n",
    "def compute(model_instance, x, layers_per_chunk, rotary_emb, position_ids, kv_cache, past_seq_len, k, max_new_tokens):\n",
    "    total_layers = len(model_instance.model.layers)\n",
    "    if layers_per_chunk >= total_layers:\n",
    "        m = Module(model_instance.model.layers, 0, total_layers).to('cuda', non_blocking=True)\n",
    "        x = m(x.to('cuda'), rotary_emb.to('cuda'), position_ids.to('cuda'), kv_cache, past_seq_len)\n",
    "        if k == max_new_tokens - 1:\n",
    "            m.to('cpu', non_blocking=True)\n",
    "            torch.cuda.empty_cache()\n",
    "        return x\n",
    "\n",
    "    stream_compute = torch.cuda.Stream()\n",
    "    stream_transfer = torch.cuda.Stream()\n",
    "    stream_offload = torch.cuda.Stream()\n",
    "    \n",
    "    i = 0\n",
    "    rem = total_layers\n",
    "    m_current = None\n",
    "    m_next = None\n",
    "    m_previous = None\n",
    "\n",
    "    if rem > 0:\n",
    "        l_first = min(layers_per_chunk, rem)\n",
    "        with torch.cuda.stream(stream_transfer):\n",
    "            m_next = Module(model_instance.model.layers, i, i + l_first).to('cuda', non_blocking=True)\n",
    "        i += l_first\n",
    "        rem -= l_first\n",
    "\n",
    "    while True:\n",
    "        stream_compute.wait_stream(stream_transfer)\n",
    "        m_previous = m_current\n",
    "        m_current = m_next\n",
    "        m_next = None\n",
    "\n",
    "        with torch.cuda.stream(stream_compute):\n",
    "            with torch.no_grad():\n",
    "                x = m_current(\n",
    "                    x.to('cuda', non_blocking=True),\n",
    "                    rotary_emb.to('cuda', non_blocking=True),\n",
    "                    position_ids.to('cuda', non_blocking=True),\n",
    "                    kv_cache,\n",
    "                    past_seq_len\n",
    "                )\n",
    "\n",
    "        if m_previous is not None:\n",
    "            stream_offload.wait_stream(stream_compute)\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_previous.to('cpu', non_blocking=True)\n",
    "                del m_previous\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        if rem == 0:\n",
    "            stream_compute.synchronize()\n",
    "            with torch.cuda.stream(stream_offload):\n",
    "                m_current.to('cpu', non_blocking=True)\n",
    "                del m_current\n",
    "            stream_offload.synchronize()\n",
    "            torch.cuda.empty_cache()\n",
    "            break\n",
    "\n",
    "        if rem > 0:\n",
    "            l_next = min(layers_per_chunk, rem)\n",
    "            with torch.cuda.stream(stream_transfer):\n",
    "                m_next = Module(model_instance.model.layers, i, i + l_next).to('cuda', non_blocking=True)\n",
    "            i += l_next\n",
    "            rem -= l_next\n",
    "    return x\n",
    "\n",
    "def model_inference(question, max_new_tokens=50):\n",
    "    if not question.strip():\n",
    "        question = \"hi\"\n",
    "    \n",
    "    inputs = tok(question, return_tensors='pt', add_special_tokens=True)\n",
    "    input_ids = inputs['input_ids'].to('cuda')\n",
    "    \n",
    "    print(f\"Input: {question}\")\n",
    "    print(\"Output: \", end=\"\", flush=True)\n",
    "    \n",
    "    num_layers = len(model.model.layers)\n",
    "    \n",
    "    # Hardcoded values for Llama-3.2-1B\n",
    "    num_key_value_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Create the KVCache with the correct hardcoded parameters\n",
    "    kv_cache = KVCache(\n",
    "        num_layers=num_layers,\n",
    "        num_key_value_heads=num_key_value_heads,\n",
    "        head_dim=head_dim,\n",
    "        max_seq_len=4096\n",
    "    )\n",
    "    \n",
    "    # Get the rotary_emb module from the main model.model object\n",
    "    # This is the correct way to access it, as it's a shared component\n",
    "    rotary_emb = model.model.rotary_emb.to('cuda', non_blocking=True)\n",
    "    \n",
    "    model.model.embed_tokens.to('cuda', non_blocking=True)\n",
    "    model.model.norm.to('cuda', non_blocking=True)\n",
    "    model.lm_head.to('cuda', non_blocking=True)\n",
    "    \n",
    "    generated_tokens = []\n",
    "    \n",
    "    for k in range(max_new_tokens):\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        is_first_iteration = (k == 0)\n",
    "        \n",
    "        if is_first_iteration:\n",
    "            current_input_ids = input_ids\n",
    "            past_seq_len = 0\n",
    "        else:\n",
    "            current_input_ids = input_ids[:, -1:].contiguous()\n",
    "            past_seq_len = input_ids.shape[1] - 1\n",
    "        \n",
    "        hidden_states = model.model.embed_tokens(current_input_ids)\n",
    "        \n",
    "        seq_len = hidden_states.shape[1]\n",
    "        position_ids = torch.arange(past_seq_len, past_seq_len + seq_len, dtype=torch.long, device=hidden_states.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(hidden_states.shape[0], -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            layers_per_chunk = get_layers(model, 7)\n",
    "            hidden_states = compute(\n",
    "                model, hidden_states, layers_per_chunk, rotary_emb, position_ids, \n",
    "                kv_cache, past_seq_len, k, max_new_tokens\n",
    "            )\n",
    "            \n",
    "            hidden_states = model.model.norm(hidden_states)\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            logits = model.lm_head(hidden_states)\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        \n",
    "        temperature = 0.7\n",
    "        top_p = 0.9\n",
    "        \n",
    "        if temperature > 0:\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            \n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "            next_token_logits[indices_to_remove] = float('-inf')\n",
    "            \n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        if next_token.item() == tok.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        token_text = tok.decode(next_token[0], skip_special_tokens=True)\n",
    "        print(token_text, end=\"\", flush=True)\n",
    "        generated_tokens.append(next_token.item())\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    model.model.embed_tokens.to('cpu', non_blocking=True)\n",
    "    model.model.norm.to('cpu', non_blocking=True)\n",
    "    model.lm_head.to('cpu', non_blocking=True)\n",
    "    rotary_emb.to('cpu', non_blocking=True) # Move rotary_emb back to CPU\n",
    "    torch.cuda.empty_cache()\n",
    "    kv_cache.clear()\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(\"Loading model...\")\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=None,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Test the model\n",
    "question = \"hi\"\n",
    "start = time.time()\n",
    "model_inference(question, 50)\n",
    "print(f\"Time taken: {time.time() - start:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daea1348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: hi\n",
      "Output: "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 132\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    131\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 132\u001b[0m \u001b[43mcompute_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m, in \u001b[0;36mcompute_stream\u001b[0;34m(model, tokenizer, input_ids, max_new_tokens)\u001b[0m\n\u001b[1;32m     92\u001b[0m end \u001b[38;5;241m=\u001b[39m start \u001b[38;5;241m+\u001b[39m chunks\n\u001b[1;32m     93\u001b[0m chunk \u001b[38;5;241m=\u001b[39m LayerChunk(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers, start, \u001b[38;5;28mmin\u001b[39m(end, num_layers))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# offload\u001b[39;00m\n\u001b[1;32m     96\u001b[0m chunk\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gen_ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m, in \u001b[0;36mLayerChunk.forward\u001b[0;34m(self, x, attn_mask, cos, sin, kv_cache, first_token)\u001b[0m\n\u001b[1;32m     46\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m i\n\u001b[1;32m     47\u001b[0m past \u001b[38;5;241m=\u001b[39m kv_cache\u001b[38;5;241m.\u001b[39mget(idx)\n\u001b[0;32m---> 48\u001b[0m out, present \u001b[38;5;241m=\u001b[39m layer(\n\u001b[1;32m     49\u001b[0m     x,\n\u001b[1;32m     50\u001b[0m     position_embeddings\u001b[38;5;241m=\u001b[39m(cos, sin),\n\u001b[1;32m     51\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast,\n\u001b[1;32m     52\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m x \u001b[38;5;241m=\u001b[39m out\n\u001b[1;32m     55\u001b[0m kv_cache\u001b[38;5;241m.\u001b[39mupdate(idx, present)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch import nn\n",
    "\n",
    "# Enable TF32 on supported cards for faster matmuls\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def get_layers_per_chunk(model, drop_gb=2):\n",
    "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    # Estimate size of one layer in GB\n",
    "    example_layer = model.model.layers[0]\n",
    "    num_params = sum(p.numel() for p in example_layer.parameters())\n",
    "    # Map dtype to bytes per element\n",
    "    dtype_size = torch.tensor([], dtype=model.dtype).element_size()\n",
    "    layer_size_gb = num_params * dtype_size / 1024**3\n",
    "    return max(1, int((total_mem_gb - drop_gb) // layer_size_gb))\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self, num_layers):\n",
    "        self.cache = [None] * num_layers\n",
    "\n",
    "    def get(self, idx):\n",
    "        return self.cache[idx]\n",
    "\n",
    "    def update(self, idx, present):\n",
    "        if self.cache[idx] is None:\n",
    "            self.cache[idx] = present\n",
    "        else:\n",
    "            k_old, v_old = self.cache[idx]\n",
    "            k_new, v_new = present\n",
    "            # concat on seq dim (-2)\n",
    "            self.cache[idx] = (torch.cat([k_old, k_new], dim=-2), torch.cat([v_old, v_new], dim=-2))\n",
    "\n",
    "    def clear(self):\n",
    "        self.cache = [None] * len(self.cache)\n",
    "\n",
    "class LayerChunk(nn.Module):\n",
    "    def __init__(self, layers, start, end):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers[start:end])\n",
    "        self.start = start\n",
    "\n",
    "    def forward(self, x, attn_mask, cos, sin, kv_cache, first_token):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            idx = self.start + i\n",
    "            past = kv_cache.get(idx)\n",
    "            out, present = layer(\n",
    "                x,\n",
    "                position_embeddings=(cos, sin),\n",
    "                past_key_value=past,\n",
    "                use_cache=True\n",
    "            )\n",
    "            x = out\n",
    "            kv_cache.update(idx, present)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_stream(model, tokenizer, input_ids, max_new_tokens=10):\n",
    "    device = torch.device('cuda')\n",
    "    batch, seq_len = input_ids.shape\n",
    "\n",
    "    # Prepare cache and move critical parts\n",
    "    num_layers = len(model.model.layers)\n",
    "    kv_cache = KVCache(num_layers)\n",
    "    model.model.embed_tokens.to(device)\n",
    "    model.model.norm.to(device)\n",
    "    model.lm_head.to(device)\n",
    "\n",
    "    rot_emb = model.model.rotary_emb\n",
    "    chunks = get_layers_per_chunk(model)\n",
    "\n",
    "    x = None\n",
    "    for step in range(max_new_tokens):\n",
    "        is_first = (step == 0)\n",
    "        cur_ids = input_ids if is_first else input_ids[:, -1:]\n",
    "        x = model.model.embed_tokens(cur_ids.to(device))\n",
    "\n",
    "                # Rotary embeddings\n",
    "        cur_seq_len = cur_ids.shape[1]\n",
    "        if is_first:\n",
    "            pos_ids = torch.arange(cur_seq_len, device=device).unsqueeze(0).expand(batch, -1)\n",
    "        else:\n",
    "            pos_ids = torch.tensor([[seq_len-1]], device=device).expand(batch, 1)\n",
    "        cos, sin = rot_emb(x=x, position_ids=pos_ids)\n",
    "\n",
    "        # Process in chunks\n",
    "        rem = num_layers\n",
    "        start = 0\n",
    "        attn_mask = torch.ones_like(cur_ids, device=device)\n",
    "        while rem > 0:\n",
    "            end = start + chunks\n",
    "            chunk = LayerChunk(model.model.layers, start, min(end, num_layers)).to(device)\n",
    "            x = chunk(x, attn_mask, cos, sin, kv_cache, is_first)\n",
    "            # offload\n",
    "            chunk.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "            start += chunks\n",
    "            rem -= chunks\n",
    "\n",
    "        x = model.model.norm(x)\n",
    "        logits = model.lm_head(x)\n",
    "        nxt = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        print(tokenizer.decode(nxt[0]), end='', flush=True)\n",
    "        input_ids = torch.cat([input_ids.to(device), nxt], dim=-1)\n",
    "        seq_len += 1\n",
    "\n",
    "    print()\n",
    "    # Cleanup\n",
    "    model.model.embed_tokens.to('cpu')\n",
    "    model.model.norm.to('cpu')\n",
    "    model.lm_head.to('cpu')\n",
    "    kv_cache.clear()\n",
    "\n",
    "\n",
    "def main():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-1B\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Single-shot prompt = \"hi\"\n",
    "    prompt = \"hi\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"Output:\", end=' ')\n",
    "    start = time.time()\n",
    "    compute_stream(model, tokenizer, inputs.input_ids, max_new_tokens=10)\n",
    "    print(f\"Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d94296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaDecoderLayer(\n",
       "  (self_attn): LlamaAttention(\n",
       "    (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "    (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "    (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "    (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "  )\n",
       "  (mlp): LlamaMLP(\n",
       "    (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "    (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "    (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "    (act_fn): SiLU()\n",
       "  )\n",
       "  (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "  (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer=model.model.layers[0]\n",
    "layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8199e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "tok = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=None,\n",
    "    use_cache=True\n",
    ").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5a44b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,  15339,   1917]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt='hello world'\n",
    "a=tok(txt,return_tensors='pt')['input_ids'].to('cpu')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1410e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 2048])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "layer=model.model.layers[0]\n",
    "emb=model.model.embed_tokens\n",
    "out=emb(a)\n",
    "print(out.shape)\n",
    "out\n",
    "seq_len = out.shape[1]\n",
    "print(seq_len)\n",
    "batch_size = out.shape[0]\n",
    "position_ids = torch.arange(seq_len, dtype=torch.long, device=out.device).unsqueeze(0).expand(batch_size, -1)\n",
    "# cache = DynamicCache()\n",
    "cos, sin = model.model.rotary_emb(x=out,position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f686b8a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0022,  0.0041, -0.0007,  ...,  0.0190, -0.0042, -0.0025],\n",
       "          [ 0.0022, -0.0168,  0.0077,  ...,  0.0016, -0.0165,  0.0051],\n",
       "          [-0.0022,  0.0164, -0.0248,  ..., -0.0225,  0.0405,  0.0153]]],\n",
       "        dtype=torch.bfloat16, grad_fn=<AddBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(out,position_embeddings=(cos,sin),use_cache=True,return_dict=True,past_key_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f37b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy key shape: torch.Size([1, 32, 5, 128])\n",
      "Dummy value shape: torch.Size([1, 32, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "# Get dimensions from your model configuration\n",
    "num_heads = layer.self_attn.num_heads\n",
    "head_dim = layer.self_attn.head_dim\n",
    "batch_size, seq_len, hidden_size = out.shape\n",
    "\n",
    "# Create dummy key and value tensors\n",
    "# Shape: [batch_size, num_heads, past_seq_len, head_dim]\n",
    "past_seq_len = 5  # You can set this to any length you want\n",
    "\n",
    "dummy_key = torch.randn(\n",
    "    batch_size, num_heads, past_seq_len, head_dim,\n",
    "    dtype=torch.bfloat16, device=out.device\n",
    ")\n",
    "\n",
    "dummy_value = torch.randn(\n",
    "    batch_size, num_heads, past_seq_len, head_dim,\n",
    "    dtype=torch.bfloat16, device=out.device\n",
    ")\n",
    "\n",
    "# Create the past_key_value tuple\n",
    "dummy_past_kv = (dummy_key, dummy_value)\n",
    "\n",
    "print(f\"Dummy key shape: {dummy_key.shape}\")\n",
    "print(f\"Dummy value shape: {dummy_value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3c43a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.llama.modeling_llama.LlamaDecoderLayer'>\n",
      "LlamaDecoderLayer\n",
      "Output type: <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# Check what type of object you're calling\n",
    "print(type(layer))\n",
    "print(layer.__class__.__name__)\n",
    "\n",
    "# Try with explicit cache initialization\n",
    "past_key_values = None  # or initialize appropriately\n",
    "output = layer(\n",
    "    out, \n",
    "    # attention_mask=attention_mask,\n",
    "    position_embeddings=(cos,sin),\n",
    "    past_key_value=past_key_values,\n",
    "    use_cache=True,\n",
    "    return_dict=True\n",
    ")\n",
    "\n",
    "# Check the output structure\n",
    "print(f\"Output type: {type(output)}\")\n",
    "if hasattr(output, 'keys'):\n",
    "    print(f\"Output keys: {output.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5717812a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 3, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache.value_cache[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02e6790c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0064, -0.0388,  0.0376,  ...,  0.0210,  0.0374, -0.0566],\n",
       "          [ 0.0047, -0.0008, -0.1338,  ..., -0.0234,  0.0312, -0.0186],\n",
       "          [-0.0037,  0.0060, -0.1680,  ...,  0.0391, -0.1133, -0.0493]]],\n",
       "        dtype=torch.bfloat16),\n",
       " DynamicCache())"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DynamicCache\n",
    "import torch\n",
    "\n",
    "# Assume `a` is input_ids, `model.model.layers` is the list of transformer layers, and `layer` is one such layer (e.g., model.model.layers[0])\n",
    "layer_idx = 0  # For example, using layer 0\n",
    "layer = model.model.layers[layer_idx]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Step 1: Prepare embeddings and rotary position embeddings\n",
    "    input_embeds = model.model.embed_tokens(a)\n",
    "    pos_ids = torch.arange(input_embeds.shape[1], device=input_embeds.device).unsqueeze(0)\n",
    "    cos, sin = model.model.rotary_emb(input_embeds, pos_ids)\n",
    "\n",
    "    # Step 2: Run a forward pass with dummy cache to populate internal shapes\n",
    "    dummy_cache = DynamicCache()\n",
    "    _ = layer(\n",
    "        input_embeds,\n",
    "        position_embeddings=(cos, sin),\n",
    "        past_key_value=dummy_cache,\n",
    "        use_cache=True,\n",
    "        return_dict=False,\n",
    "    )\n",
    "\n",
    "    # Step 3: Extract a valid key/value tensor for shape reference\n",
    "    valid_key = dummy_cache.key_cache[layer_idx]  # One layer's key\n",
    "    valid_value = dummy_cache.value_cache[layer_idx]  # One layer's value\n",
    "\n",
    "    # Step 4: Create a fresh DynamicCache and initialize with zero tensors\n",
    "    new_cache = DynamicCache()\n",
    "    new_cache.key_cache = [torch.zeros_like(valid_key)]\n",
    "    new_cache.value_cache = [torch.zeros_like(valid_value)]\n",
    "\n",
    "    # Step 5: Run again using this initialized cache\n",
    "    # Recompute embeddings\n",
    "    input_embeds = model.model.embed_tokens(a)\n",
    "    pos_ids = torch.arange(input_embeds.shape[1], device=input_embeds.device).unsqueeze(0)\n",
    "    cos, sin = model.model.rotary_emb(input_embeds, pos_ids)\n",
    "\n",
    "    # Step 6: Final forward pass\n",
    "    output = layer(\n",
    "        input_embeds,\n",
    "        position_embeddings=(cos, sin),\n",
    "        past_key_value=new_cache,\n",
    "        use_cache=True,\n",
    "        return_dict=False,\n",
    "    )\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30afb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states shape: torch.Size([1, 3, 4096])\n",
      "KV cache: None\n",
      "Full output length: 1\n"
     ]
    }
   ],
   "source": [
    "# Your current call\n",
    "# output = layer(\n",
    "#     out,\n",
    "#     position_embeddings=(cos, sin),\n",
    "#     use_cache=True,\n",
    "#     return_dict=True  # This is ignored for individual layers\n",
    "# )\n",
    "\n",
    "# Access both outputs\n",
    "hidden_states = output[0]  # Your logits/hidden states\n",
    "present_kv_cache = output[1] if len(output) > 1 else None  # Key-value cache\n",
    "\n",
    "print(f\"Hidden states shape: {hidden_states.shape}\")\n",
    "print(f\"KV cache: {present_kv_cache}\")\n",
    "print(f\"Full output length: {len(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62bf7b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output length: 2\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have your layer and input tensor\n",
    "batch_size, seq_len, hidden_size = 1, 3, 4096\n",
    "cache_position = torch.arange(seq_len, dtype=torch.long, device=hidden_states.device)\n",
    "# Call the self-attention module directly\n",
    "attn_output = layer.self_attn(\n",
    "    out,\n",
    "    position_embeddings=(cos,sin),\n",
    "    attention_mask=torch.ones(batch_size, seq_len, dtype=torch.bool),\n",
    "    position_ids=torch.arange(seq_len, dtype=torch.long).unsqueeze(0),\n",
    "    past_key_value=None,  # No previous cache\n",
    "    use_cache=True,\n",
    "    return_dict=False , # Important: use False for tuple output\n",
    "    cache_position=cache_position\n",
    ")\n",
    "\n",
    "print(f\"Attention output length: {len(attn_output)}\")\n",
    "hidden_states_out = attn_output[0]  # Attention output\n",
    "present_key_value = attn_output[1]  # KV cache tuple (key, value)\n",
    "\n",
    "if present_key_value is not None:\n",
    "    key_cache, value_cache = present_key_value\n",
    "    print(f\"Key cache shape: {key_cache.shape}\")\n",
    "    print(f\"Value cache shape: {value_cache.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "080e7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention output length: 3\n",
      "Cache after forward: None\n"
     ]
    }
   ],
   "source": [
    "from transformers import Cache, DynamicCache\n",
    "import torch\n",
    "\n",
    "# Initialize a proper cache object\n",
    "cache = DynamicCache()\n",
    "\n",
    "# Create the required cache_position tensor\n",
    "batch_size, seq_len, hidden_size = out.shape\n",
    "cache_position = torch.arange(seq_len, device=out.device, dtype=torch.long)\n",
    "\n",
    "# Call with the new cache framework\n",
    "attn_output = layer.self_attn(\n",
    "    out,\n",
    "    position_embeddings=(cos,sin),\n",
    "    position_ids=torch.arange(seq_len, dtype=torch.long, device=out.device).unsqueeze(0),\n",
    "    past_key_value=cache,  # Use DynamicCache instead of None\n",
    "    use_cache=True,\n",
    "    cache_position=cache_position  # Required in newer versions\n",
    ")\n",
    "\n",
    "print(f\"Attention output length: {len(attn_output)}\")\n",
    "print(f\"Cache after forward: {attn_output[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10db8153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 4096])\n",
      "Input device: cpu\n",
      "Layer device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Make sure your hidden_states tensor requires gradients and is on the right device\n",
    "hidden_states = out.requires_grad_(True)\n",
    "hidden_states = hidden_states.to(layer.self_attn.q_proj.weight.device)\n",
    "\n",
    "batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "print(f\"Input shape: {hidden_states.shape}\")\n",
    "print(f\"Input device: {hidden_states.device}\")\n",
    "print(f\"Layer device: {layer.self_attn.q_proj.weight.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53eda63c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0005, -0.0011,  0.0004,  ..., -0.0008, -0.0005,  0.0003],\n",
       "          [-0.0005, -0.0011,  0.0004,  ..., -0.0008, -0.0004,  0.0003],\n",
       "          [-0.0005, -0.0011,  0.0004,  ..., -0.0008, -0.0005,  0.0003]]],\n",
       "        dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e7b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kv_store = {}\n",
    "\n",
    "def capture_kv(layer, inp, out):\n",
    "    x = inp[0]\n",
    "    attn = layer.self_attn\n",
    "    kv_store[0] = {\n",
    "        \"k\": attn.k_proj(x).detach().cpu(),\n",
    "        \"v\": attn.v_proj(x).detach().cpu()\n",
    "    }\n",
    "\n",
    "layer = model.model.layers[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = model.model.embed_tokens(inputs[\"input_ids\"])\n",
    "    pos_ids = torch.arange(x.shape[1], device=x.device).unsqueeze(0)\n",
    "    cos, sin = model.model.rotary_emb(x, pos_ids)\n",
    "\n",
    "    capture_kv(layer, (x,), None)\n",
    "    _ = layer(x, position_ids=pos_ids, position_embeddings=(cos, sin), use_cache=False)\n",
    "\n",
    "for k, v in kv_store[0].items():\n",
    "    print(f\"{k}: {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec5abeb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0762,  0.0170,  0.0066,  ...,  0.0359,  0.0054, -0.0391],\n",
       "         [ 0.2676,  0.0698, -0.0217,  ..., -0.0527, -0.1035,  0.0228],\n",
       "         [ 0.3164,  0.1436, -0.0065,  ..., -0.0225, -0.0032,  0.0496]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = layer.self_attn\n",
    "attn.k_proj(out).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37615d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = layer.self_attn\n",
    "k=attn.k_proj(out).detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0081d84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12960e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.43.2\n",
      "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (2.32.4)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.2)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from transformers==4.43.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.43.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.43.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.43.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/pa/miniconda3/envs/gen_ai/lib/python3.10/site-packages (from requests->transformers==4.43.2) (2025.7.9)\n",
      "Downloading transformers-4.43.2-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers\n",
      "\u001b[2K    Found existing installation: tokenizers 0.21.2\n",
      "\u001b[2K    Uninstalling tokenizers-0.21.2:\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.21.2\n",
      "\u001b[2K  Attempting uninstall: transformers\n",
      "\u001b[2K    Found existing installation: transformers 4.53.2\n",
      "\u001b[2K    Uninstalling transformers-4.53.2:╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.53.2━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [transformers]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers][0m [transformers]\n",
      "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.19.1 transformers-4.43.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.43.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f06784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
